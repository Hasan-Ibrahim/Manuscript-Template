@article{Afzal2010,
abstract = {Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Na\&\#x0EF;ve Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques.},
author = {Afzal, Wasif},
doi = {10.1109/APSEC.2010.54},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings - Asia-Pacific Software Engineering Conference, APSEC/Using faults-slip-through metric as a predictor of fault-proneness.pdf:pdf},
isbn = {9780769542669},
issn = {15301362},
journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
keywords = {Classification,Measurement,Reliability},
pages = {414--422},
title = {{Using faults-slip-through metric as a predictor of fault-proneness}},
year = {2010}
}
@article{Airola2011,
author = {Airola, Antti and Pahikkala, Tapio and Waegeman, Willem and {De Baets}, Bernard and Salakoski, Tapio},
doi = {10.1016/j.csda.2010.11.018},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Computational Statistics \& Data Analysis/An experimental comparison of cross-validation techniques for estimating the area under the ROC curve.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
keywords = {area under the roc,classifier performance estimation,conditional auc estimation,curve},
month = apr,
number = {4},
pages = {1828--1844},
publisher = {Elsevier B.V.},
title = {{An experimental comparison of cross-validation techniques for estimating the area under the ROC curve}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947310004469},
volume = {55},
year = {2011}
}
@article{Altman2000,
author = {Altman, Douglas G. and Royston, Patrick},
doi = {10.1002/(SICI)1097-0258(20000229)19:4<453::AID-SIM350>3.3.CO;2-X},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Statistics in Medicine/What do we mean by validating a prognostic model.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
number = {4},
pages = {453--473},
title = {{What do we mean by validating a prognostic model?}},
url = {http://doi.wiley.com/10.1002/\%28SICI\%291097-0258\%2820000229\%2919\%3A4\%3C453\%3A\%3AAID-SIM350\%3E3.3.CO\%3B2-X},
volume = {19},
year = {2000}
}
@article{Amasaki2003,
abstract = { To predict software quality, we must consider various factors because software development consists of various activities, which the software reliability growth model (SRGM) does not consider. In this paper, we propose a model to predict the final quality of a software product by using the Bayesian belief network (BBN) model. By using the BBN, we can construct a prediction model that focuses on the structure of the software development process explicitly representing complex relationships between metrics, and handling uncertain metrics, such as residual faults in the software products. In order to evaluate the constructed model, we perform an empirical experiment based on the metrics data collected from development projects in a certain company. As a result of the empirical evaluation, we confirm that the proposed model can predict the amount of residual faults that the SRGM cannot handle.},
author = {Amasaki, S. and Takagi, Y. and Mizuno, O. and Kikuno, T.},
doi = {10.1109/ISSRE.2003.1251044},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003/A Bayesian belief network for assessing the likelihood of fault content.pdf:pdf},
isbn = {0-7695-2007-3},
issn = {1071-9458},
journal = {14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.},
keywords = {bayesian belief network,causal model,soft-,ware quality prediction},
title = {{A Bayesian belief network for assessing the likelihood of fault content}},
year = {2003}
}
@article{Anderssen2006,
author = {Anderssen, Endre and Dyrstad, Knut and Westad, Frank and Martens, Harald},
doi = {10.1016/j.chemolab.2006.04.021},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Chemometrics and Intelligent Laboratory Systems/Reducing over-optimism in variable selection by cross-model validation.pdf:pdf},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {cross-model validation,jack-knifing,over-fitting,qsar,regression,variable selection},
month = dec,
number = {1-2},
pages = {69--74},
title = {{Reducing over-optimism in variable selection by cross-model validation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169743906001109},
volume = {84},
year = {2006}
}
@inproceedings{Aranda2009,
author = {Aranda, Jorge and Venolia, Gina},
booktitle = {Proceedings of the 31st International Conference on Software Engineering (ICSE'09)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the 31st International Conference on Software Engineering (ICSE'09)/The Secret Life of Bugs Going Past the Errors and Omissions in Software Repositories.pdf:pdf},
pages = {298--308},
title = {{The Secret Life of Bugs : Going Past the Errors and Omissions in Software Repositories}},
year = {2009}
}
@article{P.beliniIbrunoP.Nasi2005,
abstract = {Over the last years, software quality has one of the niost requirement in the devel- opment of systems. Fault-proneness estimation could play a key role in quality control of software prod- ucts. In this area. effort has been spent defining nietrics and identifying models for system Using these nietrics to assess which parts of im- the system are more fault-proneness is of portance. This paper reports a research with the analysis of more than 100 producing objective mation and prediction of software been to find a begun models for fault-proneness esti- The between the fault-proneness estimation rate and the size of the es- timation model in terms of number of metrics used in the ologies have been used. compared, and exploited. The methodologies were the logistic regres- sion and the discriminant analyses The correspond- ing models produced for fault-proneness estimation and prediction have been based on nietrics address- ing aspects of computer comparison produced satisfactory results in terms of fault-proneness prediction The produced models hate been cross validated by using data sets derived from source codes provided by two application sce- narios},
author = {Belini, P and Bruno, I and Nasi, P and Rogai, D},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Unknown/Comparing Fault- proneness Estimation Models.pdf:pdf},
pages = {205--214},
title = {{Comparing Fault- proneness Estimation Models}},
year = {2005}
}
@article{Bell2006,
abstract = {We continue investigating the use of a, negative binomial regression model to predict which files in a large industrial software system are most likely to contain many faults in the next release. A new empirical study is described whose subject is an automated voice response system. Not only is this system's functionality substantially different from that of the earlier systems we studied (an inventory system and a service provisioning system), it also uses a significantly different software development process. Instead of having regularly scheduled releases as both of the earlier systems did, this system has what are referred to as "continuous releases." We explore the use of three versions of the negative binomial regression model, as well as a simple lines-of-code based model, to make predictions for this system and discuss the differences observed from the earlier studies. Despite the different development process, the best version of the prediction model was able to identify, over the lifetime of the project, 20\% of the system's files that contained, on average, nearly three quarters of the faults that were detected in the system's next releases. Copyright 2006 ACM.},
author = {Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
doi = {10.1145/1146238.1146246},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the 2006 international symposium on Software testing and analysis - ISSTA'06/Looking for bugs in all the right places.pdf:pdf},
isbn = {1595932631},
journal = {Proceedings of the 2006 international symposium on Software testing and analysis - ISSTA'06},
keywords = {empirical study,fault-prone,prediction,regres-,sion model,software faults,software testing},
pages = {61},
title = {{Looking for bugs in all the right places}},
url = {http://portal.acm.org/citation.cfm?doid=1146238.1146246},
year = {2006}
}
@article{Bernstein2007,
abstract = {Predicting the defects in the next release of a large software system is a very valuable asset for the project manger to plan her resources. In this paper we argue that temporal features (or aspects) of the data are central to prediction performance. ...},
author = {Bernstein, Abraham and Ekanayake, Jayalath and Pinzger, Martin},
doi = {10.1145/1294948.1294953},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Ninth international workshop on Principles of software evolution in conjunction with the 6th ESECFSE joint meeting - IWPSE '07/Improving defect prediction using temporal features and non linear models.pdf:pdf},
isbn = {9781595937223},
journal = {Ninth international workshop on Principles of software evolution in conjunction with the 6th ESEC/FSE joint meeting - IWPSE '07},
keywords = {decision tree,defect prediction,mining software repository},
pages = {11},
title = {{Improving defect prediction using temporal features and non linear models}},
url = {http://portal.acm.org/citation.cfm?doid=1294948.1294953$\backslash$nhttp://www.aaai.org/Papers/IJCAI/2007/IJCAI07-366.pdf},
year = {2007}
}
@inproceedings{Bettenburg2012,
author = {Bettenburg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories (MSR'12)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the 9th IEEE Working Conference on Mining Software Repositories (MSR'12)/Think Locally , Act Globally Improving Defect and Effort Prediction Models.pdf:pdf},
isbn = {9781467317610},
keywords = {-software metrics,as a blackbox,as a means for,closer inspection of,has also gained use,instead of,models,outputs a prediction,set of measures and,techniques,understanding,using prediction models solely,which is fed a},
pages = {60--69},
title = {{Think Locally , Act Globally : Improving Defect and Effort Prediction Models}},
year = {2012}
}
@article{Bezerra2007,
abstract = {Much of the current research in software defect prediction focuses on building classifiers to predict only whether a software module is fault-prone or not. Using these techniques, the effort to test the software is directed at modules that are labelled as fault-prone by the classifier. This paper introduces a novel algorithm based on constructive RBF neural networks aimed at predicting the probability of errors in fault-prone modules; it is called RBF-DDA with Probabilistic Outputs and is an extension of RBF-DDA neural networks. The advantage of our method is that we can inform the test team of the probability of defect in a module, instead of indicating only if the module is fault-prone or not. Experiments carried out with static code measures from well-known software defect datasets from NASA show the effectiveness of the proposed method. We also compared the performance of the proposed method in software defect prediction with kNN and two of its variants, the S-POC-NN and R-POC-NN. The experimental results showed that the proposed method outperforms both S-POC-NN and R-POC-NN and that it is equivalent to kNN in terms of performance with the advantage of producing less complex classifiers.},
author = {Bezerra, M. E R and Oliveira, a. L I and Meira, S. R L},
doi = {10.1109/IJCNN.2007.4371415},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/IEEE International Conference on Neural Networks - Conference Proceedings/A constructive RBF neural network for estimating the probability of defects in software modules.pdf:pdf},
isbn = {142441380X},
issn = {10987576},
journal = {IEEE International Conference on Neural Networks - Conference Proceedings},
pages = {2869--2874},
title = {{A constructive RBF neural network for estimating the probability of defects in software modules}},
year = {2007}
}
@article{Bibi2006,
author = {Bibi, S. and Tsoumakas, G. and Stamelos, I. and Vlahvas, I.},
doi = {10.1109/AICCSA.2006.205110},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/IEEE International Conference on Computer Systems and Applications, 2006/Software Defect Prediction Using Regression via Classification.pdf:pdf},
isbn = {1-4244-0211-5},
journal = {IEEE International Conference on Computer Systems and Applications, 2006.},
pages = {330--336},
title = {{Software Defect Prediction Using Regression via Classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1618375},
year = {2006}
}
@article{Binkley2009,
abstract = {While challenging, the ability to predict faulty modules of a program is valuable to a software project because it can reduce the cost of software development, as well as software maintenance and evolution. Three language-processing based measures are introduced and applied to the problem of fault prediction. The first measure is based on the usage of natural language in a program's identifiers. The second measure concerns the conciseness and consistency of identifiers. The third measure, referred to as the QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgments of software quality. Two case studies consider the language processing measures applicability to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and the measures. Results, while complex, show that language processing measures improve fault prediction, especially when used in combination. Overall, the models explain one-third and two-thirds of the faults in the two case studies. Consistent with other uses of language processing, the value of the three measures increases with the size of the program module considered. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Binkley, David and Feild, Henry and Lawrie, Dawn and Pighin, Maurizio},
doi = {10.1016/j.jss.2009.06.036},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Journal of Systems and Software/Increasing diversity Natural language measures for software fault prediction.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Code comprehension,Empirical software engineering,Fault prediction,Information retrieval,Linear regression models},
number = {11},
pages = {1793--1803},
publisher = {Elsevier Inc.},
title = {{Increasing diversity: Natural language measures for software fault prediction}},
url = {http://dx.doi.org/10.1016/j.jss.2009.06.036},
volume = {82},
year = {2009}
}
@inproceedings{Bird2009,
author = {Bird, Christian and Bachmann, Adrian and Aune, Eirik and Duffy, John and Bernstein, Abraham and Filkov, Vladimir and Devanbu, Premkumar},
booktitle = {Proceedings of the the 7th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering (FSE'09)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the the 7th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of./Fair and Balanced Bias in Bug-Fix Datasets.pdf:pdf},
isbn = {9781605580012},
pages = {121--130},
title = {{Fair and Balanced? Bias in Bug-Fix Datasets}},
year = {2009}
}
@inproceedings{Bird2011a,
author = {Bird, Christian and Murphy, Brendan and Gall, Harald},
booktitle = {Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering (ESEC/FSE'11)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering (ESECFSE'11)/Don't Touch My Code ! Examining the Effects of Ownership on Software Quality.pdf:pdf},
isbn = {9781450304436},
keywords = {empirical software engineering,expertise,ownership,qual-},
pages = {4--14},
title = {{Don't Touch My Code ! Examining the Effects of Ownership on Software Quality}},
year = {2011}
}
@article{Bleeker2003,
author = {Bleeker, S.E and Moll, H.a and Steyerberg, E.W and a.R.T Donders and Derksen-Lubsen, G and Grobbee, D.E and Moons, K.G.M},
doi = {10.1016/S0895-4356(03)00207-5},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/Journal of Clinical Epidemiology/External validation is necessary in prediction research.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {bootstrap,external validation,internal validation,logistic regression,prediction models},
month = sep,
number = {9},
pages = {826--832},
title = {{External validation is necessary in prediction research:}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0895435603002075},
volume = {56},
year = {2003}
}
@book{Bleeker2002,
author = {Bleeker, Sacha Elisabeth},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Unknown/Children with fever without apparent source diagnosis and dilemmas Kinderen met koorts zonder focus diagnose en dilemma ' s door.pdf:pdf},
isbn = {907701795X},
number = {september},
title = {{Children with fever without apparent source : diagnosis and dilemmas Kinderen met koorts zonder focus : diagnose en dilemma ' s door}},
year = {2002}
}
@article{Boetticher2006,
author = {Boetticher, G},
doi = {10.4018/978-1-59140-941-1.ch003},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Advanced Machine Learner Applications in Software Engineering (Series on Software Engineering and Knowledge Engineering)/Improving credibility of machine learner models in software engineering.pdf:pdf},
journal = {Advanced Machine Learner Applications in Software Engineering (Series on Software Engineering and Knowledge Engineering)},
pages = {52--72},
title = {{Improving credibility of machine learner models in software engineering}},
url = {http://nas.uhcl.edu/boetticher/boetticherimprovingcredibilityofmachinelearning.pdf},
year = {2006}
}
@article{Borra2010,
author = {Borra, Simone and {Di Ciaccio}, Agostino},
doi = {10.1016/j.csda.2010.03.004},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Computational Statistics \& Data Analysis/Measuring the prediction error. A comparison of cross-validation, bootstrap and covariance penalty methods.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
month = dec,
number = {12},
pages = {2976--2989},
publisher = {Elsevier B.V.},
title = {{Measuring the prediction error. A comparison of cross-validation, bootstrap and covariance penalty methods}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947310001064},
volume = {54},
year = {2010}
}
@article{Breiman1992,
author = {Breiman, Leo and Spector, Philip},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1992/International Statistical Institute/Submodel Selection and Evaluation in Regression. The X-Random Case.pdf:pdf},
journal = {International Statistical Institute},
number = {3},
pages = {291--319},
title = {{Submodel Selection and Evaluation in Regression. The X-Random Case}},
volume = {60},
year = {1992}
}
@article{Briand2002,
abstract = { A number of papers have investigated the relationships between design metrics and the detection of faults in object-oriented software. Several of these studies have shown that such models can be accurate in predicting faulty classes within one particular software product. In practice, however, prediction models are built on certain products to be used on subsequent software development projects. How accurate can these models be, considering the inevitable differences that may exist across projects and systems? Organizations typically learn and change. From a more general standpoint, can we obtain any evidence that such models are economically viable tools to focus validation and verification effort? This paper attempts to answer these questions by devising a general but tailorable cost-benefit model and by using fault and design data collected on two mid-size Java systems developed in the same environment. Another contribution of the paper is the use of a novel exploratory analysis technique - MARS (multivariate adaptive regression splines) to build such fault-proneness models, whose functional form is a-priori unknown. The results indicate that a model built on one system can be accurately used to rank classes within another system according to their fault proneness. The downside, however, is that, because of system differences, the predicted fault probabilities are not representative of the system predicted. However, our cost-benefit model demonstrates that the MARS fault-proneness model is potentially viable, from an economical standpoint. The linear model is not nearly as good, thus suggesting a more complex model is required.},
author = {Briand, Lionel C. and Melo, Walcelio L. and W\"{u}st, J\"{u}rgen},
doi = {10.1109/TSE.2002.1019484},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/IEEE Transactions on Software Engineering/Assessing the applicability of fault-proneness models across object-oriented software projects.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Cross-validation,Empirical validation,Measures,Metrics,Object-oriented},
number = {7},
pages = {706--720},
title = {{Assessing the applicability of fault-proneness models across object-oriented software projects}},
volume = {28},
year = {2002}
}
@article{Buckland1997,
author = {Buckland, Author S T and Burnham, K P and Augustin, N H and Buckland, S T},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1997/Unknown/Model Selection An Integral Part of Inference.pdf:pdf},
number = {2},
pages = {603--618},
title = {{Model Selection : An Integral Part of Inference}},
volume = {53},
year = {1997}
}
@article{Statistics2014,
author = {Bunke, Olaf and Droge, Bernd},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1984/The Annals of Statistics/Bootstrap and Cross-Validation Estimates of the Prediction Error for Linear Regression Models.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {1400--1424},
title = {{Bootstrap and Cross-Validation Estimates of the Prediction Error for Linear Regression Models}},
volume = {12},
year = {1984}
}
@article{Caglayan2009,
abstract = {Many corporate code developers are the beta testers of open source software. They continue testing until they are sure that they have a stable version to build their code on. In this respect defect predictors play a critical role to identify defective parts of the software. Performance of a defect predictor is determined by correctly finding defective parts of the software without giving any false alarms. Having high false alarms means testers/developers would inspect bug free code unnecessarily. Therefore in this research we focused on decreasing the false alarm rates by using repository metrics. We conducted experiments on the data sets of Eclipse project. Our results showed that repository metrics decreased the false alarm rates on the average to 23\% from 32\% corresponding up to 907 less files to inspect.},
author = {\c{C}aǧlayan, Bora and Bener, Ayşe and Koch, Stefan},
doi = {10.1109/FLOSS.2009.5071357},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the 2009 ICSE Workshop on Emerging Trends in FreeLibreOpen Source Software Research and Development, FLOSS 2009/Merits of using repository metrics in defect prediction for open source projects.pdf:pdf},
isbn = {9781424437207},
journal = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development, FLOSS 2009},
pages = {31--36},
title = {{Merits of using repository metrics in defect prediction for open source projects}},
year = {2009}
}
@article{Calikli2009,
abstract = {Application of defect predictors in software development helps the managers to allocate their resources such as time and effort more efficiently and cost effectively to test certain sections of the code. In this research, we have used naive Bayes classifier (NBC) to construct our defect prediction framework. Our proposed framework uses the hierarchical structure information about the source code of the software product, to perform defect prediction at a functional method level and source file level. We have applied our model on SoftLAB and Eclipse datasets. We have measured the performance of our proposed model and applied cost benefit analysis. Our results reveal that source file level defect prediction improves the verification effort, while decreasing the defect prediction performance in all datasets.},
author = {Calikli, Gul and Tosun, Ayse and Bener, Ayse and Celik, Melih},
doi = {10.1109/ISCIS.2009.5291866},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 24th International Symposium on Computer and Information Sciences, ISCIS 2009/The effect of granularity level on software defect prediction.pdf:pdf},
isbn = {9781424450237},
journal = {2009 24th International Symposium on Computer and Information Sciences, ISCIS 2009},
keywords = {Component,Cost-benefit analysis,Defect prediciton,Na??ve Bayes Classifier,Static code attributes},
pages = {531--536},
title = {{The effect of granularity level on software defect prediction}},
year = {2009}
}
@article{Cartwright2000,
abstract = {The paper describes an empirical investigation into an industrial
object oriented (OO) system comprised of 133000 lines of C++. The system
was a subsystem of a telecommunications product and was developed using
the Shlaer-Mellor method (S. Shlaer and S.J. Mellor, 1988; 1992). From
this study, we found that there was little use of OO constructs such as
inheritance, and therefore polymorphism. It was also found that there
was a significant difference in the defect densities between those
classes that participated in inheritance structures and those that did
not, with the former being approximately three times more defect-prone.
We were able to construct useful prediction systems for size and number
of defects based upon simple counts such as the number of states and
events per class. Although these prediction systems are only likely to
have local significance, there is a more general principle that software
developers can consider building their own local prediction systems.
Moreover, we believe this is possible, even in the absence of the suites
of metrics that have been advocated by researchers into OO technology.
As a consequence, measurement technology may be accessible to a wider
group of potential users},
author = {Cartwright, M. and Shepperd, M.},
doi = {10.1109/32.879814},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/IEEE Transactions on Software Engineering/An empirical investigation of an object-oriented software system.pdf:pdf},
isbn = {0098-5589},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {8},
pages = {786--796},
title = {{An empirical investigation of an object-oriented software system}},
volume = {26},
year = {2000}
}
@article{Catal2007,
abstract = {The features of real-time dependable systems are availability, reliability, safety and security. In the near future, real-time systems will be able to adapt themselves according to the specific requirements and real-time dependability assessment technique will be able to classify modules as faulty or fault-free. Software fault prediction models help us in order to develop dependable software and they are commonly applied prior to system testing. In this study, we examine Chidamber-Kemerer (CK) metrics and some method-level metrics for our model which is based on artificial immune recognition system (AIRS) algorithm. The dataset is a part of NASA Metrics Data Program and class-level metrics are from PROMISE repository. Instead of validating individual metrics, our mission is to improve the prediction performance of our model. The experiments indicate that the combination of CK and the lines of code metrics provide the best prediction results for our fault prediction model. The consequence of this study suggests that class-level data should be used rather than method-level data to construct relatively better fault prediction models. Furthermore, this model can constitute a part of real-time dependability assessment technique for the future.},
author = {Catal, C. and Diri, B. and Ozumut, B.},
doi = {10.1109/DEPCOS-RELCOMEX.2007.8},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/2nd International Conference on Dependability of Computer Systems (DepCoS-RELCOMEX '07)/An Artificial Immune System Approach for Fault Prediction in Object-Oriented Software.pdf:pdf},
isbn = {0-7695-2850-3},
journal = {2nd International Conference on Dependability of Computer Systems (DepCoS-RELCOMEX '07)},
pages = {1--8},
title = {{An Artificial Immune System Approach for Fault Prediction in Object-Oriented Software}},
year = {2007}
}
@article{Celisse2009,
author = {Celisse, Alain},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Unknown/A survey of cross-validation procedures for model.pdf:pdf},
keywords = {62-02,62g05,62g08,62g09,cross-validation,efficiency,leave-one-out,leave-p-out,model consistency,model selection,msc classifications,risk estimation,v-fold cross-validation},
title = {{A survey of cross-validation procedures for model}},
volume = {69},
year = {2009}
}
@article{Chen,
author = {Chen, Chao and Liaw, Andy and Breiman, Leo},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Using Random Forest to Learn Imbalanced Data.pdf:pdf},
number = {1999},
pages = {1--12},
title = {{Using Random Forest to Learn Imbalanced Data}}
}
@article{Chena,
author = {Chen, Tse-hsun and Nagappan, Meiyappan and Shihab, Emad and Hassan, Ahmed E},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/An Empirical Study of Dormant Bugs.pdf:pdf},
keywords = {all or part of,empirical study,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,software bugs,software quality,this work for},
title = {{An Empirical Study of Dormant Bugs}}
}
@article{Jin2010,
abstract = {Software quality prediction can play a role of importance in software management, and thus in improve the quality of software systems. By mining software with data mining technique, predictive models can be induced that software managers the insights they need to tackle these quality problems in an efficient way. This paper deals with the adaptive dynamic and median particle swarm optimization (ADMPSO) based on the PSO classification technique. ADMPSO can act as a valid data mining technique to predict erroneous software modules. The predictive model in this paper extracts the relationship rules of software quality and metrics. Information entropy approach is applied to simplify the extraction rule set. The empirical result shows that this method set of rules can be streamlined and the forecast accuracy can be improved.},
author = {Cong, Jin and En-Mei, Dong and Li-Na, Qin},
doi = {10.1109/MMIT.2010.11},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Multimedia and Information Technology (MMIT), 2010 Second International Conference on/Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization.pdf:pdf},
isbn = {978-0-7695-4008-5},
journal = {Multimedia and Information Technology (MMIT), 2010 Second International Conference on},
keywords = {- data mining,admpso,and then the,as follows,classification,depicted in section 1,fault prediction,first,of software prediction is,rule simplification,the importance,this paper is structured},
pages = {44--47},
title = {{Software Fault Prediction Model Based on Adaptive Dynamical and Median Particle Swarm Optimization}},
volume = {1},
year = {2010}
}
@article{Ambros,
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Empirical Software Engineering/Evaluating Defect Prediction Approaches A Benchmark and an Extensive Comparison(2).pdf:pdf},
journal = {Empirical Software Engineering},
keywords = {change metrics,defect prediction,source code metrics},
number = {4-5},
pages = {531--577},
title = {{Evaluating Defect Prediction Approaches : A Benchmark and an Extensive Comparison}},
volume = {17},
year = {2012}
}
@article{DeCarvalho2008,
abstract = {Software testing is a fundamental software engineering activity for quality assurance that is also traditionally very expensive. To reduce efforts of testing strategies, some design metrics have been used to predict the fault-proneness of a software class or module. Recent works have explored the use of machine learning (ML) techniques for fault prediction. However most used ML techniques can not deal with unbalanced data and their results usually have a difficult interpretation. Because of this, this paper introduces a multi-objective particle swarm optimization (MOPSO) algorithm for fault prediction. It allows the creation of classifiers composed by rules with specific properties by exploring Pareto dominance concepts. These rules are more intuitive and easier to understand because they can be interpreted independently one of each other. Furthermore, an experiment using the approach is presented and the results are compared to the other techniques explored in the area.},
author = {{De Carvalho}, Andre B. and Pozo, Aurora and Vergilio, Silvia and Lenz, Alexandre},
doi = {10.1109/ICTAI.2008.76},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI/Predicting fault proneness of classes trough a multiobjective particle swarm optimization algorithm.pdf:pdf},
isbn = {9780769534404},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
pages = {387--394},
title = {{Predicting fault proneness of classes trough a multiobjective particle swarm optimization algorithm}},
volume = {2},
year = {2008}
}
@article{DeWinter2009,
author = {de Winter∗, J. C. F. and Dodou∗, D. and Wieringa, P. a.},
doi = {10.1080/00273170902794206},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Multivariate Behavioral Research/Exploratory Factor Analysis With Small Sample Sizes.pdf:pdf},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
month = apr,
number = {2},
pages = {147--181},
title = {{Exploratory Factor Analysis With Small Sample Sizes}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00273170902794206},
volume = {44},
year = {2009}
}
@article{Denaro2002a,
abstract = {The effectiveness of the software testing process is a key issue for meeting the increasing demand of quality without augmenting the overall costs of software development. The estimation of software fault-proneness is important for assessing costs and quality and thus better planning and tuning the testing process. Unfortunately, no general techniques are available for estimating software fault-proneness and the distribution of faults to identify the correct level of test for the required quality. Although software complexity and testing thoroughness are intuitively related to the costs of quality assurance and the quality of the final product, single software metrics and coverage criteria provide limited help in planning the testing process and assuring the required quality. By using logistic regression, this paper shows how models can be built that relate software measures and software fault-proneness for classes of homogeneous software products. It also proposes the use of cross-validation for selecting valid models even for small data sets. The early results show that it is possible to build statistical models based on historical data for estimating fault-proneness of software modules before testing, and thus better planning and monitoring the testing activities. Copyright 2002 ACM.},
author = {Denaro, Giovanni and Denaro, Giovanni and Morasca, Sandro and Morasca, Sandro and Pezz\`{e}, Mauro and Pezz\`{e}, Mauro},
doi = {10.1145/568760.568824},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings of the 14th international conference on Software engineering and knowledge engineering - SEKE '02/Deriving models of software fault-proneness.pdf:pdf},
isbn = {1581135564},
journal = {Proceedings of the 14th international conference on Software engineering and knowledge engineering - SEKE '02},
keywords = {cross-valida-,fault-proneness models,logistic regression,ness,software faulti-,software metrics,software testing process},
pages = {361},
title = {{Deriving models of software fault-proneness}},
url = {http://portal.acm.org/citation.cfm?doid=568760.568824},
year = {2002}
}
@article{Efron2014,
author = {Efron, B and Tibshirani, R},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Unknown/Bootstrap Methods for Standard Errors , Confidence Intervals , and Other Measures of Statistical Accuracy.pdf:pdf},
keywords = {bootstrap method},
number = {1},
pages = {54--75},
title = {{Bootstrap Methods for Standard Errors , Confidence Intervals , and Other Measures of Statistical Accuracy}},
volume = {1},
year = {2014}
}
@article{Efron1983,
author = {Efron, Bradley},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1983/Journal of the American Statistical Association/Estimating the error rate of a prediction rule some improvements on cross-validation.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {382},
pages = {316--331},
title = {{Estimating the error rate of a prediction rule: some improvements on cross-validation}},
volume = {78},
year = {1983}
}
@article{Efron1997,
author = {Efron, Bradley and Tibshirani, Robert},
doi = {10.1080/01621459.1997.10474007},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1997/Journal of the American Statistical Association/Improvements on Cross-Validation The 632 Bootstrap Method.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {classification,cross-validation bootstrap,prediction rule},
number = {438},
pages = {548--560},
title = {{Improvements on Cross-Validation: The 632+ Bootstrap Method}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1997.10474007},
volume = {92},
year = {1997}
}
@article{Esbensen2010,
author = {Esbensen, Kim H. and Geladi, Paul},
doi = {10.1002/cem.1310},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Journal of Chemometrics/Principles of Proper Validation use and abuse of re-sampling for validation.pdf:pdf},
issn = {08869383},
journal = {Journal of Chemometrics},
keywords = {cross-validation,future performance assessment,ppv,predictive regression,principles of proper validation,re-sampling,test set validation,theory of sampling,tos},
month = apr,
number = {3-4},
pages = {168--187},
title = {{Principles of Proper Validation: use and abuse of re-sampling for validation}},
url = {http://doi.wiley.com/10.1002/cem.1310},
volume = {24},
year = {2010}
}
@article{Fenton2007,
abstract = {To make accurate predictions of attributes like defects found in complex software projects we need a rich set of process factors. We have developed a causal model that includes such process factors, both quantitative and qualitative. The factors in the model were identified as part of a major collaborative project. A challenge for such a model is getting the data needed to validate it. We present a dataset, elicited from 31 completed software projects in the consumer electronics industry, which we used for validation. The data were gathered using a questionnaire distributed to managers of recent projects. The dataset will be of interest to other researchers evaluating models with similar aims. We make both the dataset and causal model available for research use.},
author = {Fenton, Norman and Neil, Martin and Marsh, William and Hearty, Peter and Radlinski, Lukasz and Krause, Paul},
doi = {10.1109/PROMISE.2007.11},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Unknown/Project Data Incorporating Qualitative Facts for Improved Software Defect Prediction.pdf:pdf},
isbn = {0-7695-2954-2},
title = {{Project Data Incorporating Qualitative Facts for Improved Software Defect Prediction}},
url = {http://epubs.surrey.ac.uk/publcomp3/15},
year = {2007}
}
@inproceedings{Giger2012,
author = {Giger, Emanuel and D'Ambros, Marco and Pinzger, Martin and Gall, Harald C.},
booktitle = {Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement (ESEM'12)},
doi = {10.1145/2372251.2372285},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement (ESEM'12)/Method-level bug prediction.pdf:pdf},
isbn = {9781450310567},
keywords = {fine-grained source code changes,method-level bug prediction},
pages = {171--180},
title = {{Method-level bug prediction}},
year = {2012}
}
@article{Giger2011,
abstract = {A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified (LM) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes (SCC) for bug prediction. SCC captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of SCC and LM. The results show that SCC outperforms LM for learning bug prediction models.},
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C},
doi = {10.1145/1985441.1985456},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the 8th Working Conference on Mining Software Repositories/Comparing Fine-grained Source Code Changes and Code Churn for Bug Prediction.pdf:pdf},
isbn = {978-1-4503-0574-7},
issn = {02705257},
journal = {Proceedings of the 8th Working Conference on Mining Software Repositories},
keywords = {code churn,nonlinear regression,prediction models,software bugs,source code changes},
pages = {83--92},
title = {{Comparing Fine-grained Source Code Changes and Code Churn for Bug Prediction}},
url = {http://doi.acm.org/10.1145/1985441.1985456},
year = {2011}
}
@article{Gondra2008,
abstract = {The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module's fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA's Metrics Data Program data repository. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Gondra, Iker},
doi = {10.1016/j.jss.2007.05.035},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Journal of Systems and Software/Applying machine learning to software fault-proneness prediction.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Fault-proneness,Machine learning,Neural network,Sensitivity analysis,Software metrics,Software testing,Support vector machine},
pages = {186--195},
title = {{Applying machine learning to software fault-proneness prediction}},
volume = {81},
year = {2008}
}
@article{Guo2003,
abstract = { This paper describes a novel methodology for predicting fault prone modules. The methodology is based on Dempster-Shafer (D-S) belief networks. Our approach consists of three steps: first, building the D-S network by the induction algorithm; second, selecting the predictors (attributes) by the logistic procedure; third, feeding the predictors describing the modules of the current project into the inducted D-S network and identifying fault prone modules. We applied this methodology to a NASA dataset. The prediction accuracy of our methodology is higher than that achieved by logistic regression or discriminant analysis on the same dataset.},
archivePrefix = {arXiv},
arxivId = {0709.1706v2},
author = {Guo, L. and Cukic, B. and Singh, H.},
doi = {10.1109/ASE.2003.1240314},
eprint = {0709.1706v2},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings/Predicting fault prone modules by the Dempster-Shafer belief networks.pdf:pdf},
isbn = {0-7695-2035-9},
issn = {1527-1366},
journal = {18th IEEE International Conference on Automated Software Engineering, 2003. Proceedings.},
pages = {3--6},
title = {{Predicting fault prone modules by the Dempster-Shafer belief networks}},
year = {2003}
}
@article{Gyimothy2005,
abstract = { Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.},
author = {Gyimothy, T. and Ferenc, R. and Siket, I.},
doi = {10.1109/TSE.2005.112},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/IEEE Transactions on Software Engineering/Empirical validation of object-oriented metrics on open source software for fault prediction.pdf:pdf},
isbn = {0098-5589},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Bugzilla,C++,Columbus.,Index Terms- Fact extraction,Mozilla,compiler wrapping,fault-proneness detection,metrics validation,open source software,reverse engineering},
number = {10},
pages = {897--910},
title = {{Empirical validation of object-oriented metrics on open source software for fault prediction}},
volume = {31},
year = {2005}
}
@article{Jr1996,
author = {{Harrell Jr.}, Frank E. and Lee, Kerry L and Mark, Daniel B},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1996/Statistics in Medicine/Tutorial in Biostatistics Multivariable Prognostic Models Issues in Developing Models, Evaluting Assumptions and Adequacy, and Measurin.pdf:pdf},
journal = {Statistics in Medicine},
pages = {361--387},
title = {{Tutorial in Biostatistics Multivariable Prognostic Models : Issues in Developing Models, Evaluting Assumptions and Adequacy, and Measuring and Reducing Errors}},
volume = {15},
year = {1996}
}
@article{Hassan2005,
abstract = { To remain competitive in the fast paced world of software development, managers must optimize the usage of their limited resources to deliver quality products on time and within budget. In this paper, we present an approach (the top ten list) which highlights to managers the ten most susceptible subsystems (directories) to have a fault. Managers can focus testing resources to the subsystems suggested by the list. The list is updated dynamically as the development of the system progresses. We present heuristics to create the top ten list and develop techniques to measure the performance of these heuristics. To validate our work, we apply our presented approach to six large open source projects (three operating systems: NetBSD, FreeBSD, OpenBSD; a window manager: KDE; an office productivity suite: KOffice; and a database management system: Postgres). Furthermore, we examine the benefits of increasing the size of the top ten list and study its performance.},
author = {a.E. Hassan and Holt, R.C.},
doi = {10.1109/ICSM.2005.91},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/21st IEEE International Conference on Software Maintenance (ICSM'05)/The top ten list dynamic fault prediction.pdf:pdf},
isbn = {0-7695-2368-4},
issn = {1063-6773},
journal = {21st IEEE International Conference on Software Maintenance (ICSM'05)},
title = {{The top ten list: dynamic fault prediction}},
year = {2005}
}
@article{He2009,
author = {He, Haibo and Garcia, Edwardo A},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/IEEE Transactions on Knowledge and Data Engineering/Learning from Imbalanced Data(2).pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data Engineering},
number = {9},
pages = {1263--1284},
title = {{Learning from Imbalanced Data}},
volume = {21},
year = {2009}
}
@inproceedings{Herzig2013,
author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
booktitle = {Proceedings of the 35th International Conference on Software Engineering (ICSE'13)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Proceedings of the 35th International Conference on Software Engineering (ICSE'13)/It's not a Bug, it's a Feature How Misclassiﬁcation Impacts Bug Prediction.pdf:pdf},
number = {Section XII},
pages = {392--401},
title = {{It's not a Bug, it's a Feature: How Misclassiﬁcation Impacts Bug Prediction}},
year = {2013}
}
@article{Higo2008,
abstract = {This paper describe a method for identifying fault-prone modules. The method utilizes metrics transitions rather than raw metrics values. Metrics transitions are measured from the source code of consecutive versions, which is archived in software repositories. Metrics transitions should be an good indicator of software quality because they reflect how the software system has evolved. This paper exhibits a case study, which is a comparison between metrics transitions and CK metrics suite. In the case study, the metrics transitions could precisely identify fault-prone modules. Copyright 2008 ACM.},
author = {Higo, Yoshiki and Murao, Kenji and Kusumoto, Shinji and Inoue, Katsuro},
doi = {10.1145/1390817.1390820},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 2008 workshop on Defects in large software systems - DEFECTS '08/Predicting fault-prone modules based on metrics transitions.pdf:pdf},
isbn = {9781605580517},
journal = {Proceedings of the 2008 workshop on Defects in large software systems - DEFECTS '08},
pages = {6},
title = {{Predicting fault-prone modules based on metrics transitions}},
url = {http://portal.acm.org/citation.cfm?doid=1390817.1390820},
year = {2008}
}
@article{Holschuh2009,
abstract = {Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50-60\% of the 20\% most defect-prone components.},
author = {Holschuh, Tilman and Zimmermann, Thomas and P\"{a}user, Markus and Premraj, Rahul and Herzig, Kim and Zeller, Andreas},
doi = {10.1109/ICSE-COMPANION.2009.5070975},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 31st International Conference on Software Engineering - Companion Volume, ICSE 2009/Predicting defects in SAP java code An experience report.pdf:pdf},
isbn = {9781424434947},
journal = {2009 31st International Conference on Software Engineering - Companion Volume, ICSE 2009},
pages = {172--181},
title = {{Predicting defects in SAP java code: An experience report}},
year = {2009}
}
@article{Høst2009,
author = {H\o st, Einar W and \O stvold, Bjarte M},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Unknown/Debugging Method Names.pdf:pdf},
pages = {294--317},
title = {{Debugging Method Names}},
year = {2009}
}
@article{Hribar2010,
abstract = {Prediction of product quality within software engineering, preventive and corrective actions within the various project phases are constantly improved over the past decades. Practitioners and software companies were using various methods, different approaches and best practices in software development projects. Nevertheless, the issue of quality is pushing software companies to constantly invest in efforts to produce enough quality products that will arrive in time, with good enough quality to the customer. However, the quality is not for free, it has a price that is required at the time you notice about her. In this paper fuzzy logic and KNN classification method approaches are presented to predict Weibull distribution parameters shape, slope and the total number of faults in the system based on the software components individual contribution. Since the Weibull distribution is one of the most widely used probability distributions in the reliability engineering, predicting of its characteristics early in the software lifecycle might be useful input for the planning and control of verification activities.},
author = {Hribar, L. and Duka, D.},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/MIPRO, 2010 Proceedings of the 33rd International Convention/Software component quality prediction using KNN and Fuzzy logic.pdf:pdf},
isbn = {978-1-4244-7763-0},
journal = {MIPRO, 2010 Proceedings of the 33rd International Convention},
pages = {402--408},
title = {{Software component quality prediction using KNN and Fuzzy logic}},
year = {2010}
}
@article{Hu,
author = {Hu, Wei and Wong, Kenny},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Using Citation Influence to Predict Software Defects.pdf:pdf},
title = {{Using Citation Influence to Predict Software Defects}}
}
@article{Isaksson2008,
author = {Isaksson, A. and Wallman, M. and G\"{o}ransson, H. and Gustafsson, M.G.},
doi = {10.1016/j.patrec.2008.06.018},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Pattern Recognition Letters/Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {performance estimation,supervised classification},
number = {14},
pages = {1960--1965},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865508002158},
volume = {29},
year = {2008}
}
@article{Jia2009,
abstract = {Data transformation and attribute subset selection have been adopted in improving software defect/failure prediction methods. However, little consensus was achieved on their effectiveness. This paper reports a comparative study on these two kinds of techniques combined with four classifier and datasets from two projects. The results indicate that data transformation displays un obvious influence on improving the performance, while attribute subset selection methods show distinguishably inconsistent output. Besides, consistency across releases and discrepancy between the open-source and in-house maintenance projects in the evaluation of these methods are discussed.},
author = {Jia, Hao and Shu, Fengdi and Yang, Ye and Li, Qi},
doi = {10.1109/ICSM.2009.5306382},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/IEEE International Conference on Software Maintenance, ICSM/Data transformation and attribute subset selection Do they help make differences in software failure prediction.pdf:pdf},
isbn = {9781424448289},
issn = {1063-6773},
journal = {IEEE International Conference on Software Maintenance, ICSM},
pages = {519--522},
title = {{Data transformation and attribute subset selection: Do they help make differences in software failure prediction?}},
year = {2009}
}
@article{Jiang2007a,
abstract = {Studies show that programs contain much similar code, commonly known as clones. One of the main reasons for introducing clones is programmers' tendency to copy and paste code to quickly duplicate functionality. We commonly believe that clones can make programs difficult to maintain and introduce subtle bugs. Although much research has proposed techniques for detecting and removing clones to improve software maintainability, little has considered how to detect latent bugs introduced by clones. In this paper, we introduce a general notion of context-based inconsistencies among clones and develop an efficient algorithm to detect such inconsistencies for locating bugs. We have implemented our algorithm and evaluated it on large open source projects including the latest versions of the Linux kernel and Eclipse. We have discovered many previously unknown bugs and programming style issues in both projects (with 57 for the Linux kernel and 38 for Eclipse). We have also categorized the bugs and style issues and noticed that they exhibit diverse characteristics and are difficult to detect with any single existing bug detection technique. We believe that our approach complements well these existing techniques.},
author = {Jiang, Lingxiao and Su, Zhendong and Chiu, Edwin},
doi = {http://doi.acm.org/10.1145/1287624.1287634},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/. of the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering (ESEC-FSE 2007)/Context-based detection of clone-related bugs.pdf:pdf},
isbn = {978-1-59593-811-4},
journal = {Proc. of the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering (ESEC-FSE 2007)},
keywords = {clone detection,fault detection},
pages = {55--64},
title = {{Context-based detection of clone-related bugs}},
year = {2007}
}
@article{Jiang2013,
author = {Jiang, Tian and Tan, Lin and Kim, Sunghun},
doi = {10.1109/ASE.2013.6693087},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/2013 28th IEEEACM International Conference on Automated Software Engineering (ASE)/Personalized defect prediction.pdf:pdf},
isbn = {978-1-4799-0215-6},
journal = {2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
month = nov,
pages = {279--289},
publisher = {Ieee},
title = {{Personalized defect prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6693087},
year = {2013}
}
@article{Jiang2007,
abstract = {The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.},
author = {Jiang, Yue and Cukic, Bojan and Menzies, Tim},
doi = {10.1109/ISSRE.2007.24},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/The 18th IEEE International Symposium on Software Reliability (ISSRE '07)/Fault Prediction using Early Lifecycle Data.pdf:pdf},
isbn = {978-0-7695-3024-6},
issn = {1071-9458},
journal = {The 18th IEEE International Symposium on Software Reliability (ISSRE '07)},
pages = {237--246},
title = {{Fault Prediction using Early Lifecycle Data}},
year = {2007}
}
@inproceedings{Jiang2009,
author = {Jiang, Yue and Lin, Jie and Cukic, Bojan and Menzies, Tim},
booktitle = {Proceedings of the International Symposium on Software Reliability Engineering (ISSRE'09)},
doi = {10.1109/ISSRE.2009.13},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the International Symposium on Software Reliability Engineering (ISSRE'09)/Variance Analysis in Software Fault Prediction Models.pdf:pdf},
isbn = {978-1-4244-5375-7},
pages = {99--108},
title = {{Variance Analysis in Software Fault Prediction Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5362090},
year = {2009}
}
@article{Jones2002,
abstract = {One of the most expensive and time-consuming components of the debugging process is locating the erros or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. This paper presens a new technique that uses visualization to assit with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a teste suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, idetify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can be effective in helping a user locate faults in a program.},
author = {Jones, James a and Harrold, Mary Jean and Stasko, John},
doi = {10.1109/ICSE.2002.1007991},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings of ICSE/Visualization of test information to assit fault localization.pdf:pdf},
isbn = {1-58113-472-X},
journal = {Proceedings of ICSE},
keywords = {code coverage,software engineering,software visualisation,test suite},
pages = {467--477},
title = {{Visualization of test information to assit fault localization}},
year = {2002}
}
@article{Kamei2013,
author = {Kamei, Y. and Shihab, E. and Adams, B. and Hassan, A. E. and Mockus, A. and Sinha, A. and Ubayashi, N.},
doi = {10.1109/TSE.2012.70},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/IEEE Transactions on Software Engineering/A large-scale empirical study of just-in-time quality assurance.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {6},
pages = {757--773},
title = {{A large-scale empirical study of just-in-time quality assurance}},
volume = {39},
year = {2013}
}
@inproceedings{Kawrykow2011,
author = {Kawrykow, David and Robillard, Martin P.},
booktitle = {Proceeding of the 33rd International conference on Software Sngineering (ICSE '11)},
doi = {10.1145/1985793.1985842},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceeding of the 33rd International conference on Software Sngineering (ICSE '11)/Non-essential changes in version histories.pdf:pdf},
isbn = {9781450304450},
keywords = {differenc-,mining software repositories,software change analysis},
pages = {351--360},
publisher = {ACM Press},
title = {{Non-essential changes in version histories}},
year = {2011}
}
@article{Khoshgoftaar2000b,
author = {Khoshgoftaar, T.M. and Allen, E.B.},
doi = {10.1109/HASE.2000.895473},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Proceedings. Fifth IEEE International Symposium on High Assurance Systems Engineering (HASE 2000)/Prediction of software faults using fuzzy nonlinear regression modeling.pdf:pdf},
isbn = {0-7695-0927-4},
journal = {Proceedings. Fifth IEEE International Symposium on High Assurance Systems Engineering (HASE 2000)},
keywords = {f u z z,multiple,neural networks,sion model,software metrics,software reliability,y nonlinear regres-},
pages = {281--290},
title = {{Prediction of software faults using fuzzy nonlinear regression modeling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=895473},
year = {2000}
}
@article{Khoshgoftaar2001,
abstract = { Poisson regression model is widely used in software quality modeling. When the response variable of a data set includes a large number of zeros, Poisson regression model will underestimate the probability of zeros. A zero-inflated model changes the mean structure of the pure Poisson model. The predictive quality is therefore improved. In this paper, we examine a full-scale industrial software system and develop two models, Poisson regression and zero-inflated Poisson regression. To our knowledge, this is the first study that introduces the zero-inflated Poisson regression model in software reliability. Comparing the predictive qualities of the two competing models, we conclude that for this system, the zero-inflated Poisson regression model is more appropriate in theory and practice.},
author = {Khoshgoftaar, T.M. and Gao, K. and Szabo, R.M.},
doi = {10.1109/ISSRE.2001.989459},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2001/Proceedings 12th International Symposium on Software Reliability Engineering/An application of zero-inflated Poisson regression for software fault prediction.pdf:pdf},
isbn = {0-7695-1306-9},
issn = {10719458},
journal = {Proceedings 12th International Symposium on Software Reliability Engineering},
keywords = {nested models,poisson regres-,program module,sion model,software quality modeling,vuong hypothesis test,zero-inflated poisson regression model},
pages = {66--73},
title = {{An application of zero-inflated Poisson regression for software fault prediction}},
year = {2001}
}
@article{Khoshgoftaar2010,
abstract = {The data mining and machine learning community is often faced with two key problems: working with imbalanced data and selecting the best features for machine learning. This paper presents a process involving a feature selection technique for selecting the important attributes and a data sampling technique for addressing class imbalance. The application domain of this study is software engineering, more specifically, software quality prediction using classification models. When using feature selection and data sampling together, different scenarios should be considered. The four possible scenarios are: (1) feature selection based on original data, and modeling (defect prediction) based on original data; (2) feature selection based on original data, and modeling based on sampled data; (3) feature selection based on sampled data, and modeling based on original data; and (4) feature selection based on sampled data, and modeling based on sampled data. The research objective is to compare the software defect prediction performances of models based on the four scenarios. The case study consists of nine software measurement data sets obtained from the PROMISE software project repository. Empirical results suggest that feature selection based on sampled data performs significantly better than feature selection based on original data, and that defect prediction models perform similarly regardless of whether the training data was formed using sampled or original data.},
author = {Khoshgoftaar, T.M. and Gao, Kehan Gao Kehan and Seliya, N.},
doi = {10.1109/ICTAI.2010.27},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Tools with Artificial Intelligence (ICTAI), 2010 22nd IEEE International Conference on/Attribute Selection and Imbalanced Data Problems in Software Defect Prediction.pdf:pdf},
isbn = {978-1-4244-8817-9},
issn = {1082-3409},
journal = {Tools with Artificial Intelligence (ICTAI), 2010 22nd IEEE International Conference on},
keywords = {data sampling,defect prediction,feature selection,software measurements},
title = {{Attribute Selection and Imbalanced Data: Problems in Software Defect Prediction}},
volume = {1},
year = {2010}
}
@article{Khoshgoftaar2002,
abstract = { Software quality prediction models are used to achieve high software reliability. A module-order model (MOM) uses an underlying quantitative prediction model to predict this rank-order. This paper compares performances of module-order models of two different count models which are used as the underlying prediction models. They are the Poisson regression model and the zero-inflated Poisson regression model. It is demonstrated that improving a count model for prediction does not ensure a better MOM performance. A case study of a full-scale industrial software system is used to compare performances of module-order models of the two count models. It was observed that improving prediction of the Poisson count model by using zero-inflated Poisson regression did not yield module-order models with better performance. Thus, it was concluded that the degree of prediction accuracy of the underlying model did not influence the results of the subsequent module-order model. Module-order modeling is proven to be a robust and effective method even though both underlying prediction may sometimes lack acceptable prediction accuracy.},
author = {Khoshgoftaar, T.M. and Geleyn, E. and Gao, K.},
doi = {10.1109/METRIC.2002.1011335},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings Eighth IEEE Symposium on Software Metrics/An empirical study of the impact of count models predictions on module-order models.pdf:pdf},
isbn = {0-7695-1339-5},
journal = {Proceedings Eighth IEEE Symposium on Software Metrics},
keywords = {count models,module-order modeling,poisson,software metrics,software reliability,zip},
title = {{An empirical study of the impact of count models predictions on module-order models}},
year = {2002}
}
@article{Khoshgoftaar2002a,
abstract = { Complex high-assurance software systems depend highly on reliability of their underlying software applications. Early identification of high-risk modules can assist in directing quality enhancement efforts to modules that are likely to have a high number of faults. Regression tree models are simple and effective as software quality prediction models, and timely predictions from such models can be used to achieve high software reliability. This paper presents a case study from our comprehensive evaluation (with several large case studies) of currently available regression tree algorithms for software fault prediction. These are, CART-LS (least squares), S-PLUS, and CART-LAD (least absolute deviation). The case study presented comprises of software design metrics collected from a large network telecommunications system consisting of almost 13 million lines of code. Tree models using design metrics are built to predict the number of faults in modules. The algorithms are also compared based on the structure and complexity of their tree models. Performance metrics, average absolute and average relative errors are used to evaluate fault prediction accuracy.},
author = {Khoshgoftaar, T.M. and Seliya, N.},
doi = {10.1109/METRIC.2002.1011339},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings Eighth IEEE Symposium on Software Metrics/Tree-based software quality estimation models for fault prediction.pdf:pdf},
isbn = {0-7695-1339-5},
journal = {Proceedings Eighth IEEE Symposium on Software Metrics},
keywords = {authors through taghi m,cart,deptart-,empirical softw areengineering laboratory,goftaar,khosh-,least absolute deviation,least squares,readers may contact the,regression trees,s-plus,software fault prediction},
title = {{Tree-based software quality estimation models for fault prediction}},
year = {2002}
}
@article{Khoshgoftaar2000,
author = {Khoshgoftaar, Taghi M and Allen, Edward B},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Empirical Software Engineering/Improving Tree-Based Models of Software Quality with Principal Components Analysis.pdf:pdf},
isbn = {0769508073},
journal = {Empirical Software Engineering},
keywords = {classification trees,fault-,principal,prone modules,s-plus,software metrics,software quality},
pages = {198--209},
title = {{Improving Tree-Based Models of Software Quality with Principal Components Analysis}},
year = {2000}
}
@inproceedings{Kim2007,
author = {Kim, Sunghun and Zimmermann, Thomas and {Whitehead Jr.}, E. James and Zeller, Andreas},
booktitle = {Proceedings of the 29th International Conference on Software Engineering (ICSE'07)},
doi = {10.1109/ICSE.2007.66},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the 29th International Conference on Software Engineering (ICSE'07)/Predicting Faults from Cached History.pdf:pdf},
isbn = {0-7695-2828-7},
issn = {0270-5257},
month = may,
pages = {489--498},
title = {{Predicting Faults from Cached History}},
year = {2007}
}
@article{Klas2008,
abstract = {Planning quality assurance (QA) activities in a systematic way and controlling their execution are challenging tasks for companies that develop software or software-intensive systems. Both require estimation capabilities regarding the effectiveness of the applied QA techniques and the defect content of the checked artifacts. Existing approaches for these purposes need extensive measurement data from his-torical projects. Due to the fact that many companies do not collect enough data for applying these approaches (es-pecially for the early project lifecycle), they typically base their QA planning and controlling solely on expert opinion. This article presents a hybrid method that combines commonly available measurement data and context-specific expert knowledge. To evaluate the methodpsilas applicability and usefulness, we conducted a case study in the context of independent verification and validation activities for critical software in the space domain. A hybrid defect content and effectiveness model was developed for the software requirements analysis phase and evaluated with available legacy data. One major result is that the hybrid model provides improved estimation accuracy when compared to applicable models based solely on data. The mean magni-tude of relative error (MMRE) determined by cross-validation is 29.6\% compared to 76.5\% obtained by the most accurate data-based model.},
author = {Kl\"{a}s, Michael and Nakao, Haruka and Elberzhager, Frank and M\"{u}nch, J\"{u}rgen},
doi = {10.1109/ISSRE.2008.43},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings - International Symposium on Software Reliability Engineering, ISSRE/Predicting defect content and quality assurance effectiveness by combining expert judgment and defect data - A case study.pdf:pdf},
isbn = {9780769534053},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
pages = {17--26},
title = {{Predicting defect content and quality assurance effectiveness by combining expert judgment and defect data - A case study}},
year = {2008}
}
@article{Knab2006,
abstract = {With the advent of open source software repositories the data available for defect prediction in source files increased tremendously. Although traditional statistics turned out to derive reasonable results the sheer amount of data and the problem context of defect prediction demand sophisticated analysis such as provided by current data mining and machine learning techniques.In this work we focus on defect density prediction and present an approach that applies a decision tree learner on evolution data extracted from the Mozilla open source web browser project. The evolution data includes different source code, modification, and defect measures computed from seven recent Mozilla releases. Among the modification measures we also take into account the change coupling, a measure for the number of change-dependencies between source files. The main reason for choosing decision tree learners, instead of for example neural nets, was the goal of finding underlying rules which can be easily interpreted by humans. To find these rules, we set up a number of experiments to test common hypotheses regarding defects in software entities. Our experiments showed, that a simple tree learner can produce good results with various sets of input data.},
author = {Knab, Patrick and Pinzger, Martin and Bernstein, Abraham},
doi = {10.1145/1137983.1138012},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the 2006 international workshop on Mining software repositories - MSR '06/Predicting defect densities in source code files with decision tree learners.pdf:pdf},
isbn = {159593085X},
issn = {02705257},
journal = {Proceedings of the 2006 international workshop on Mining software repositories - MSR '06},
keywords = {data mining,decision tree learner,defect prediction},
pages = {119},
title = {{Predicting defect densities in source code files with decision tree learners}},
url = {http://portal.acm.org/citation.cfm?doid=1137983.1138012},
year = {2006}
}
@article{Koru2005,
abstract = {The article focuses on building effective defect-prediction models in practice. Defective software modules cause software failures, increase development and maintenance costs, and decrease customer satisfaction. Effective defect prediction models can help developers focus quality assurance activities on defect-prone modules and thus improve software quality by using resources more efficiently. Successfully predicting defect-prone software modules can help developers improve product quality by focusing quality assurance activities on those modules. Emerging repositories of publicly available software engineering data sets support research in this area by providing static measures and defect data that developers can use to build prediction models and test their effectiveness. Stratifying the U.S. National Aeronautics and Space Administration data sets from the predictor models in software engineering repository according to module size showed improved prediction performance in the sub-sets that included larger modules.},
author = {Koru, a G\"{u}neş and Liu, Hongfang},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/IEEE Software/Building Effective Defect-Prediction Models in Practice.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {APPLICATION software,CUSTOMER satisfaction,ELECTRONIC data processing,PREDICTION models,QUALITY assurance,SOFTWARE engineering},
number = {December},
pages = {23--29},
title = {{Building Effective Defect-Prediction Models in Practice}},
url = {http://search.ebscohost.com/login.aspx?direct=true\&db=bth\&AN=18794543\&site=ehost-live},
volume = {22},
year = {2005}
}
@article{Koru2008,
abstract = {In this study, we investigated the functional form of the size-defect$\backslash$nrelationship for software modules through replicated studies conducted$\backslash$non ten open-source products. We consistently observed a power-law$\backslash$nrelationship where defect proneness increases at a slower rate compared$\backslash$nto size. Therefore, smaller modules are proportionally more defect$\backslash$nprone. We externally validated the application of our results for$\backslash$ntwo commercial systems. Given limited and fixed resources for code$\backslash$ninspections, there would be an impressive improvement in the cost-effectiveness,$\backslash$nas much as 341\% in one of the systems, if a smallest-first strategy$\backslash$nwere preferred over a largest-first one. The consistent results obtained$\backslash$nin this study led us to state a theory of relative defect proneness$\backslash$n(RDP): In large-scale software systems, smaller modules will be proportionally$\backslash$nmore defect-prone compared to larger ones. We suggest that practitioners$\backslash$nconsider our results and give higher priority to smaller modules$\backslash$nin their focused quality assurance efforts.},
author = {Koru, a. G. and {El Emam}, Khaled and Zhang, Dongsong and Liu, Hongfang and Mathew, Divya},
doi = {10.1007/s10664-008-9080-x},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Empirical Software Engineering/Theory of relative defect proneness Replicated studies on the functional form of the size-defect relationship.pdf:pdf},
issn = {13823256},
journal = {Empirical Software Engineering},
keywords = {Open-source software,Planning for software quality assurance,Size-defect relationship,Software inspections,Software metrics,Software reviews,Software science,Software testing},
pages = {473--498},
title = {{Theory of relative defect proneness: Replicated studies on the functional form of the size-defect relationship}},
volume = {13},
year = {2008}
}
@article{Koru2007,
abstract = {Quality is becoming increasingly important with the continuous adoption of open-source software. Previous research has found that there is generally a positive relationship between module size and defect proneness. Therefore, in open-source software development, it is important to monitor module size and understand its impact on defect proneness. However, traditional approaches to quality modeling, which measure specific system snapshots and obtain future defect counts, are not well suited because open-source modules usually evolve and their size changes over time. In this study, we used Cox proportional hazards modeling with recurrent events to study the effect of class size on defect-proneness in the Mozilla product. We found that the effect of size was significant, and we quantified this effect on defect proneness.},
author = {Koru, a. G. and Zhang, Dongsong and Liu, Hongfang},
doi = {10.1109/ICSECOMPANION.2007.54},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings - International Conference on Software Engineering/Modeling the effect of size on defect proneness for open-source software.pdf:pdf},
isbn = {0769528929},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {115--124},
title = {{Modeling the effect of size on defect proneness for open-source software}},
year = {2007}
}
@article{Koru2005,
author = {Koru, a. G\"{u}nes and Liu, Hongfang},
doi = {10.1145/1082983.1083172},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/ACM SIGSOFT Software Engineering Notes/An investigation of the effect of module size on defect prediction using static measures.pdf:pdf},
isbn = {1595931252},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {defect prediction,els,prediction mod-,software metrics,software quality management,static measures},
pages = {1},
title = {{An investigation of the effect of module size on defect prediction using static measures}},
volume = {30},
year = {2005}
}
@article{Kpodjedo2011,
abstract = {Testing is the most widely adopted practice to ensure software quality. However, this activity is often a compromise between the available resources and software quality. In object-oriented development, testing effort should be focused on defective classes. Unfortunately, identifying those classes is a challenging and difficult activity on which many metrics, techniques, and models have been tried. In this paper, we investigate the usefulness of elementary design evolution metrics to identify defective classes. The metrics include the numbers of added, deleted, and modified attributes, methods, and relations. The metrics are used to recommend a ranked list of classes likely to contain defects for a system. They are compared to Chidamber and Kemerer’s metrics on several versions of Rhino and of ArgoUML. Further comparison is conducted with the complexity metrics computed by Zimmermann et al. on several releases of Eclipse. The comparisons are made according to three criteria: presence of defects, number of defects, and defect density in the top-ranked classes. They show that the design evolution metrics, when used in conjunction with known metrics, improve the identification of defective classes. In addition, they show that the design evolution metrics make significantly better predictions of defect density than other metrics and, thus, can help in reducing the testing effort by focusing test activity on a reduced volume of code.},
author = {Kpodjedo, Segla and Ricca, Filippo and Galinier, Philippe and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
doi = {10.1007/s10664-010-9151-7},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Empirical Software Engineering/Design evolution metrics for defect prediction in object oriented systems.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
pages = {141--175},
title = {{Design evolution metrics for defect prediction in object oriented systems}},
volume = {16},
year = {2011}
}
@article{Layman2008,
abstract = {Code churn, the amount of code change taking place within a software unit over time, has been correlated with fault-proneness in software systems. We investigate the use of code churn and static metrics collected at regular time intervals during the development cycle to predict faults in an iterative, in-process manner. We collected 159 churn and structure metrics from six, four-month snapshots of a 1 million LOC Microsoft product. The number of software faults fixed during each period is recorded per binary module. Using stepwise logistic regression, we create a prediction model to identify fault-prone binaries using three parameters: code churn (the number of new and changed blocks); class Fan In and class Fan Out (normalized by lines of code). The iteratively-built model is 80.0\% accurate at predicting faultprone and non-fault-prone binaries. These fault-prediction models have the advantage of allowing the engineers to observe how their fault-prediction profile evolves over time. Copyright 2008 ACM.},
author = {Layman, Lucas and Kudrjavets, Gunnar and Nagappan, Nachiappan},
doi = {10.1145/1414004.1414038},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '08/Iterative identification of fault-prone binaries using in-process metrics.pdf:pdf},
isbn = {9781595939715},
journal = {Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '08},
pages = {206},
title = {{Iterative identification of fault-prone binaries using in-process metrics}},
url = {http://portal.acm.org/citation.cfm?doid=1414004.1414038},
year = {2008}
}
@article{Lei2003,
abstract = { Sample statistics and model parameters can be used to infer the properties, or characteristics, of the underlying population in typical data-analytic situations. Confidence intervals can provide an estimate of the range within which the true value of the statistic lies. A narrow confidence interval implies low variability of the statistic, justifying a strong conclusion made from the analysis. Many statistics used in software metrics analysis do not come with theoretical formulas to allow such accuracy assessment. The Efron bootstrap statistical analysis appears to address this weakness. In this paper, we present an empirical analysis of the reliability of several Efron nonparametric bootstrap methods in assessing the accuracy of sample statistics in the context of software metrics. A brief review on the basic concept of various methods available for the estimation of statistical errors is provided, with the stated advantages of the Efron bootstrap discussed. Validations of several different bootstrap algorithms are performed across basic software metrics in both simulated and industrial software engineering contexts. It was found that the 90 percent confidence intervals for mean, median, and Spearman correlation coefficients were accurately predicted. The 90 percent confidence intervals for the variance and Pearson correlation coefficients were typically underestimated (60-70 percent confidence interval), and those for skewness and kurtosis overestimated (98-100 percent confidence interval). It was found that the Bias-corrected and accelerated bootstrap approach gave the most consistent confidence intervals, but its accuracy depended on the metric examined. A method for correcting the under-/ overestimation of bootstrap confidence intervals for small data sets is suggested, but the success of the approach was found to be inconsistent across the tested metrics.},
author = {Lei, Skylar and Smith, Michael R.},
doi = {10.1109/TSE.2003.1245301},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/IEEE Transactions on Software Engineering/Evaluation of Several Nonparametric Bootstrap Methods to Estimate Confidence Intervals for Software Metrics.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Confidence intervals,Correction of possible biases in Efron bootstrap e,Efron bootstrap,Software metrics},
number = {11},
pages = {996--1004},
title = {{Evaluation of Several Nonparametric Bootstrap Methods to Estimate Confidence Intervals for Software Metrics}},
volume = {29},
year = {2003}
}
@article{Li2007,
author = {Li, Zhan and Reformat, Marek},
doi = {10.1109/IRI.2007.4296695},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/2007 IEEE International Conference on Information Reuse and Integration/A practical method for the software fault-prediction.pdf:pdf},
isbn = {1-4244-1499-7},
journal = {2007 IEEE International Conference on Information Reuse and Integration},
pages = {659--666},
title = {{A practical method for the software fault-prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4296695},
year = {2007}
}
@article{Ma2006,
author = {Ma, Yan and Guo, L and Cukic, B},
doi = {10.4018/978-1-59140-941-1.ch010},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Advances in machine learning application \ldots/A statistical framework for the prediction of fault-proneness.pdf:pdf},
journal = {Advances in machine learning application \ldots},
pages = {1--26},
title = {{A statistical framework for the prediction of fault-proneness}},
url = {http://books.google.com/books?hl=en\&lr=\&id=5TyWZxUTkogC\&oi=fnd\&pg=PA237\&dq=A+Statistical+Framework+for+the+Prediction+of+Fault-Proneness\&ots=QJiC04or4Q\&sig=E\_kdJaz5wO6z2xf83ifC7kS6Jss},
year = {2006}
}
@inproceedings{Mahaweerawat2002,
author = {Mahaweerawat, Atchara and Sophatsathit, Peraphon and Lursinsap, Chidchanok},
booktitle = {Proc. Int’l Conf. Intelligent Technologies},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proc. Int’l Conf. Intelligent Technologies/Software Fault Prediction Using Fuzzy Clustering and Radial-Basis Function Network.pdf:pdf},
title = {{Software Fault Prediction Using Fuzzy Clustering and Radial-Basis Function Network}},
volume = {5},
year = {2002}
}
@article{Marcus2008,
abstract = {High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.},
author = {Marcus, Andrian and Poshyvanyk, Denys and Ferenc, Rudolf},
doi = {10.1109/TSE.2007.70768},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/IEEE Transactions on Software Engineering/Using the conceptual cohesion of classes for fault prediction in object-oriented systems.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Fault prediction,Fault proneness,Information retrieval,Latent Semantic Indexing,Program comprehension,Software cohesion,Textual coherence},
number = {2},
pages = {287--300},
title = {{Using the conceptual cohesion of classes for fault prediction in object-oriented systems}},
volume = {34},
year = {2008}
}
@article{Menzies2002,
author = {Menzies, Tim},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Unknown/Data Sniffing - Monitoring of Machine Learning for Online Adaptive Systems ∗.pdf:pdf},
title = {{Data Sniffing - Monitoring of Machine Learning for Online Adaptive Systems ∗}},
year = {2002}
}
@article{Menzies2013a,
author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
doi = {10.1109/TSE.2012.83},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/IEEE Transactions on Software Engineering/Local versus Global Lessons for Defect Prediction and Effort Estimation(2).pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = jun,
number = {6},
pages = {822--834},
title = {{Local versus Global Lessons for Defect Prediction and Effort Estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6363444},
volume = {39},
year = {2013}
}
@article{Menzies2012,
author = {Menzies, Tim and Shepperd, Martin},
doi = {10.1007/s10664-011-9193-5},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Empirical Software Engineering/Special issue on repeatable results in software engineering prediction.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
number = {1-2},
pages = {1--17},
title = {{Special issue on repeatable results in software engineering prediction}},
url = {http://link.springer.com/10.1007/s10664-011-9193-5},
volume = {17},
year = {2012}
}
@article{Mertik2006,
abstract = {Current software quality estimation models often involve the use of supervised learning methods for building a software fault prediction models. In such models, dependent variable usually represents a software quality measurement indicating the quality of a module by risk-basked class membership, or the number of faults. Independent variables include various software metrics as McCabe, Error Count, Halstead, Line of Code, etc... In this paper we present the use of advanced tool for data mining called Multimethod on the case of building software fault prediction model. Multimethod combines different aspects of supervised learning methods in dynamical environment and therefore can improve accuracy of generated prediction model. We demonstrate the use Multimethod tool on the real data from the Metrics Data Project Data (MDP) Repository. Our preliminary empirical results show promising potentials of this approach in predicting software quality in a software measurement and quality dataset.},
author = {Mertik, M. and Lenic, M. and Stiglic, G. and Kokol, P.},
doi = {10.1109/ICSEA.2006.261275},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/2006 International Conference on Software Engineering Advances (ICSEA'06)/Estimating Software Quality with Advanced Data Mining Techniques.pdf:pdf},
isbn = {0-7695-2703-5},
journal = {2006 International Conference on Software Engineering Advances (ICSEA'06)},
keywords = {Multimethod data mining,Software fault prediction models,Software quality,Supervised learning},
number = {c},
title = {{Estimating Software Quality with Advanced Data Mining Techniques}},
volume = {00},
year = {2006}
}
@article{Method,
author = {Method, Delta},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Bootstrapping and Applications.pdf:pdf},
pages = {41--51},
title = {{Bootstrapping and Applications}}
}
@inproceedings{MieThetThwin2002,
author = {{Mie Thet Thwin}, Mie and Tong-Seng, Quah},
booktitle = {Proceedings of the 9th International Conference on Neural Information Processing},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings of the 9th International Conference on Neural Information Processing/Application of Neural Network for Predicting Software Development Faults Using Object-Oriented Design Metrics.pdf:pdf},
pages = {2312 -- 2316},
title = {{Application of Neural Network for Predicting Software Development Faults Using Object-Oriented Design Metrics}},
volume = {5},
year = {2002}
}
@article{Misirli2014,
author = {Misirli, Ayse Tosun and Bener, Ayse Basar},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Unknown/Bayesian Networks For Evidence-Based Decision-Making in Software Engineering.pdf:pdf},
number = {6},
pages = {533--554},
title = {{Bayesian Networks For Evidence-Based Decision-Making in Software Engineering}},
volume = {40},
year = {2014}
}
@inproceedings{Mockus2010,
author = {Mockus, Audris},
booktitle = {Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering (FSE'10)},
doi = {10.1145/1882291.1882311},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering (FSE'10)/Organizational volatility and its effects on software defects.pdf:pdf},
isbn = {9781605587912},
keywords = {organizational volatility,software defects},
pages = {117--127},
title = {{Organizational volatility and its effects on software defects}},
year = {2010}
}
@article{Mockus2000,
author = {Mockus, Audris and Weiss, David M},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Bell Labs Technical Journal/Predicting Risk of Software Changes.pdf:pdf},
journal = {Bell Labs Technical Journal},
number = {6},
pages = {169--180},
title = {{Predicting Risk of Software Changes}},
volume = {5},
year = {2000}
}
@article{Mogensen2012,
author = {Mogensen, Ulla B and Gerds, Thomas A},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Unknown/Journal of Statistical Software.pdf:pdf},
keywords = {prediction error curves,r,random survival forest,survival prediction},
number = {11},
title = {{Journal of Statistical Software}},
volume = {50},
year = {2012}
}
@article{Moreno-torres2012,
author = {Moreno-torres, Jose Garc\'{\i}a and S\'{a}ez, Jos\'{e} A and Herrera, Francisco},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Unknown/Study on the Impact of Partition-Induced Dataset Shift on k -fold Cross-Validation.pdf:pdf},
number = {8},
pages = {1304--1312},
title = {{Study on the Impact of Partition-Induced Dataset Shift on k -fold Cross-Validation}},
volume = {23},
year = {2012}
}
@inproceedings{Morgenthaler,
author = {Morgenthaler, J David and Penix, John},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/software development tools Using Static Analysis to Find Bugs.pdf:pdf},
title = {{software development tools Using Static Analysis to Find Bugs}}
}
@article{Moser2008a,
address = {New York, New York, USA},
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
doi = {10.1145/1368088.1368114},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 13th international conference on Software engineering - ICSE '08/A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction.pdf:pdf},
isbn = {9781605580791},
journal = {Proceedings of the 13th international conference on Software engineering - ICSE '08},
keywords = {cost-sensitive classification,defect prediction,software metrics},
pages = {181},
publisher = {ACM Press},
title = {{A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1368088.1368114},
year = {2008}
}
@article{Myrtveit2005,
author = {Myrtveit, I. and Stensrud, E. and Shepperd, M.},
doi = {10.1109/TSE.2005.58},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/IEEE Transactions on Software Engineering/Reliability and validity in comparative studies of software prediction models.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {5},
pages = {380--391},
title = {{Reliability and validity in comparative studies of software prediction models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1438374},
volume = {31},
year = {2005}
}
@article{Nagappan2006,
abstract = {What is it that makes software fail? In an empirical study of the post-release defect history of five Microsoft software systems, we found that failure-prone software entities are statistically correlated with code complexity measures. However, there is no single set of complexity metrics that could act as a universally best defect predictor. Using principal component analysis on the code metrics, we built regression models that accurately predict the likelihood of post-release defects for new entities. The approach can easily be generalized to arbitrary projects; in particular, predictors obtained from one project can also be significant for new, similar projects.},
author = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
doi = {10.1145/1134285.1134349},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the 28th international conference on Software engineering/Mining metrics to predict component failures.pdf:pdf},
isbn = {1-59593-375-1},
issn = {02705257},
journal = {Proceedings of the 28th international conference on Software engineering},
keywords = {bug database,complexity metrics,empirical study,principal component analysis,regression model},
pages = {452--461},
title = {{Mining metrics to predict component failures}},
url = {http://doi.acm.org/10.1145/1134285.1134349},
year = {2006}
}
@book{Nam2014,
author = {Nam, Jaechang},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Unknown/Survey on Software Defect Prediction.pdf:pdf},
isbn = {2222222222222},
pages = {1--34},
title = {{Survey on Software Defect Prediction}},
year = {2014}
}
@article{Neufelder2000,
abstract = {This author has mathematically correlated specific development
practices to defect density and probability of on time delivery. She
summarizes the results of this ongoing study that has evolved into a
software prediction modeling and management technique. She has collected
data from 45 organizations developing software primarily for equipment
or electronic systems. Of these 45 organizations, complete and unbiased
delivered defect data and actual schedule delivery data was available
for 17 organizations. She presents the mathematical correlation between
the practices employed by these organizations and defect density. This
correlation can and is used to: predict defect density; and improve
software development practices for the best return on investment},
author = {a.M. Neufelder},
doi = {10.1109/ISSRE.2000.885868},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000/How to measure the impact of specific development practices on fielded defect density.pdf:pdf},
isbn = {0-7695-0807-3},
issn = {1071-9458},
journal = {Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000},
pages = {148--159},
title = {{How to measure the impact of specific development practices on
fielded defect density}},
year = {2000}
}
@inproceedings{Nguyen2012,
annote = {        From Duplicate 1 ( 
        
        
          Multi-layered approach for recovering links between bug reports and fixes
        
        
         - Nguyen, Anh Tuan; Nguyen, Tung Thanh; Nguyen, Hoan Anh; Nguyen, Tien N. )

        
        

        

        

      },
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE '12)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE '12)/Multi-layered approach for recovering links between bug reports and fixes.pdf:pdf},
isbn = {9781450316149},
keywords = {bug-to-fix links,bugs,fixes,mining software repository},
pages = {1--11},
title = {{Multi-layered approach for recovering links between bug reports and fixes}},
year = {2012}
}
@inproceedings{Nguyen2010,
author = {Nguyen, Thanh H.D. and Adams, Bram and Hassan, Ahmed E.},
booktitle = {Proceedings of the 17th Working Conference on Reverse Engineering (WCRE'10)},
doi = {10.1109/WCRE.2010.37},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the 17th Working Conference on Reverse Engineering (WCRE'10)/A Case Study of Bias in Bug-Fix Datasets.pdf:pdf},
isbn = {978-1-4244-8911-4},
keywords = {-sample,bias,bug-fix,data quality,prediction},
month = oct,
pages = {259--268},
publisher = {Ieee},
title = {{A Case Study of Bias in Bug-Fix Datasets}},
year = {2010}
}
@article{Nugroho2010,
abstract = {Identifying and fixing software problems before implementation are believed to be much cheaper than after implementation. Hence, it follows that predicting fault-proneness of software modules based on early software artifacts like software design is beneficial as it allows software engineers to perform early predictions to anticipate and avoid faults early enough. Taking this motivation into consideration, in this paper we evaluate the usefulness of UML design metrics to predict fault-proneness of Java classes. We use historical data of a significant industrial Java system to build and validate a UML-based prediction model. Based on the case study we have found that level of detail of messages and import coupling-both measured from sequence diagrams, are significant predictors of class fault-proneness. We also learn that the prediction model built exclusively using the UML design metrics demonstrates a better accuracy than the one built exclusively using code metrics.},
author = {Nugroho, Ariadi and Chaudron, M. R V and Arisholm, Erik},
doi = {10.1109/MSR.2010.5463285},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings - International Conference on Software Engineering/Assessing UML design metrics for predicting fault-prone classes in a Java system.pdf:pdf},
isbn = {9781424468034},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Classification,Defect,Fault,Logistic regression,Prediction,Quality},
pages = {21--30},
title = {{Assessing UML design metrics for predicting fault-prone classes in a Java system}},
year = {2010}
}
@article{Olague2007,
abstract = {Empirical validation of software metrics suites to predict fault proneness in object-oriented (OO) components is essential to ensure their practical use in industrial settings. In this paper, we empirically validate three OO metrics suites for their ability to predict software quality in terms of fault-proneness: the Chidamber and Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design (QMOOD). Some CK class metrics have previously been shown to be good predictors of initial OO software quality. However, the other two suites have not been heavily validated except by their original proposers. Here, we explore the ability of these three metrics suites to predict fault-prone classes using defect data for six versions of Rhino, an open-source implementation of JavaScript written in Java. We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes. We also conclude that the class components in the MOOD metrics suite are not good class fault-proneness predictors. Analyzing multivariate binary logistic regression models across six Rhino versions indicates these models may be useful in assessing quality in OO classes produced using modern highly iterative or agile software development processes.},
author = {Olague, Hector M. and Etzkorn, Letha H. and Gholston, Sampson and Quattlebaum, Stephen},
doi = {10.1109/TSE.2007.1015},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/IEEE Transactions on Software Engineering/Empirical validation of three software metrics suites to predict fault-proneness of object-oriented classes developed using highly Itera.pdf:pdf},
isbn = {00985589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Object-oriented metrics,Object-oriented software metrics,Software maintenance programming,Software quality metrics,Software reuse},
number = {6},
pages = {402--419},
pmid = {4181709},
title = {{Empirical validation of three software metrics suites to predict fault-proneness of object-oriented classes developed using highly Iterative or agile software development processes}},
volume = {33},
year = {2007}
}
@article{Oral2007,
author = {Oral, Atac Deniz and Bener, Ayse Basar},
doi = {10.1109/ISCIS.2007.4456886},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/2007 22nd International International Symposium on Computer and Information Sciences/Defect prediction for embedded software.pdf:pdf},
isbn = {978-1-4244-1363-8},
journal = {2007 22nd International International Symposium on Computer and Information Sciences},
keywords = {- software quality,embedded software,machine learning,metrics},
pages = {1--6},
title = {{Defect prediction for embedded software}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4456886},
year = {2007}
}
@article{Ostrand2004,
author = {Ostrand, T.J. and Weyuker, E.J. and Bell, R.M.},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/ACM SIGSOFT Software Engineering Notes/Where the bugs are.pdf:pdf},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {empirical study,fault-prone,gression model,prediction,re-,software faults,software testing},
pages = {86--96},
title = {{Where the bugs are}},
url = {http://dl.acm.org/citation.cfm?id=1007524},
volume = {29},
year = {2004}
}
@article{Ostrand2005b,
abstract = {The goal of this research is to allow software developers and testers to become aware of which files in the next release of a large software system are likely to contain the largest numbers of faults or the highest fault densities in the next release, thereby allowing testers to focus their efforts on the most fault-prone files. This is done by developing a negative binomial regression model to help predict characteristics of new releases of a software system, based on information collected about prior releases and the new release under development. The same prediction model was also used to allow a tester to select the files of a new release that collectively contain any desired percentage of the faults. The benefit of being able to make these sorts of predictions accurately should be clear: if we know where to look for bugs, we should be able to target our testing efforts there and, as a result, find problems more quickly and therefore more economically. Two case studies using large industrial software systems are summarized. The first study used seventeen consecutive releases of a large inventory system, representing more than four years of field exposure. The second study used nine releases of a service provisioning system with two years of field experience.},
author = {Ostrand, T.J. and Weyuker, E.J. and Bell, R.M.},
doi = {10.1109/RTCDC.2005.201644},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/2005 Richard Tapia Celebration of Diversity in Computing Conference/Locating where faults will be.pdf:pdf},
isbn = {1-59593-257-7},
journal = {2005 Richard Tapia Celebration of Diversity in Computing Conference},
keywords = {Experimentation,empirical study,fault-prone,prediction,regression model,software faults,software testing},
pages = {48--50},
title = {{Locating where faults will be}},
year = {2005}
}
@article{Ostrand2007,
abstract = {This research investigates ways of predicting which files would be most likely to contain large numbers of faults in the next release of a large industrial software system. Previous work involved making predictions using several different models ranging from a simple, fully-automatable model (the LOC model) to several different variants of a negative binomial regression model that were customized for the particular software system under study. Not surprisingly, the custom models invariably predicted faults more accurately than the simple model. However, development of customized models requires substantial time and analytic effort, as well as statistical expertise. We now introduce new, more sophisticated models that yield more accurate predictions than the earlier LOC model, but which nonetheless can be fully automated. We also extend our earlier research by presenting another large-scale empirical study of the value of these prediction models, using a new industrial software system over a nine year period.},
author = {Ostrand, Thomas J and Weyuker, Elaine J and Bell, Robert M},
doi = {10.1145/1273463.1273493},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the 2007 international symposium on Software testing and analysis - ISSTA '07/Automating algorithms for the identification of fault-prone files.pdf:pdf},
isbn = {9781595937346},
journal = {Proceedings of the 2007 international symposium on Software testing and analysis - ISSTA '07},
keywords = {fault-prone,prediction,regres-,software faults},
pages = {219},
title = {{Automating algorithms for the identification of fault-prone files}},
url = {http://portal.acm.org/citation.cfm?doid=1273463.1273493},
year = {2007}
}
@article{Ostrand2010,
abstract = {Background: Previous research has provided evidence that a combination of static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions. Aims: We investigate whether files in a large system that are modified by an individual developer consistently con- tain either more or fewer faults than the average of all files in the system. The goal of the investigation is to deter- mine whether information about which particular developer modified a file is able to improve defect predictions. We also continue an earlier study to evaluate the use of counts of the number of developers who modified a file as predictors of the file’s future faultiness. Method: We analyzed change reports filed by 107 pro- grammers for 16 releases of a system with 1,400,000 LOC and 3100 files. A ”bug ratio” was defined for programmers, measuring the proportion of faulty files in release R out of all files modified by the programmer in release R-1. The study compares the bug ratios of individual programmers to the average bug ratio, and also assesses the consistency of the bug ratio across releases for individual programmers. Results: Bug ratios varied widely among all the program- mers, as well as for many individual programmers across all the releases that they participated in. We found a sta- tistically significant correlation between the bug ratios for programmers for the first half of changed files versus the ratios for the second half, indicating a measurable degree of persistence in the bug ratio. However, when the compu- tation was repeated with the bug ratio controlled not only by release, but also by file size, the correlation disappeared. In addition to the bug ratios, we confirmed that counts of the cumulative number of different developers changing a file over its lifetime can help to improve predictions, while other developer counts are not helpful. Conclusions: The results from this preliminary study indicate that adding information to a model about which particular developer modified a file is not likely to improve defect predictions. The study is limited to a single large sys- tem, and its resultsmay not hold more widely. The bug ratio is only one way of measuring the ”fault-proneness” of an in- dividual programmer’s coding, and we intend to investigate other ways of evaluating bug introduction by individuals.},
author = {Ostrand, Thomas J and Weyuker, Elaine J and Bell, Robert M and Avenue, Park and Park, Florham},
doi = {10.1145/1868328.1868357},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Promise 2010/Programmer-based Fault Prediction.pdf:pdf},
isbn = {9781450304047},
journal = {Promise 2010},
keywords = {bug ratio,empirical study,fault-prone,predic-,regression model,software faults,tion},
pages = {1--10},
title = {{Programmer-based Fault Prediction}},
year = {2010}
}
@article{Padberg2004,
abstract = { We view the problem of estimating the defect content of a document after an inspection as a machine learning problem: The goal is to learn from empirical data the relationship between certain observable features of an inspection (such as the total number of different defects detected) and the number of defects actually contained in the document. We show that some features can carry significant nonlinear information about the defect content. Therefore, we use a nonlinear regression technique, neural networks, to solve the learning problem. To select the best among all neural networks trained on a given data set, one usually reserves part of the data set for later cross-validation; in contrast, we use a technique which leaves the full data set for training. This is an advantage when the data set is small. We validate our approach on a known empirical inspection data set. For that benchmark, our novel approach clearly outperforms both linear regression and the current standard methods in software engineering for estimating the defect content, such as capture-recapture. The validation also shows that our machine learning approach can be successful even when the empirical inspection data set is small.},
author = {Padberg, Frank and Ragg, Thomas and Schoknecht, Ralf},
doi = {10.1109/TSE.2004.1265733},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/IEEE Transactions on Software Engineering/Using machine learning for estimating the defect content after an inspection.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Defect content estimation,Empirical methods,Neural networks,Nonlinear regression,Software inspections},
number = {1},
pages = {17--28},
title = {{Using machine learning for estimating the defect content after an inspection}},
volume = {30},
year = {2004}
}
@article{Pai2007,
abstract = {We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.},
author = {Pai, G.J. and Dugan, J.B.},
doi = {10.1109/TSE.2007.70722},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/IEEE Transactions on Software Engineering/Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Bayesian analysis,Bayesian networks,defects,fault proneness,metrics,object-oriented,regression,software quality},
number = {10},
pages = {675--686},
title = {{Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods}},
volume = {33},
year = {2007}
}
@article{Pelayo2007,
abstract = {Due to the tremendous complexity and sophistication of software, improving software reliability is an enormously difficult task. We study the software defect prediction problem, which focuses on predicting which modules will experience a failure during operation. Numerous studies have applied machine learning to software defect prediction; however, skewness in defect-prediction datasets usually undermines the learning algorithms. The resulting classifiers will often never predict the faulty minority class. This problem is well known in machine learning and is often referred to as learning from unbalanced datasets. We examine stratification, a widely used technique for learning unbalanced data that has received little attention in software defect prediction. Our experiments are focused on the SMOTE technique, which is a method of over-sampling minority-class examples. Our goal is to determine if SMOTE can improve recognition of defect-prone modules, and at what cost. Our experiments demonstrate that after SMOTE resampling, we have a more balanced classification. We found an improvement of at least 23\% in the average geometric mean classification accuracy on four benchmark datasets.},
author = {Pelayo, Lourdes and Dick, Scott},
doi = {10.1109/NAFIPS.2007.383813},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Annual Conference of the North American Fuzzy Information Processing Society - NAFIPS/Applying novel resampling strategies to software defect prediction.pdf:pdf},
isbn = {1424412145},
journal = {Annual Conference of the North American Fuzzy Information Processing Society - NAFIPS},
pages = {69--72},
title = {{Applying novel resampling strategies to software defect prediction}},
year = {2007}
}
@article{Peters2012,
author = {Peters, Fayola and Menzies, Tim},
doi = {10.1109/ICSE.2012.6227194},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/2012 34th International Conference on Software Engineering (ICSE)/Privacy and utility for defect prediction Experiments with MORPH.pdf:pdf},
isbn = {978-1-4673-1067-3},
journal = {2012 34th International Conference on Software Engineering (ICSE)},
keywords = {-privacy,data mining,defect prediction},
month = jun,
pages = {189--199},
publisher = {Ieee},
title = {{Privacy and utility for defect prediction: Experiments with MORPH}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6227194},
year = {2012}
}
@article{Peters2013,
author = {Peters, Fayola and Menzies, Tim and Gong, Liang and Zhang, Hongyu},
doi = {10.1109/TSE.2013.6},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/IEEE Transactions on Software Engineering/Balancing Privacy and Utility in Cross-Company Defect Prediction.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {8},
pages = {1054--1068},
title = {{Balancing Privacy and Utility in Cross-Company Defect Prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6419712},
volume = {39},
year = {2013}
}
@inproceedings{Peters2013a,
author = {Peters, Fayola and Menzies, Tim and Marcus, Andrian},
booktitle = {Proceedings of the Working Conference on Mining Software Repositories (MSR'13)},
doi = {10.1109/MSR.2013.6624057},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Proceedings of the Working Conference on Mining Software Repositories (MSR'13)/Better cross company defect prediction.pdf:pdf},
isbn = {978-1-4673-2936-1},
pages = {409--418},
title = {{Better cross company defect prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6624057},
year = {2013}
}
@article{Pighin2003,
abstract = { This work is based on the idea of analyzing the behavior all over the life-cycle of source files having a high number of faults at their first release. In terms of predictability, our study helps to understand if files that are faulty in their first release tend to remain faulty in later releases, and investigates the ways to assure a higher reliability to the faultiest programs, testing them carefully or lowering the complexity of their structure. The purpose of this paper is to verify empirically our hypothesis, through an experimental analysis on two different projects, and to find causes observing the structure of the faulty files. As a conclusion, we can say that the number of faults at the first release of source files is an early and significant index of its expected defect rate and reliability.},
author = {Pighin, M. and Marzona, a.},
doi = {10.1109/ISESE.2003.1237979},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/2003 International Symposium on Empirical Software Engineering, 2003. ISESE 2003. Proceedings/An empirical analysis of fault persistence through software releases.pdf:pdf},
isbn = {0-7695-2002-2},
journal = {2003 International Symposium on Empirical Software Engineering, 2003. ISESE 2003. Proceedings.},
title = {{An empirical analysis of fault persistence through software releases}},
year = {2003}
}
@article{Radjenovic2013,
author = {Radjenovic, Danijel and Hericko, Marjan and Torkar, Richard and Zivkovic, Ales},
doi = {10.1016/j.infsof.2013.02.009},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Information and Software Technology/Software fault prediction metrics A systematic literature review.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
month = aug,
number = {8},
pages = {1397--1418},
title = {{Software fault prediction metrics: A systematic literature review}},
volume = {55},
year = {2013}
}
@inproceedings{Rahman2013,
author = {Rahman, Foyzur and Devanbu, Premkumar},
booktitle = {Proceedings of the International Conference on Software Engineering (ICSE'13)},
doi = {10.1109/ICSE.2013.6606589},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Proceedings of the International Conference on Software Engineering (ICSE'13)/How, and why, process metrics are better.pdf:pdf},
isbn = {978-1-4673-3076-3},
pages = {432--441},
title = {{How, and why, process metrics are better}},
year = {2013}
}
@inproceedings{Rahman2013a,
author = {Rahman, Foyzur and Herraiz, Israel and Posnett, Daryl},
booktitle = {Proceedings of the 9th Joint Meeting on Foundations of Software Engineering (FSE'13)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Proceedings of the 9th Joint Meeting on Foundations of Software Engineering (FSE'13)/Sample Size vs . Bias in Defect Prediction.pdf:pdf},
isbn = {9781450322379},
pages = {147--157},
title = {{Sample Size vs . Bias in Defect Prediction}},
year = {2013}
}
@article{Rana2009,
abstract = {Software science metrics (SSM) have been widely used as predictors of software defects. The usage of SSM is an effect of correlation of size and complexity metrics with number of defects. The SSM have been proposed keeping in view the procedural paradigm and structural nature of the programs. There has been a shift in software development paradigm from procedural to object oriented (OO) and SSM have been used as defect predictors of OO software as well. However, the effectiveness of SSM in OO software needs to be established. This paper investigates the effectiveness of use of SSM for: (a)classification of defect prone modules in OO software (b) prediction of number of defects. Various binary and numeric classification models have been applied on dataset kc1 with class level data to study the role of SSM. The results show that the removal of SSM from the set of independent variables does not significantly affect the classification of modules as defect prone and the prediction of number of defects. In most of the cases the accuracy and mean absolute error has improved when SSM were removed from the set of independent variables. The results thus highlight the ineffectiveness of use of SSM in defect prediction in OO software.},
author = {Rana, Zeeshan Ali and Shamail, Shafay and Awais, Mian Muhammad},
doi = {10.1109/WCSE.2009.92},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 WRI World Congress on Software Engineering, WCSE 2009/Ineffectiveness of use of software science metrics as predictors of defects in object oriented software.pdf:pdf},
isbn = {9780769535708},
journal = {2009 WRI World Congress on Software Engineering, WCSE 2009},
pages = {3--7},
title = {{Ineffectiveness of use of software science metrics as predictors of defects in object oriented software}},
volume = {4},
year = {2009}
}
@article{Raton2003,
author = {Raton, Boca},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/Unknown/Fault Prediction Modeling for Software Quality Estimation Comparing Commonly Used Techniques.pdf:pdf},
keywords = {cart,case-based reasoning,fault prediction,linear regression,multiple,neural networks,s-plus,software metrics,software quality prediction},
pages = {255--283},
title = {{Fault Prediction Modeling for Software Quality Estimation : Comparing Commonly Used Techniques}},
year = {2003}
}
@article{Ratzinger2008,
abstract = {This paper analyzes the influence of evolution activities such as refactoring on software defects. In a case study of five open source projects we used attributes of software evolution to predict defects in time periods of six months. We use versioning and issue tracking systems to extract 110 data mining features, which are separated into refactoring and non-refactoring related features. These features are used as input into classification algorithms that create prediction models for software defects. We found out that refactoring related features as well as non-refactoring related features lead to high quality prediction models. Additionally, we discovered that refactorings and defects have an inverse correlation: The number of software defects decreases, if the number of refactorings increased in the preceding time period. As a result, refactoring should be a significant part of both bug fixes and other evolutionary changes to reduce software defects. Copyright 2008 ACM.},
author = {Ratzinger, Jacek and Sigmund, Thomas and Gall, Harald C.},
doi = {10.1145/1370750.1370759},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings - International Conference on Software Engineering/On the relation of refactoring and software defects.pdf:pdf},
isbn = {9781605580241},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Mining software archives,Refactoring,Software evolution},
pages = {35--38},
title = {{On the relation of refactoring and software defects}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-57049131468\&partnerID=tZOtx3y1},
year = {2008}
}
@article{Ray,
author = {Ray, Baishakhi and Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar T},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/A Large Scale Study of Programming Languages and Code Quality in Github.pdf:pdf},
title = {{A Large Scale Study of Programming Languages and Code Quality in Github}}
}
@article{Rodriguez2007,
abstract = {At present, automated data collection tools allow us to collect large amounts of information, not without associated problems. This paper, we apply feature selection to several software engineering databases selecting attributes with the final aim that project managers can have a better global vision of the data they manage. In this paper, we make use of attribute selection techniques in different datasets publicly available (PROMISE repository), and different data mining algorithms for classification to defect faulty modules. The results show that in general, smaller datasets with less attributes maintain or improve the prediction capability with less attributes than the original datasets.},
author = {Rodr\'{\i}guez, D. and Ruiz, R. and Cuadrado-Gallego, J. and Aguilar-Ruiz, J.},
doi = {10.1109/IRI.2007.4296696},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/2007 IEEE International Conference on Information Reuse and Integration, IEEE IRI-2007/Detecting fault modules applying feature selection to classifiers.pdf:pdf},
isbn = {1424414997},
journal = {2007 IEEE International Conference on Information Reuse and Integration, IEEE IRI-2007},
pages = {667--672},
title = {{Detecting fault modules applying feature selection to classifiers}},
year = {2007}
}
@article{Roulston2007,
author = {Roulston, Mark S.},
doi = {10.1002/met.21},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Meteorological Applications/Performance targets and the Brier score.pdf:pdf},
issn = {13504827},
journal = {Meteorological Applications},
keywords = {accepted 17th april 2007,brier score,hedging,propriety,received 18th january 2007,revised 4th april 2007,risk-neutrality,targets,utility,verification},
month = jun,
number = {2},
pages = {185--194},
title = {{Performance targets and the Brier score}},
url = {http://doi.wiley.com/10.1002/met.21},
volume = {14},
year = {2007}
}
@article{Rufibach2010,
author = {Rufibach, Kaspar},
doi = {10.1016/j.jclinepi.2009.11.009},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Journal of Clinical Epidemiology/Use of Brier score to assess binary predictions.pdf:pdf},
isbn = {1878-5921 (Electronic)$\backslash$r0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
number = {8},
pages = {938--939},
pmid = {20189763},
publisher = {Elsevier Inc.},
title = {{Use of Brier score to assess binary predictions}},
url = {http://dx.doi.org/10.1016/j.jclinepi.2009.11.009},
volume = {63},
year = {2010}
}
@article{Sandhu2010,
abstract = {Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules using decision tree based Model in combination of K-means clustering as preprocessing technique. This approach has been tested with CM1 real time defect datasets of NASA software projects. The high accuracy of testing results show that the proposed Model can be used for the prediction of the fault proneness of software modules early in the software life cycle.},
author = {Sandhu, Parvinder S and Goel, Raman and Brar, Amanpreet S and Kaur, Jagdeep and Anand, Sanyam},
doi = {10.1109/ICCAE.2010.5451695},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)/A model for early prediction of faults in software systems.pdf:pdf},
isbn = {978-1-4244-5569-0},
journal = {2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)},
keywords = {Clustering,Decision Tree,K-means,K-means clustering,Learning systems,Neural networks,Predictive models,Process control,Software measurement,Software quality,Software systems,Software testing,Statistical analysis,Training data,clustering techniques,code metrics,early fault prediction,fault proneness,learning (artificial intelligence),machine learning methods,neural nets,neural network techniques,pattern clustering,quality estimations,requirement metrics,software component quality,software fault tolerance,software metrics,software quality,statistical analysis,statistical method},
pages = {281--285},
title = {{A model for early prediction of faults in software systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5451695},
volume = {4},
year = {2010}
}
@article{Seliya2005,
abstract = {Assuring whether the desired software quality and reliability is met for a project is as important as delivering it within scheduled budget and time. This is especially vital for high-assurance software systems where software failures can have severe consequences. To achieve the desired software quality, practitioners utilize software quality models to identify high-risk program modules: e.g., software quality classification models are built using training data consisting of software measurements and fault-proneness data from previous development experiences similar to the project currently under-development. However, various practical issues can limit availability of fault-proneness data for all modules in the training data, leading to the data consisting of many modules with no fault-proneness data, i.e., unlabeled data. To address this problem, we propose a novel semi-supervised clustering scheme for software quality analysis with limited fault-proneness data. It is a constraint-based semi-supervised clustering scheme based on the k-means algorithm. The proposed approach is investigated with software measurement data of two NASA software projects, JM1 and KC2. Empirical results validate the promise of our semi-supervised clustering technique for software quality modeling and analysis in the presence of limited defect data. Additionally, the approach provides some valuable insight into the characteristics of certain program modules that remain unlabeled subsequent to our semi-supervised clustering analysis.},
author = {Seliya, N. and Khoshgoftaar, T.M. and Zhong, S.},
doi = {10.1109/HASE.2005.4},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Ninth IEEE International Symposium on High-Assurance Systems Engineering (HASE'05)/Analyzing software quality with limited fault-proneness defect data.pdf:pdf},
isbn = {0-7695-2377-3},
issn = {1530-2059},
journal = {Ninth IEEE International Symposium on High-Assurance Systems Engineering (HASE'05)},
keywords = {k-means,semi-supervised clustering,software faults,software measurements,software quality},
pages = {89--98},
title = {{Analyzing software quality with limited fault-proneness defect data}},
year = {2005}
}
@book{Seni2010,
author = {Seni, Giovanni and Elder, John F.},
booktitle = {Synthesis Lectures on Data Mining and Knowledge Discovery},
doi = {10.2200/S00240ED1V01Y200912DMK002},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Synthesis Lectures on Data Mining and Knowledge Discovery/Ensemble Methods in Data Mining Improving Accuracy Through Combining Predictions.pdf:pdf},
isbn = {9781608452842},
issn = {2151-0067},
month = jan,
number = {1},
pages = {1--126},
title = {{Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00240ED1V01Y200912DMK002},
volume = {2},
year = {2010}
}
@article{Shepperd2014,
author = {Shepperd, Martin and Bowes, David and Hall, Tracy},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/IEEE Transactions on Software Engineering/Researcher Bias The Use of Machine Learning in Software Defect Prediction.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
number = {6},
pages = {603--616},
title = {{Researcher Bias : The Use of Machine Learning in Software Defect Prediction}},
volume = {40},
year = {2014}
}
@article{Shepperd2001,
author = {Shepperd, Martin and Kadoda, Gada},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2001/IEEE Transactions on Software Engineering/Comparing software prediction techniques using simulation.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
number = {11},
pages = {1014--1022},
title = {{Comparing software prediction techniques using simulation}},
volume = {27},
year = {2001}
}
@article{Shepperd2012,
author = {Shepperd, Martin and MacDonell, Steve},
doi = {10.1016/j.infsof.2011.12.008},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Information and Software Technology/Evaluating prediction systems in software project estimation.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {software engineering},
month = aug,
number = {8},
pages = {820--827},
publisher = {Elsevier B.V.},
title = {{Evaluating prediction systems in software project estimation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S095058491200002X},
volume = {54},
year = {2012}
}
@article{Song2006,
abstract = { Much current software defect prediction work focuses on the number of defects remaining in a software system. In this paper, we present association rule mining based methods to predict defect associations and defect correction effort. This is to help developers detect software defects and assist project managers in allocating testing resources more effectively. We applied the proposed methods to the SEL defect data consisting of more than 200 projects over more than 15 years. The results show that, for defect association prediction, the accuracy is very high and the false-negative rate is very low. Likewise, for the defect correction effort prediction, the accuracy for both defect isolation effort prediction and defect correction effort prediction are also high. We compared the defect correction effort prediction method with other types of methods - PART, C4.5, and Naive Bayes - and show that accuracy has been improved by at least 23 percent. We also evaluated the impact of support and confidence levels on prediction accuracy, false-negative rate, false-positive rate, and the number of rules. We found that higher support and confidence levels may not result in higher prediction accuracy, and a sufficient number of rules is a precondition for high prediction accuracy.},
author = {Song, Qinbao and Shepperd, Martin and Cartwright, Michelle and Mair, Carolyn},
doi = {10.1109/TSE.2006.1599417},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/IEEE Transactions on Software Engineering/Software defect association mining and defect correction effort prediction.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Defect association,Defect correction effort,Defect isolation effort,Software defect prediction},
number = {2},
pages = {69--82},
title = {{Software defect association mining and defect correction effort prediction}},
volume = {32},
year = {2006}
}
@article{Steyerberg2003,
author = {Steyerberg, Ewout W and Bleeker, Sacha E and Moll, Henri\"{e}tte a and Grobbee, Diederick E and Moons, Karel G.M},
doi = {10.1016/S0895-4356(03)00047-7},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/Journal of Clinical Epidemiology/Internal and external validation of predictive models A simulation study of bias and precision in small samples.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {bootstrap,external validation,internal validation,logistic regression,prediction models},
month = may,
number = {5},
pages = {441--447},
title = {{Internal and external validation of predictive models: A simulation study of bias and precision in small samples}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0895435603000477},
volume = {56},
year = {2003}
}
@article{Steyerberg2013,
author = {Steyerberg, Ewout W and Vickers, Andrew J and Cook, Nancy R and Gerds, Thomas and Obuchowski, Nancy and Pencina, Michael J and Kattan, Michael W},
doi = {10.1097/EDE.0b013e3181c30fb2.Assessing},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Epidemiology/Assessing the performance of prediction models a framework for some traditional and novel measures.pdf:pdf},
journal = {Epidemiology},
number = {1},
pages = {128--138},
title = {{Assessing the performance of prediction models: a framework for some traditional and novel measures}},
volume = {21},
year = {2010}
}
@inproceedings{Tantithamthavorn,
author = {Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, Ahmed E and Ihara, Akinori and Matsumoto, Kenichi},
booktitle = {Proceedings of the International Conference on Software Engineering},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2015/Proceedings of the International Conference on Software Engineering/The Impact of Mislabelling on the Performance and Interpretation of Defect Prediction Models.pdf:pdf},
pages = {To appear},
title = {{The Impact of Mislabelling on the Performance and Interpretation of Defect Prediction Models}},
year = {2015}
}
@article{Tarvo2013,
author = {Tarvo, Alexander and Nagappan, Nachiappan and Zimmermann, Thomas},
doi = {10.1109/ISSRE.2013.6698912},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)/Predicting risk of pre-release code changes with Checkinmentor.pdf:pdf},
isbn = {978-1-4799-2366-3},
journal = {2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)},
keywords = {an expert or a,change is estimated by,code branch,code change,code review,e,estimation of the change,g,group of experts,however,it relies on skills,often,risk,risk is subjective as,software metrics,the manual,the risk of a},
month = nov,
pages = {128--137},
publisher = {Ieee},
title = {{Predicting risk of pre-release code changes with Checkinmentor}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6698912},
year = {2013}
}
@article{Tomaszewski2006,
abstract = {In this paper we suggest and evaluate a method for predicting fault densities in modified classes early in the development process, i.e., before the modifications are implemented. We start by establishing methods that according to literature are considered the best for predicting fault densities of modified classes. We find that these methods can not be used until the system is implemented. We suggest our own methods, which are based on the same concept as the methods suggested in the literature, with the difference that our methods are applicable before the coding has started. We evaluate our methods using three large telecommunication systems produced by Ericsson. We find that our methods provide predictions that are of similar quality to the predictions based on metrics available after the code is implemented. Our predictions are, however, available much earlier in the development process. Therefore, they enable better planning of efficient fault prevention and fault detection activities},
author = {Tomaszewski, Piotr and Grahn, H\aa kan and Lundberg, Lars},
doi = {10.1109/ICSM.2006.6},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/IEEE International Conference on Software Maintenance, ICSM/A method for an accurate early prediction of faults in modified classes.pdf:pdf},
isbn = {0769523544},
issn = {1063-6773},
journal = {IEEE International Conference on Software Maintenance, ICSM},
pages = {487--495},
title = {{A method for an accurate early prediction of faults in modified classes}},
year = {2006}
}
@article{Tomaszewski2007,
abstract = {Statistical fault prediction models and expert estimations are two popular methods for deciding where to focus the fault detection efforts when the fault detection budget is limited. In this paper, we present a study in which we empirically compare the accuracy of fault prediction offered by statistical prediction models with the accuracy of expert estimations. The study is performed in an industrial setting. We invited eleven experts that are involved in the development of two large telecommunication systems. Our statistical prediction models are built on historical data describing one release of one of those systems. We compare the performance of these statistical fault prediction models with the performance of our experts when predicting faults in the latest releases of both systems. We show that the statistical methods clearly outperform the expert estimations. As the main reason for the superiority of the statistical models we see their ability to cope with large datasets. This makes it possible for statistical models to perform reliable predictions for all components in the system. This also enables prediction at a more fine-grain level, e.g., at the class instead of at the component level. We show that such a prediction is better both from the theoretical and from the practical perspective. © 2006 Elsevier Inc. All rights reserved.},
author = {Tomaszewski, Piotr and H\aa kansson, Jim and Grahn, H\aa kan and Lundberg, Lars},
doi = {10.1016/j.jss.2006.12.548},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Journal of Systems and Software/Statistical models vs. expert estimation for fault prediction in modified code - an industrial case study.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Evaluation,Expert estimation,Fault prediction},
pages = {1227--1238},
title = {{Statistical models vs. expert estimation for fault prediction in modified code - an industrial case study}},
volume = {80},
year = {2007}
}
@article{Tosun2008,
abstract = {In this paper, we present a defect prediction model based on ensemble of classifiers, which has not been fully explored so far in this type of research. We have conducted several experiments on public datasets. Our results reveal that ensemble of classifiers considerably improve the defect detection capability compared to Naive Bayes algorithm. We also conduct a cost-benefit analysis for our ensemble, where it turns out that it is enough to inspect 32\% of the code on the average, for detecting 76\% of the defects.},
author = {Tosun, Ayse},
doi = {10.1145/1414004.1414066},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Esem/Ensemble of Software Defect Predictors A Case Study.pdf:pdf},
isbn = {9781595939715},
journal = {Esem},
keywords = {Defect prediction,Ensemble of classifiers,Static code attributes.},
pages = {318--320},
title = {{Ensemble of Software Defect Predictors : A Case Study}},
year = {2008}
}
@article{Turhan2007,
abstract = {Defect prediction is important in order to reduce test times by allocating valuable test resources effectively. In this work, we propose a model using multivariate approaches in conjunction with Bayesian methods for defect predictions. The motivation behind using a multivariate approach is to overcome the independence assumption of univariate approaches about software attributes. Using Bayesian methods gives practitioners an idea about the defectiveness of software modules in a probabilistic framework rather than the hard classification methods such as decision trees. Furthermore the software attributes used in this work are chosen among the static code attributes that can easily be extracted from source code, which prevents human errors or subjectivity. These attributes are preprocessed with feature selection techniques to select the most relevant attributes for prediction. Finally we compared our proposed model with the best results reported so far on public datasets and we conclude that using multivariate approaches can perform better.},
author = {Turhan, Burak and Bener, Ayşe},
doi = {10.1109/QSIC.2007.4385500},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings - International Conference on Quality Software/A multivariate analysis of static code attributes for defect prediction.pdf:pdf},
isbn = {0769530354},
issn = {15506002},
journal = {Proceedings - International Conference on Quality Software},
keywords = {Defect prediction,Na\"{\i}ve bayes,Software metrics},
number = {Qsic},
pages = {231--237},
title = {{A multivariate analysis of static code attributes for defect prediction}},
year = {2007}
}
@article{Turhan2010,
author = {Turhan, Burak and Bener, Ayse and Menzies, Tim},
doi = {10.1007/978-3-642-13792-1\_11},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Regularities in learning defect predictors.pdf:pdf},
isbn = {3642137911},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Code metrics,Cross-company,Defect prediction,Software quality},
pages = {116--130},
title = {{Regularities in learning defect predictors}},
volume = {6156 LNCS},
year = {2010}
}
@article{Turhan1994,
author = {Turhan, Burak and Menzies, Tim},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Unknown/Practical Considerations in Deploying AI A Case Study within the Turkish Telecommunications Industry.pdf:pdf},
title = {{Practical Considerations in Deploying AI : A Case Study within the Turkish Telecommunications Industry}},
year = {2009}
}
@article{Turhan2013,
author = {Turhan, Burak and {Tosun Misirli}, Ayse and Bener, Ayse},
doi = {10.1016/j.infsof.2012.10.003},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Information and Software Technology/Empirical evaluation of the effects of mixed project data on learning defect predictors.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
number = {6},
pages = {1101--1118},
title = {{Empirical evaluation of the effects of mixed project data on learning defect predictors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584912002108},
volume = {55},
year = {2013}
}
@article{Wahyudin2008,
abstract = {The quality evaluation of open source software (OSS) products, e.g., defect estimation and prediction approaches of individual releases, gains importance with increasing OSS adoption in industry applications. Most empirical studies on the accuracy of defect prediction and software maintenance focus on product metrics as predictors that are available only when the product is finished. Only few prediction models consider information on the development process (project metrics) that seems relevant to quality improvement of the software product. In this paper, we investigate defect prediction with data from a family of widely used OSS projects based both on product and project metrics as well as on combinations of these metrics. Main results of data analysis are (a) a set of project metrics prior to product release that had strong correlation to potential defect growth between releases and (b) a combination of product and project metrics enables a more accurate defect prediction than the application of one single type of measurement. Thus, the combined application of project and product metrics can (a) improve the accuracy of defect prediction, (b) enable a better guidance of the release process from project management point of view, and (c) help identifying areas for product and process improvement.},
author = {Wahyudin, D. and Schatten, a. and Winkler, D. and a.M. Tjoa and Biffl, S.},
doi = {10.1109/SEAA.2008.36},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/2008 34th Euromicro Conference Software Engineering and Advanced Applications/Defect Prediction using Combined Product and Project Metrics - A Case Study from the Open Source Apache MyFaces Project Family.pdf:pdf},
isbn = {978-0-7695-3276-9},
issn = {1089-6503},
journal = {2008 34th Euromicro Conference Software Engineering and Advanced Applications},
keywords = {Defect Prediction,Open Source Software,Software Quality,Software metrics},
number = {March},
pages = {207--215},
title = {{Defect Prediction using Combined Product and Project Metrics - A Case Study from the Open Source "Apache" MyFaces Project Family}},
year = {2008}
}
@article{Wang2010,
author = {Wang, Wei and Ding, Xuan and Li, Chunping and Wang, Hui},
doi = {10.1109/CISE.2010.5676810},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 International Conference on Computational Intelligence and Software Engineering/A Novel Evaluation Method for Defect Prediction in Software Systems.pdf:pdf},
isbn = {978-1-4244-5391-7},
journal = {2010 International Conference on Computational Intelligence and Software Engineering},
keywords = {A Novel Evaluation Method for Defect Prediction in},
number = {90718022},
pages = {1--5},
title = {{A Novel Evaluation Method for Defect Prediction in Software Systems}},
year = {2010}
}
@article{Watanabe2008,
abstract = {An important step in predicting error prone modules in a project is to construct the prediction model by using training data of that project, but the resulting prediction model depends on the training data. Therefore it is difficult to apply the model to other projects. The training data consists of metrics data and bug data, and these data should be prepared for each project. Metrics data can be computed by using metric tools, but it is not so easy to collect bug data. In this paper, we try to reuse the generated prediction model. By using the metrics and bug data which are computed from C++ and Java projects, we have evaluated the possibility of applying the prediction model, which is generated based on one project, to other projects, and have proposed compensation techniques for applying to other projects. We showed the evaluation result based on open source projects. Copyright 2008 ACM.},
author = {Watanabe, Shinya and Kaiya, Haruhiko and Kaijiri, Kenji},
doi = {http://doi.acm.org/10.1145/1370788.1370794},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 4th international \ldots/Adapting a fault prediction model to allow inter languagereuse.pdf:pdf},
isbn = {9781605580364},
issn = {02705257},
journal = {Proceedings of the 4th international \ldots},
keywords = {error prone,inter language prediction,metrics,open source},
pages = {19--24},
title = {{Adapting a fault prediction model to allow inter languagereuse}},
url = {http://dl.acm.org/citation.cfm?id=1370794},
year = {2008}
}
@article{Wehrens2000,
author = {Wehrens, Ron and Putter, Hein and Buydens, Lutgarde M.C},
doi = {10.1016/S0169-7439(00)00102-7},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Chemometrics and Intelligent Laboratory Systems/The bootstrap a tutorial.pdf:pdf},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {bootstrap,chemometrics,confidence intervals,error estimate},
month = dec,
number = {1},
pages = {35--52},
title = {{The bootstrap: a tutorial}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169743900001027},
volume = {54},
year = {2000}
}
@article{Weyuker2007,
abstract = {We have been investigating different prediction models to identify which files of a large multi-release industrial software system are most likely to contain the largest numbers of faults in the next release. To make predictions we considered a number of different file characteristics and change information about the files, and have built fully- automatable models that do not require that the user have any statistical expertise. We now consider the effect of adding developer information as a prediction factor and assess the extent to which this affects the quality of the predictions.},
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
doi = {10.1109/PROMISE.2007.14},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings - ICSE 2007 Workshops Third International Workshop on Predictor Models in Software Engineering, PROMISE'07/Using developer information as a factor for fault prediction.pdf:pdf},
isbn = {0769529542},
journal = {Proceedings - ICSE 2007 Workshops: Third International Workshop on Predictor Models in Software Engineering, PROMISE'07},
pages = {1--7},
title = {{Using developer information as a factor for fault prediction}},
year = {2007}
}
@article{Wiens2008,
author = {Wiens, Trevor S. and Dale, Brenda C. and Boyce, Mark S. and Kershaw, G. Peter},
doi = {10.1016/j.ecolmodel.2007.10.005},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Ecological Modelling/Three way k-fold cross-validation of resource selection functions.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
month = apr,
number = {3-4},
pages = {244--255},
title = {{Three way k-fold cross-validation of resource selection functions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0304380007005339},
volume = {212},
year = {2008}
}
@article{Wu2008,
abstract = {To date, very little published empirical data has reported on the quality and reliability aspects of commercial software systems. In this paper, we present quantitative empirical study results on faults and failures with four releases of SoftPM 29, ...},
author = {Wu, Shujian and Wang, Qing and Yang, Ye},
doi = {10.1145/1414004.1414037},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the Second ACMIEEE international symposium on Empirical software engineering and measurement ESEM 08/Quantitative analysis of faults and failures with multiple releases of softpm.pdf:pdf},
isbn = {9781595939715},
journal = {Proceedings of the Second ACMIEEE international symposium on Empirical software engineering and measurement ESEM 08},
keywords = {all part,empirical studies,granted without fee,hard copies,metrics,permission make digital,personal classroom use,provided copies,software faults failures,work},
pages = {198},
title = {{Quantitative analysis of faults and failures with multiple releases of softpm}},
url = {http://portal.acm.org/citation.cfm?doid=1414004.1414037},
year = {2008}
}
@article{Yang2007,
abstract = {For the management of a software development project, a software quality prediction model is very helpful since it can provide the management with useful information needed for decision-makings. Many software quality prediction models and techniques have been proposed and studied in the literature. Nevertheless, the complicated situations of software development process call for a more flexible model that can cater for all factors that have impact on the quality of the target software, including the characteristics of the software product, the characteristics of the development process and the operation conditions. In this paper, a software quality prediction model based on a fuzzy neural network is presented. The proposed model is a hybrid model of Artificial Neural Network (ANN) and Fuzzy Logic (FL), which exploits the advantages of ANN and FL while eliminating their limitations. The model exhibits some favorable features such as being able to deal with objective data collected in the software development process as well as knowledge/experiences obtained from experts or from similar projects, which is the main information that is available in the early phases of a software development process. Using this model, early prediction of software quality becomes feasible and the management can have knowledge of the quality of target software product as early as possible, which helps to identify design errors and avoid expensive rework. Experimental results show that for problems of small-to-medium scale, the proposed model can be easily trained and thus can be of practical use. © 2007 IEEE.},
author = {Yang, Bo and Yao, Lan and Huang, Hong-Zhong},
doi = {10.1109/ICNC.2007.347},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Third International Conference on Natural Computation (ICNC 2007)/Early Software Quality Prediction Based on a Fuzzy Neural Network Model.pdf:pdf},
isbn = {0-7695-2875-9},
journal = {Third International Conference on Natural Computation (ICNC 2007)},
number = {Icnc},
pages = {760--764},
title = {{Early Software Quality Prediction Based on a Fuzzy Neural Network Model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4344293},
volume = {2},
year = {2007}
}
@article{Yang2008,
abstract = {High assurance software requires extensive and expensive assessment. Many software organizations frequently do not allocate enough resources for software quality. We research the defect detectors focusing on the data sets of software defect prediction. A rough set model is presented to deal with the attributes of data sets of software defect prediction in this paper. Appling this model to the most famous public domain data set created by the NASA's metrics data program shows its splendid performance.},
author = {Yang, Weimin Yang Weimin and Li, Longshu Li Longshu},
doi = {10.1109/ICICTA.2008.76},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/2008 International Conference on Intelligent Computation Technology and Automation (ICICTA)/A Rough Set Model for Software Defect Prediction.pdf:pdf},
isbn = {978-0-7695-3357-5},
journal = {2008 International Conference on Intelligent Computation Technology and Automation (ICICTA)},
pages = {747--751},
title = {{A Rough Set Model for Software Defect Prediction}},
volume = {1},
year = {2008}
}
@article{Yuan2000,
author = {Yuan, Xiaohong and Khoshgoftaar, T.M. and Allen, E.B. and Ganesan, K.},
doi = {10.1109/ASSET.2000.888052},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Proceedings 3rd IEEE Symposium on Application-Specific Systems and Software Engineering Technology/An application of fuzzy clustering to software quality prediction.pdf:pdf},
isbn = {0-7695-0559-7},
journal = {Proceedings 3rd IEEE Symposium on Application-Specific Systems and Software Engineering Technology},
keywords = {clustering,fuzzy inference,fuzzy logic,software metrics,software quality,subtractive,telecom-},
number = {561},
pages = {85--90},
title = {{An application of fuzzy clustering to software quality prediction}},
year = {2000}
}
@article{Zhang2010,
abstract = {BACKGROUND: Defect predictors learned from static code measures can isolate code modules with a higher than usual probability of defects. AIMS: To improve those learners by focusing on the defect-rich portions of the training sets. METHOD: Defect data CM1, KC1, MC1, PC1, PC3 was separated into components. A subset of the projects (selected at random) were set aside for testing. Training sets were generated for a NaiveBayes classifier in two ways. In sample the dense treatment, the components with higher than the median number of defective modules were used for training. In the standard treatment, modules from any component were used for training. Both samples were run against the test set and evaluated using recall, probability of false alarm, and precision. In addition, under sampling and over sampling was performed on the defect data. Each method was repeated in a 10-by-10 cross-validation experiment. RESULTS: Prediction models learned from defect dense components out-performed standard method, under sampling, as well as over sampling. In statistical rankings based on recall, probability of false alarm, and precision, models learned from dense components won 4-5 times more often than any other method, and also lost the least amount of times. CONCLUSIONS: Given training data where most of the defects exist in small numbers of components, better defect predictors can be trained from the defect dense components.},
author = {Zhang, Hongyu and Nelson, Adam and Menzies, Tim},
doi = {10.1145/1868328.1868350},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the 6th International Conference on Predictive Models in Software Engineering - PROMISE '10/On the value of learning from defect dense components for software defect prediction.pdf:pdf},
isbn = {9781450304047},
journal = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering - PROMISE '10},
keywords = {ceiling ef-,defect dense components,defect prediction,sampling},
pages = {1},
title = {{On the value of learning from defect dense components for software defect prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1868328.1868350},
year = {2010}
}
@article{Zhou2006,
abstract = {In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes},
author = {Zhou, Yuming and Leung, Hareton},
doi = {10.1109/TSE.2006.102},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/IEEE Transactions on Software Engineering/Empirical analysis of object-oriented design metrics for predicting high and low severity faults.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Cross validation,Fault-proneness,Faults,Metrics,Object-oriented,Prediction},
number = {10},
pages = {771--789},
title = {{Empirical analysis of object-oriented design metrics for predicting high and low severity faults}},
volume = {32},
year = {2006}
}
@article{Zhou2010,
abstract = {Many studies use logistic regression models to investigate the ability of complexity metrics to predict fault-prone classes. However, it is not uncommon to see the inappropriate use of performance indictors such as odds ratio in previous studies. In particular, a recent study by Olague et al. uses the odds ratio associated with one unit increase in a metric to compare the relative magnitude of the associations between individual metrics and fault-proneness. In addition, the percents of concordant, discordant, and tied pairs are used to evaluate the predictive effectiveness of a univariate logistic regression model. Their results suggest that lesser known complexity metrics such as standard deviation method complexity (SDMC) and average method complexity (AMC) are better predictors than the two commonly used metrics: lines of code (LOC) and weighted method McCabe complexity (WMC). In this paper, however, we show that (1) the odds ratio associated with one standard deviation increase, rather than one unit increase, in a metric should be used to compare the relative magnitudes of the effects of individual metrics on fault-proneness. Otherwise, misleading results may be obtained; and that (2) the connection of the percents of concordant, discordant, and tied pairs with the predictive effectiveness of a univariate logistic regression model is false, as they indeed do not depend on the model. Furthermore, we use the data collected from three versions of Eclipse to re-examine the ability of complexity metrics to predict fault-proneness. Our experimental results reveal that: (1) many metrics exhibit moderate or almost moderate ability in discriminating between fault-prone and not fault-prone classes; (2) LOC and WMC are indeed better fault-proneness predictors than SDMC and AMC; and (3) the explanatory power of other complexity metrics in addition to LOC is limited. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Zhou, Yuming and Xu, Baowen and Leung, Hareton},
doi = {10.1016/j.jss.2009.11.704},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Journal of Systems and Software/On the ability of complexity metrics to predict fault-prone classes in object-oriented systems.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Complexity,Concordant pairs,Fault-prone,Logistic regression,Metrics,Odds ratio,Prediction},
number = {4},
pages = {660--674},
publisher = {Elsevier Inc.},
title = {{On the ability of complexity metrics to predict fault-prone classes in object-oriented systems}},
url = {http://dx.doi.org/10.1016/j.jss.2009.11.704},
volume = {83},
year = {2010}
}
@inproceedings{Zimmermann2007,
abstract = {In any software project, developers need to be aware of existing dependencies and how they affect their system. We investigated the architecture and dependencies of Windows Server 2003 to show how to use the complexity of a subsystem's dependency graph to predict the number of failures at statistically significant levels. Such estimations can help to allocate software quality resources to the parts of a product that need it most, and as early as possible.},
author = {Zimmermann, Thomas and Nagappan, Nachiappan},
booktitle = {Proceedings of the International Symposium on Software Reliability Engineering (ISSRE'07)},
doi = {10.1109/ISSRE.2007.19},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the International Symposium on Software Reliability Engineering (ISSRE'07)/Predicting subsystem failures using dependency graph complexities.pdf:pdf},
isbn = {0769530249},
issn = {10719458},
pages = {227--236},
title = {{Predicting subsystem failures using dependency graph complexities}},
year = {2007}
}
@inproceedings{Zimmermann2009,
author = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
booktitle = {Proceedings of the European Software Engineering Conference and the symposium on the Foundations of Software Engineering (FSE'09)},
doi = {10.1145/1595696.1595713},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the European Software Engineering Conference and the symposium on the Foundations of Software Engineering (FSE'09)/Cross-project defect prediction.pdf:pdf},
isbn = {9781605580012},
pages = {91--100},
title = {{Cross-project defect prediction}},
year = {2009}
}
@article{,
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Unknown/Bias , Variance , and MSE of Estimators.pdf:pdf},
number = {1},
pages = {1--2},
title = {{Bias , Variance , and MSE of Estimators}},
year = {2010}
}
@article{Abdelmoez2012,
author = {Abdelmoez, W and Kholief, Mohamed and Elsalmy, Fayrouz M},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Unknown/Bug Fix-Time Prediction Model Using Na\"{\i}ve Bayes Classifier.pdf:pdf},
isbn = {9781467328241},
number = {October},
pages = {13--15},
title = {{Bug Fix-Time Prediction Model Using Na\"{\i}ve Bayes Classifier}},
year = {2012}
}
@article{Afzal2008a,
abstract = {Software reliability growth modeling helps in deciding project release time and managing project resources. A large number of such models have been presented in the past. Due to the existence of many models, the models' inherent complexity, and their accompanyjng assumptions; the selection of suitable models becomes a challenging task. This paper presents empirical results of using genetic programming (GP) for modeling software reliability growth based on weekly fault count data of three different industrial projects. The goodness of fit (adaptability) and predictive accuracy of the evolved model is measured using five different measures in an attempt to present a fair evaluation. The results show that the GP evolved model has statistically significant goodness of fit and predictive accuracy.},
author = {Afzal, Wasif and Torkar, Richard and Feldt, Robert},
doi = {10.1109/INMIC.2008.4777762},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/2008 IEEE International Multitopic Conference/Prediction of Fault Count Data Using Genetic Programming.pdf:pdf},
isbn = {978-1-4244-2823-6},
journal = {2008 IEEE International Multitopic Conference},
keywords = {-genetic programming,fault count data,prediction},
pages = {349--356},
title = {{Prediction of Fault Count Data Using Genetic Programming}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4777762},
year = {2008}
}
@article{Andersson2007,
abstract = {To contribute to the body of empirical research on fault distributions during development of complex software systems, a replication of a study of Fenton and Ohlsson is conducted. The hypotheses from the original study are investigated using data taken from an environment that differs in terms of system size, project duration, and programming language. We have investigated four sets of hypotheses on data from three successive telecommunications projects: 1) the Pareto principle, that is, a small number of modules contain a majority of the faults (in the replication, the Pareto principle is confirmed), 2) fault persistence between test phases (a high fault incidence in function testing is shown to imply the same in system testing, as well as prerelease versus postrelease fault incidence), 3) the relation between number of faults and lines of code (the size relation from the original study could be neither confirmed nor disproved in the replication), and 4) fault density similarities across test phases and projects (in the replication study, fault densities are confirmed to be similar across projects). Through this replication study, we have contributed to what is known on fault distributions, which seem to be stable across environments.},
author = {Andersson, Carina and Runeson, Per},
doi = {10.1109/TSE.2007.1005},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/IEEE Transactions on Software Engineering/A replicated quantitative analysis of fault distributions in complex software systems.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Empirical research,Replication,Software fault distributions},
number = {5},
pages = {273--286},
pmid = {4160967},
title = {{A replicated quantitative analysis of fault distributions in complex software systems}},
volume = {33},
year = {2007}
}
@inproceedings{Antoniol,
author = {Antoniol, Giuliano and Ayari, Kamel and Penta, Massimiliano Di and Khomh, Foutse},
booktitle = {Proceedings of the 2008 Conference of the Center for Advanced Studies on Collaborative Research (CASCON'08)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 2008 Conference of the Center for Advanced Studies on Collaborative Research (CASCON'08)/Is it a Bug or an Enhancement A Text-based Approach to Classify Change Requests.pdf:pdf},
pages = {1--15},
title = {{Is it a Bug or an Enhancement ? A Text-based Approach to Classify Change Requests}},
year = {2008}
}
@inproceedings{Arisholm2006,
abstract = {This paper reports on the construction and validation of faultproneness prediction models in the context of an object-oriented, evolving, legacy system. The goal is to help QA engineers focus their limited verification resources on parts of the system likely to contain faults. A number of measures including code quality, class structure, changes in class structure, and the history of class-level changes and faults are included as candidate predictors of class fault-proneness. A cross-validated classification analysis shows that the obtained model has less than 20\% of false positives and false negatives, respectively. However, as shown in this paper, statistics regarding the classification accuracy tend to inflate the potential usefulness of the fault-proneness prediction models. We thus propose a simple and pragmatic methodology for assessing the costeffectiveness of the predictions to focus verification effort. On the basis of the cost-effectiveness analysis we show that change and fault data from previous releases is paramount to developing a practically useful prediction model. When our model is applied to predict faults in a new release, the estimated potential savings in verification effort is about 29\%. In contrast, the estimated savings in verification effort drops to 0\% when history data is not included.},
author = {Arisholm, Erik and Briand, Lionel C},
booktitle = {Proceedings of the International Symposium on International Symposium on Empirical Software Engineering (ISESE'06)},
doi = {10.1145/1159733.1159738},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the International Symposium on International Symposium on Empirical Software Engineering (ISESE'06)/Predicting fault-prone components in a java legacy system.pdf:pdf},
isbn = {1595932186},
issn = {1-59593-218-6},
pages = {8--17},
title = {{Predicting fault-prone components in a java legacy system}},
url = {http://portal.acm.org/citation.cfm?doid=1159733.1159738},
year = {2006}
}
@article{Arisholm2007,
abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and inspections and would like to be able to devote extra resources to faulty system parts. The main research focus of this paper is two-fold: (1) use and compare many data mining and machine learning techniques to build fault-proneness models based mostly on source code measures and change/fault history data, and (2) demonstrate that the usual classification evaluation criteria based on confusion matrices may not be fully appropriate to compare and evaluate models.},
author = {Arisholm, Erik and Briand, Lionel C. and Fuglerud, Magnus},
doi = {10.1109/ISSRE.2007.16},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings - International Symposium on Software Reliability Engineering, ISSRE/Data mining techniques for building fault-proneness models in telecom java software.pdf:pdf},
isbn = {0769530249},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
keywords = {class fault-proneness,prediction,testing},
pages = {215--224},
title = {{Data mining techniques for building fault-proneness models in telecom java software}},
year = {2007}
}
@article{Arisholm2010,
author = {Arisholm, Erik and Briand, Lionel C. and Johannessen, Eivind B.},
doi = {10.1016/j.jss.2009.06.055},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Journal of Systems and Software/A systematic and comprehensive investigation of methods to build and evaluate fault prediction models.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {fault prediction models},
number = {1},
pages = {2--17},
publisher = {Elsevier Inc.},
title = {{A systematic and comprehensive investigation of methods to build and evaluate fault prediction models}},
volume = {83},
year = {2010}
}
@article{Arlot2010,
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Statistics Surveys/A survey of cross-validation procedures for model selection.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {and phrases,cross-validation,leave-one-out,model selection},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Ayewah2007,
abstract = {Static analysis tools for software defect detection are becoming widely used in practice. However, there is little public information regarding the experimental evaluation of the accuracy and value of the warnings these tools report. In this paper, we discuss the warn- ings found by FindBugs, a static analysis tool that finds defects in Java programs.We discuss the kinds of warnings generated and the classification of warnings into false positives, trivial bugs and se- rious bugs. We also provide some insight into why static analysis tools often detect true but trivial bugs, and some information about defect warnings across the development lifetime of software re- lease.We report data on the defectwarnings in Sun’s Java 6 JRE, in Sun’s Glassfish JEE server, and in portions of Google’s Java code- base. Finally, we report on some experiences from incorporating static analysis into the software development process at Google. Categories},
author = {Ayewah, Nathaniel and Pugh, William and Morgenthaler, J. David and Penix, John and Zhou, YuQian},
doi = {10.1145/1251535.1251536},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the 7th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering - PASTE '07/Evaluating static analysis defect warnings on production software.pdf:pdf},
isbn = {9781595935953},
journal = {Proceedings of the 7th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering - PASTE '07},
keywords = {bug,bugs,false positives,findbugs,java,patterns,software defects,software quality,static analysis},
pages = {1--8},
title = {{Evaluating static analysis defect warnings on production software}},
url = {http://portal.acm.org/citation.cfm?doid=1251535.1251536},
year = {2007}
}
@inproceedings{Bachmann2010,
author = {Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
booktitle = {Proceedings of the ACM SIGSOFT 18th International Symposium on the Foundations of Software Engineering ( FSE'10)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the ACM SIGSOFT 18th International Symposium on the Foundations of Software Engineering ( FSE'10)/The Missing Links Bugs and Bug-fix Commits Categories and Subject Descriptors.pdf:pdf},
isbn = {9781605587912},
keywords = {apache,are widely used in,bias,case study,commit,especially bug reports and,logs,manual annotation,software engineering research,software process data,the,tool},
pages = {97--106},
title = {{The Missing Links : Bugs and Bug-fix Commits Categories and Subject Descriptors}},
year = {2010}
}
@article{Beleites2005,
abstract = {In biomedical applications, frequently only a limited number of samples are available for the development and testing of classification rules. Understanding the behavior of the error estimators in this setting is therefore highly desirable. In an extensive study using simulated as well as real-life data we investigated the properties of commonly used error estimators in terms of their bias and variance, and have found that in these small-sample size situations, the influence of variance on the error estimates can be significant, and can dominate the bias. Consequently, our results strongly suggest that bootstrap resampling and/or k-fold crossvalidation-based estimators, especially when computed over multiple data splits, should be preferred in these small-sample size scenarios, because of their reduced variance compared to the more routinely used crossvalidation approaches. While linear partial least squares was used as the classifier/regressor, the general conclusions arising from this study are not qualitatively affected for other classifiers, linear or nonlinear. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Beleites, Claudia and Baumgartner, Richard and Bowman, Christopher and Somorjai, Ray and Steiner, Gerald and Salzer, Reiner and Sowa, Michael G.},
doi = {10.1016/j.chemolab.2005.04.008},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Chemometrics and Intelligent Laboratory Systems/Variance reduction in estimating classification error using sparse datasets.pdf:pdf},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Bootstrap resampling,Crossvalidation,Error rate estimation,Small sample size},
number = {1-2},
pages = {91--100},
title = {{Variance reduction in estimating classification error using sparse datasets}},
volume = {79},
year = {2005}
}
@article{Binkley2007,
abstract = {Accurate prediction of faulty modules reduces the cost of software development and evolution. Two case studies with a language-processing based fault prediction measure are presented. The measure, refereed to as a QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgements of software quality. The two case studies consider the measure's application to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and QALP score. Results, while complex, show that little correlation exists in the first case study, while statistically significant correlations exists in the second. In this second study the QALP score is helpful in predicting faults in modules (files) with its usefulness growing as module size increases.},
author = {Binkley, D. and Feild, H. and Lawrie, D. and Pighin, M.},
doi = {10.1109/TAIC.PART.2007.10},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Testing Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)/Software Fault Prediction using Language Processing.pdf:pdf},
isbn = {978-0-7695-2984-4},
journal = {Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)},
keywords = {code comprehension,empirical software engineering,fault prediction,for example,for more,however,information retrieval,it has been observed,nortel networks and,sophisticated measures,that there is need},
pages = {99--108},
title = {{Software Fault Prediction using Language Processing}},
year = {2007}
}
@article{Bird2009,
abstract = {Studies have shown that social factors in development organizations have a dramatic effect on software quality. Separately, program dependency information has also been used successfully to predict which software components are more fault prone. Interestingly, the influence of these two phenomena have only been studied separately. Intuition and practical experience suggests,however, that task assignment (i.e. who worked on which components and how much) and dependency structure (which components have dependencies on others)together interact to influence the quality of the resulting software. We study the influence of combined socio-technical software networks on the fault-proneness of individual software components within a system. The network properties of a software component in this combined network are able to predict if an entity is failure prone with greater accuracy than prior methods which use dependency or contribution information in isolation. We evaluate our approach in different settings by using it on Windows Vista and across six releases of the Eclipse development environment including using models built from one release to predict failure prone components in the next release. We compare this to previous work. In every case, our method performs as well or better and is able to more accurately identify those software components that have more post-release failures, with precision and recall rates as high as 85\%.},
author = {Bird, Christian and Nagappan, Nachiappan and Murphy, Brendan and Gall, Harald and Devanbu, Premkumar},
doi = {10.1109/ISSRE.2009.17},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings - International Symposium on Software Reliability Engineering, ISSRE/Putting it all together Using socio-technical networks to predict failures.pdf:pdf},
isbn = {9780769538785},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
pages = {109--119},
title = {{Putting it all together: Using socio-technical networks to predict failures}},
year = {2009}
}
@article{Bontempi,
author = {Bontempi, Gianluca},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Resampling techniques for statistical modeling.pdf:pdf},
title = {{Resampling techniques for statistical modeling}}
}
@article{Braga-Neto2004a,
abstract = {MOTIVATION: Microarray classification typically possesses two striking attributes: (1) classifier design and error estimation are based on remarkably small samples and (2) cross-validation error estimation is employed in the majority of the papers. Thus, it is necessary to have a quantifiable understanding of the behavior of cross-validation in the context of very small samples. RESULTS: An extensive simulation study has been performed comparing cross-validation, resubstitution and bootstrap estimation for three popular classification rules-linear discriminant analysis, 3-nearest-neighbor and decision trees (CART)-using both synthetic and real breast-cancer patient data. Comparison is via the distribution of differences between the estimated and true errors. Various statistics for the deviation distribution have been computed: mean (for estimator bias), variance (for estimator precision), root-mean square error (for composition of bias and variance) and quartile ranges, including outlier behavior. In general, while cross-validation error estimation is much less biased than resubstitution, it displays excessive variance, which makes individual estimates unreliable for small samples. Bootstrap methods provide improved performance relative to variance, but at a high computational cost and often with increased bias (albeit, much less than with resubstitution).},
author = {Braga-Neto, Ulisses M and Dougherty, Edward R},
doi = {10.1093/bioinformatics/btg419},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Bioinformatics/Is cross-validation valid for small-sample microarray classification.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Automated,Benchmarking,Benchmarking: methods,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: genetics,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genetic Testing,Genetic Testing: methods,Humans,Models,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Statistical},
number = {3},
pages = {374--80},
pmid = {14960464},
title = {{Is cross-validation valid for small-sample microarray classification?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14960464},
volume = {20},
year = {2004}
}
@article{Brier1950,
abstract = {Two methods of solving the balance equation are outlined. Both methods have been used successfully on a daily operational basis at the Joint Numerical Weather Prediction Unit for a period of more than a year. Solutions were on the operational grid of 30 x 34 points spaced at 381-km. intervals.},
author = {Brier, Glenn W.},
doi = {10.1126/science.27.693.594},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1950/Monthly Weather Review/Verification of Forecasets Expressed in Terms of Probability.pdf:pdf},
isbn = {1520-0493},
issn = {0036-8075},
journal = {Monthly Weather Review},
number = {1},
pages = {25--27},
pmid = {17754781},
title = {{Verification of Forecasets Expressed in Terms of Probability}},
volume = {78},
year = {1950}
}
@book{Casella,
author = {Casella, G and Fienberg, S and Olkin, I},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/An Introduction to Statistical Learning.pdf:pdf},
isbn = {9781461471370},
title = {{An Introduction to Statistical Learning}}
}
@article{Cawley2003,
author = {Cawley, Gavin C. and Talbot, Nicola L.C.},
doi = {10.1016/S0031-3203(03)00136-5},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/Pattern Recognition/Efficient leave-one-out cross-validation of kernel fisher discriminant classifiers.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {cross-validation,kernel fisher discriminant analysis,model selection},
month = nov,
number = {11},
pages = {2585--2592},
title = {{Efficient leave-one-out cross-validation of kernel fisher discriminant classifiers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320303001365},
volume = {36},
year = {2003}
}
@article{Challagulla2006,
abstract = {Failures of mission-critical software systems can have catastrophic consequences and, hence, there is strong need for scientifically rigorous methods for assuring high system reliability. To reduce the V\&amp;amp;V cost for achieving high confidence levels, quantitatively based software defect prediction techniques can be used to effectively estimate defects from prior data. Better prediction models facilitate better project planning and risk/cost estimation. Memory based reasoning (MBR) is one such classifier that quantitatively solves new cases by reusing knowledge gained from past experiences. However, it can have different configurations by varying its input parameters, giving potentially different predictions. To overcome this problem, we develop a framework that derives the optimal configuration of an MBR classifier for software defect data, by logical variation of its configuration parameters. We observe that this adaptive MBR technique provides a flexible and effective environment for accurate prediction of mission-critical software defect data},
author = {Challagulla, Venkata U B and Bastani, Farokh B. and Yen, I. Ling},
doi = {10.1109/ICTAI.2006.23},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI/A unified framework for defect data analysis using the MBR technique.pdf:pdf},
isbn = {0769527280},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
pages = {39--46},
title = {{A unified framework for defect data analysis using the MBR technique}},
year = {2006}
}
@article{Chawla,
author = {Chawla, Nitesh V},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Chapter 40 DATA MINING FOR IMBALANCED DATASETS AN OVERVIEW.pdf:pdf},
title = {{Chapter 40 DATA MINING FOR IMBALANCED DATASETS : AN OVERVIEW}}
}
@article{Concato1993,
author = {Concato, John and Feinstein, Alvan R and Holford, Theodore R},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1993/Annals of Internal Medicine/The Risk of Determining Risk with Multivariable Models.pdf:pdf},
journal = {Annals of Internal Medicine},
number = {3},
pages = {201--210},
title = {{The Risk of Determining Risk with Multivariable Models}},
volume = {118},
year = {1993}
}
@article{Cruz2010,
abstract = {This paper describes the use of a UML metric, an approximation of the CK-RFC metric, for predicting faulty classes before their implementation. We built a code-based prediction model of faulty classes using Logistic Regression. Then, we tested it in different projects, using on the one hand their UML metrics, and on the other hand their code metrics. To decrease the difference of values between UML and code measures, we normalized them using Linear Scaling to Unit Variance. Our results indicate that the proposed UML RFC metric can predict faulty code as well as its corresponding code metric does. Moreover, the normalization procedure used was of great utility, not just for enabling our UML metric to predict faulty code, using a code-based prediction model, but also for improving the prediction results across different packages and projects, using the same model.},
author = {Cruz, Ana Erika Camargo},
doi = {10.1145/1810295.1810393},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 ACMIEEE 32nd International Conference on Software Engineering/Exploratory study of a UML metric for fault prediction.pdf:pdf},
isbn = {978-1-60558-719-6},
issn = {0270-5257},
journal = {2010 ACM/IEEE 32nd International Conference on Software Engineering},
keywords = {CK metrics,UML,fault-prone code,fault-proneness prediction,logistic regression},
pages = {361--364},
title = {{Exploratory study of a UML metric for fault prediction}},
volume = {2},
year = {2010}
}
@article{Cruz2009,
author = {Cruz, Ana Erika Camargo and Ochimizu, Koichiro},
doi = {10.1109/ESEM.2009.5316002},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 3rd International Symposium on Empirical Software Engineering and Measurement/Towards logistic regression models for predicting fault-prone code across software projects.pdf:pdf},
isbn = {978-1-4244-4842-5},
journal = {2009 3rd International Symposium on Empirical Software Engineering and Measurement},
pages = {460--463},
title = {{Towards logistic regression models for predicting fault-prone code across software projects}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5316002},
year = {2009}
}
@inproceedings{D&apos;Ambros2009,
abstract = {Change coupling is the implicit relationship between two or more software artifacts that have been observed to frequently change together during the evolution of a software system. Researchers have studied this dependency and have observed that it points to design issues such as architectural decay. It is still unknown whether change coupling correlates with a tangible effect of design issues, i.e., software defects.In this paper we analyze the relationship between change coupling and software defects on three large software systems. We investigate whether change coupling correlates with defects, and if the performance of bug prediction models based on software metrics can be improved with change coupling information.},
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
booktitle = {Proceedings of the Working Conference on Reverse Engineering (WCRE'09)},
doi = {10.1109/WCRE.2009.19},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the Working Conference on Reverse Engineering (WCRE'09)/On the relationship between change coupling and software defects.pdf:pdf},
isbn = {9780769538679},
issn = {10951350},
keywords = {Change coupling,Software defects},
pages = {135--144},
title = {{On the relationship between change coupling and software defects}},
year = {2009}
}
@article{Davison1992,
author = {Davison, A. C. and Hall, Peter},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1992/Biometrika/On the bias and variability of bootstrap and cross-validation estimates of error rate in discrimination problems.pdf:pdf},
journal = {Biometrika},
number = {2},
pages = {279--284},
title = {{On the bias and variability of bootstrap and cross-validation estimates of error rate in discrimination problems}},
volume = {79},
year = {1992}
}
@article{DeCarvalho2010,
abstract = {In the literature the fault-proneness of classes or methods has been used to devise strategies for reducing testing costs and efforts. In general, fault-proneness is predicted through a set of design metrics and, most recently, by using Machine Learning (ML) techniques. However, some ML techniques cannot deal with unbalanced data, characteristic very common of the fault datasets and, their produced results are not easily interpreted by most programmers and testers. Considering these facts, this paper introduces a novel fault-prediction approach based on Multiobjective Particle Swarm Optimization (MOPSO). Exploring Pareto dominance concepts, the approach generates a model composed by rules with specific properties. These rules can be used as an unordered classifier, and because of this, they are more intuitive and comprehensible. Two experiments were accomplished, considering, respectively, fault-proneness of classes and methods. The results show interesting relationships between the studied metrics and fault prediction. In addition to this, the performance of the introduced MOPSO approach is compared with other ML algorithms by using several measures including the area under the ROC curve, which is a relevant criterion to deal with unbalanced data. © 2010 Elsevier Inc. All rights reserved.},
author = {de Carvalho, Andr\'{e} B. and Pozo, Aurora and Vergilio, Silvia Regina},
doi = {10.1016/j.jss.2009.12.023},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Journal of Systems and Software/A symbolic fault-prediction model based on multiobjective particle swarm optimization.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Fault prediction,Multiobjective,Particle swarm optimization,Rule learning algorithm},
number = {5},
pages = {868--882},
publisher = {Elsevier Inc.},
title = {{A symbolic fault-prediction model based on multiobjective particle swarm optimization}},
url = {http://dx.doi.org/10.1016/j.jss.2009.12.023},
volume = {83},
year = {2010}
}
@article{Dejaeger2012,
author = {Dejaeger, Karel and Verbeke, Wouter and Martens, David and Baesens, Bart},
doi = {10.1109/TSE.2011.55},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/IEEE Transactions on Software Engineering/Data Mining Techniques for Software Effort Estimation A Comparative Study.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {2},
pages = {375--397},
title = {{Data Mining Techniques for Software Effort Estimation: A Comparative Study}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5928350},
volume = {38},
year = {2012}
}
@article{Dejaeger2013,
author = {Dejaeger, Karel and Verbraken, Thomas and Baesens, Bart},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Unknown/Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers.pdf:pdf},
number = {2},
pages = {237--257},
title = {{Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers}},
volume = {39},
year = {2013}
}
@article{DenBoer2005,
abstract = {INTRODUCTION: Prognostic models, such as the Acute Physiology and Chronic Health Evaluation (APACHE) II or III, the Simplified Acute Physiology Score (SAPS) II, and the Mortality Probability Models (MPM) II were developed to quantify the severity of illness and the likelihood of hospital survival for a general intensive care unit (ICU) population. Little is known about the performance of these models in specific populations, such as patients with cancer. Recently, specific prognostic models have been developed to predict mortality for cancer patients who are admitted to the ICU. The present analysis reviews the performance of general prognostic models and specific models for cancer patients to predict in-hospital mortality after ICU admission. METHODS: Studies were identified by searching the Medline databases from 1994 to 2004. We included studies evaluating the performance of mortality prediction models in critically ill cancer patients. RESULTS: Ten studies were identified that evaluated prognostic models in cancer patients. Discrimination between survivors and non-survivors was fair to good, but calibration was insufficient in most studies. General prognostic models uniformly underestimate the likelihood of hospital mortality in oncological patients. Two versions of a specific oncological scoring systems (Intensive Care Mortality Model (ICMM)) were evaluated in five studies and showed better discrimination and calibration than the general prognostic models. CONCLUSION: General prognostic models generally underestimate the risk of mortality in critically ill cancer patients. Both general prognostic models and specific oncology models may reliably identify subgroups of patients with a very high risk of mortality.},
author = {den Boer, Sylvia and de Keizer, Nicolette F and de Jonge, Evert},
doi = {10.1186/cc3765},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Critical care/Performance of prognostic models in critically ill cancer patients - a review.pdf:pdf},
issn = {1466-609X},
journal = {Critical care},
number = {4},
pages = {R458--R463},
pmid = {16137361},
title = {{Performance of prognostic models in critically ill cancer patients - a review.}},
volume = {9},
year = {2005}
}
@article{Denaro2000,
abstract = {The article investigates whether a correlation exists between the
fault-proneness of software and the measurable attributes of the code
(i.e. the static metrics) and of the testing (i.e. the dynamic metrics).
The article also studies how to use such data for tuning the testing
process. The goal is not to find a general solution to the problem (a
solution may not even exist), but to investigate the scope of specific
solutions, i.e., to what extent homogeneity of the development process,
organization, environment and application domain allows data computed on
past projects to be projected onto new projects. A suitable variety of
case studies is selected to investigate a methodology applicable to
classes of homogeneous products, rather than investigating if a specific
solution exists for few cases},
author = {Denaro, G.},
doi = {10.1109/ICSE.2000.870474},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium/Estimating software fault-proneness for tuning testing activities.pdf:pdf},
isbn = {1-58113-206-9},
issn = {0270-5257},
journal = {Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium},
pages = {704--706},
title = {{Estimating software fault-proneness for tuning testing activities}},
year = {2000}
}
@article{Denaro2002,
abstract = {Planning and allocating resources for testing is difficult and it is usually done on an empirical basis, often leading to unsatisfactory results. The possibility of early estimation of the potential faultiness of software could be of great help for planning and executing testing activities. Most research concentrates on the study of different techniques for computing multivariate models and evaluating their statistical validity, but we still lack experimental data about the validity of such models across different software applications. The paper reports on an empirical study of the validity of multivariate models for predicting software fault-proneness across different applications. It shows that suitably selected multivariate models can predict fault-proneness of modules of different software packages.},
author = {Denaro, G. and Pezze, M.},
doi = {10.1109/ICSE.2002.1007972},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings of the 24th International Conference on Software Engineering. ICSE 2002/An empirical evaluation of fault-proneness models.pdf:pdf},
isbn = {1-58113-472-X},
issn = {02705257},
journal = {Proceedings of the 24th International Conference on Software Engineering. ICSE 2002},
keywords = {cross-,fault-proneness models,faultiness,logistic regression,principal component analysis,software,software metrics,software process,testing process,validation},
title = {{An empirical evaluation of fault-proneness models}},
year = {2002}
}
@article{Dhiauddin2010,
abstract = {Defect prediction is an important aspect of the Product Development Life Cycle. The rationale in knowing predicted number of functional defects earlier on in the lifecycle, rather than to just find as many defects as possible during testing phase is to determine when to stop testing and ensure all the in-phase defects have been found in-phase before a product is delivered to the intended end user. It also ensures that wider test coverage is put in place to discover the predicted defects. This research is aimed to achieve zero known post release defects of the software delivered to the end user by MIMOS Berhad. To achieve the target, the research effort focuses on establishing a test defect prediction model using Design for Six Sigma methodology in a controlled environment where all the factors contributing to the defects of the product is within MIMOS Berhad. It identifies the requirements for the prediction model and how the model can benefit them. It also outlines the possible predictors associated with defect discovery in the testing phase. Analysis of the repeatability and capability of test engineers in finding defects are demonstrated. This research also describes the process of identifying characteristics of data that need to be collected and how to obtain them. Relationship of customer needs with the technical requirements of the proposed model is then clearly analyzed and explained. Finally, the proposed test defect prediction model is demonstrated via multiple regression analysis. This is achieved by incorporating testing metrics and development-related metrics as the predictors. The achievement of the whole research effort is described at the end of this study together with challenges faced and recommendation for future research work.},
author = {Dhiauddin, Muhammad and Suffian, Mohamed and Abdullah, Mohamed Redzuan},
doi = {10.1109/ITSIM.2010.5561516},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings 2010 International Symposium on Information Technology - System Development and Application and Knowledge Society, ITSim'10/Establishing a defect prediction model using a combination of product metrics as predictors via six sigma methodology.pdf:pdf},
isbn = {9781424467181},
issn = {2155-897},
journal = {Proceedings 2010 International Symposium on Information Technology - System Development and Application and Knowledge Society, ITSim'10},
keywords = {Defect prediction,Model,Prediction model,Six sigma,Test defect,Test defect prediction model},
pages = {1087--1092},
title = {{Establishing a defect prediction model using a combination of product metrics as predictors via six sigma methodology}},
volume = {3},
year = {2010}
}
@article{DiStefano2002,
author = {{Di Stefano}, J.S. and Menzies, T.},
doi = {10.1109/TAI.2002.1180811},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings/Machine learning for software engineering case studies in software reuse.pdf:pdf},
isbn = {0-7695-1849-4},
journal = {14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.},
keywords = {5,8,8 part,ai algorithms,ai in data mining,ai in software engineer-,apriori,association rule learning,c4,deci-,empirical,ing,j4,machine learning,reuse,sion tree learning,studies,tar2,treatment learning},
pages = {246--251},
publisher = {IEEE Comput. Soc},
title = {{Machine learning for software engineering: case studies in software reuse}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1180811},
year = {2002}
}
@article{Dougherty2010,
author = {Dougherty, Edward and Sima, Chao and Hua, Jianping and Hanczar, Blaise and Braga-Neto, Ulisses},
doi = {10.2174/157489310790596385},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Bioinformatics/Performance of Error Estimators for Classification.pdf:pdf},
issn = {15748936},
journal = {Bioinformatics},
number = {1},
pages = {53--67},
title = {{Performance of Error Estimators for Classification}},
url = {http://www.eurekaselect.com/openurl/content.php?genre=article\&issn=1574-8936\&volume=5\&issue=1\&spage=53},
volume = {5},
year = {2010}
}
@article{Efron2007,
author = {Efron, Bradley},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1986/Journal of the American Statistical Association/How Biased Is the Apparent Error Rate of a Prediction Rule.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {012,aic,bootstrap,cross-validation,generalized linear models,in this case,indicating that bias is,logistic regression,mallows,methods,not a serious problem,s c,the,the apparent error rate,the downward bias of,to be only},
number = {394},
pages = {461--470},
title = {{How Biased Is the Apparent Error Rate of a Prediction Rule ?}},
volume = {81},
year = {1986}
}
@article{Efron1983bootstrap,
author = {Efron, Bradley and Gong, Gail},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1983/The American Statistician/A Leisurely Look at the Bootstrap , the Jackknife , and Cross-Validation.pdf:pdf},
journal = {The American Statistician},
keywords = {bias estimation,error rate prediction,nonparametric con-,nonparametric standard errors,variance estimation},
number = {1},
pages = {36--48},
title = {{A Leisurely Look at the Bootstrap , the Jackknife , and Cross-Validation}},
volume = {37},
year = {1983}
}
@book{Efron1993,
address = {Boston, MA},
author = {Efron, Bradley and Tibshirani, Robert J.},
doi = {10.1007/978-1-4899-4541-9},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1993/Unknown/An Introduction to the Bootstrap.pdf:pdf},
isbn = {978-0-412-04231-7},
publisher = {Springer US},
title = {{An Introduction to the Bootstrap}},
url = {http://link.springer.com/10.1007/978-1-4899-4541-9},
year = {1993}
}
@article{Fenton2000,
abstract = {The authors describe a number of results from a quantitative study
of faults and failures in two releases of a major commercial software
system. They tested a range of basic software engineering hypotheses
relating to: the Pareto principle of distribution of faults and
failures; the use of early fault data to predict later fault and failure
data; metrics for fault prediction; and benchmarking fault data. For
example, we found strong evidence that a small number of modules contain
most of the faults discovered in prerelease testing and that a very
small number of modules contain most of the faults discovered in
operation. We found no evidence to support previous claims relating
module size to fault density nor did we find evidence that popular
complexity metrics are good predictors of either fault-prone or
failure-prone modules. We confirmed that the number of faults discovered
in prerelease testing is an order of magnitude greater than the number
discovered in 12 months of operational use. The most important result
was strong evidence of a counter-intuitive relationship between pre- and
postrelease faults; those modules which are the most fault-prone
prerelease are among the least fault-prone postrelease, while
conversely, the modules which are most fault-prone postrelease are among
the least fault-prone prerelease. This observation has serious
ramifications for the commonly used fault density measure. Our results
provide data-points in building up an empirical picture of the software
development process},
author = {Fenton, Norman E.},
doi = {10.1109/32.879815},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/IEEE Transactions on Software Engineering/Quantitative analysis of faults and failures in a complex software system.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
number = {8},
pages = {797--814},
title = {{Quantitative analysis of faults and failures in a complex software system}},
volume = {26},
year = {2000}
}
@article{Ferzund2009,
abstract = {Change management is a challenging task in software maintenance. Changes are made to the software during its whole life. Some of these changes introduce errors in the code which result in failures. Software changes are composed of small code units called hunks, dispersed in source code files. In this paper we present a technique for classifying software changes based on hunk metrics. We classify individual hunks as buggy or bug-free, thus we provide an approach for bug prediction at the smallest level of granularity. We introduce a set of hunk metrics and build classification models based on these metrics. Classification models are built using logistic regression and random forests. We evaluated the performance of our approach on 7 open source software projects. Our classification approach can classify hunks as buggy or bug free with 81 percent accuracy, 77 percent buggy hunk precision and 67 percent buggy hunk recall on average. Most of the hunk metrics are significant predictors of bugs but the set of significant metrics varies among different projects.},
author = {Ferzund, Javed and Ahsan, Syed Nadeem and Wotawa, Franz},
doi = {10.1109/ICSM.2009.5306274},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/IEEE International Conference on Software Maintenance, ICSM/Software change classification using hunk metrics.pdf:pdf},
isbn = {9781424448289},
issn = {1063-6773},
journal = {IEEE International Conference on Software Maintenance, ICSM},
pages = {471--474},
title = {{Software change classification using hunk metrics}},
year = {2009}
}
@article{Fox2002,
author = {Fox, John},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Unknown/Bootstrapping Regression Models.pdf:pdf},
number = {January},
pages = {1--14},
title = {{Bootstrapping Regression Models}},
year = {2002}
}
@article{Fu2005,
abstract = {MOTIVATION: Estimation of misclassification error has received increasing attention in clinical diagnosis and bioinformatics studies, especially in small sample studies with microarray data. Current error estimation methods are not satisfactory because they either have large variability (such as leave-one-out cross-validation) or large bias (such as resubstitution and leave-one-out bootstrap). While small sample size remains one of the key features of costly clinical investigations or of microarray studies that have limited resources in funding, time and tissue materials, accurate and easy-to-implement error estimation methods for small samples are desirable and will be beneficial. RESULTS: A bootstrap cross-validation method is studied. It achieves accurate error estimation through a simple procedure with bootstrap resampling and only costs computer CPU time. Simulation studies and applications to microarray data demonstrate that it performs consistently better than its competitors. This method possesses several attractive properties: (1) it is implemented through a simple procedure; (2) it performs well for small samples with sample size, as small as 16; (3) it is not restricted to any particular classification rules and thus applies to many parametric or non-parametric methods.},
author = {Fu, Wenjiang J and Carroll, Raymond J and Wang, Suojin},
doi = {10.1093/bioinformatics/bti294},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Bioinformatics/Estimating misclassification error with small samples via bootstrap cross-validation.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Biological,Biological: genetics,Biological: metabolism,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: genetics,Breast Neoplasms: metabolism,Data Interpretation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genetic Testing,Genetic Testing: methods,Humans,Neoplasm Proteins,Neoplasm Proteins: genetics,Neoplasm Proteins: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Prognosis,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Statistical,Tumor Markers},
month = may,
number = {9},
pages = {1979--86},
pmid = {15691862},
title = {{Estimating misclassification error with small samples via bootstrap cross-validation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15691862},
volume = {21},
year = {2005}
}
@inproceedings{Fukushima2014,
author = {Fukushima, Takafumi and Kamei, Yasutaka and Mcintosh, Shane and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
booktitle = {Proceeding of the 11th Working Conference on Mining Software Repositories (MSR'14)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Proceeding of the 11th Working Conference on Mining Software Repositories (MSR'14)/An Empirical Study of Just-in-Time Defect Prediction using Cross-Project Models.pdf:pdf},
isbn = {9781450328630},
keywords = {all or part of,empirical study,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,software quality,this work for},
title = {{An Empirical Study of Just-in-Time Defect Prediction using Cross-Project Models}},
year = {2014}
}
@article{Fushiki2009,
author = {Fushiki, Tadayoshi},
doi = {10.1007/s11222-009-9153-8},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Statistics and Computing/Estimation of prediction error by using K-fold cross-validation.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {1989,and yanagihara et al,bias correction,bias-corrected versions of cross-validation,burman,have been proposed by,hinkley 1997,k -fold cross-validation,large dataset,prediction error},
month = oct,
number = {2},
pages = {137--146},
title = {{Estimation of prediction error by using K-fold cross-validation}},
url = {http://link.springer.com/10.1007/s11222-009-9153-8},
volume = {21},
year = {2009}
}
@article{Galster2014,
author = {Galster, Matthias and Weyns, Danny and Tofan, Dan and Michalik, Bartosz and Avgeriou, Paris},
doi = {10.1109/TSE.2013.56},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/IEEE Transactions on Software Engineering/Variability in Software Systems—A Systematic Literature Review.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = mar,
number = {3},
pages = {282--306},
title = {{Variability in Software Systems—A Systematic Literature Review}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6682901},
volume = {40},
year = {2014}
}
@article{Gao2007,
abstract = {Count models, such as the Poisson regression model, and the negative binomial regression model, can be used to obtain software fault predictions. With the aid of such predictions, the development team can improve the quality of operational software. The zero-inflated, and hurdle count models may be more appropriate when, for a given software system, the number of modules with faults are very few. Related literature lacks quantitative guidance regarding the application of count models for software quality prediction. This study presents a comprehensive empirical investigation of eight count models in the context of software fault prediction. It includes comparative hypothesis testing, model selection, and performance evaluation for the count models with respect to different criteria. The case study presented is that of a full-scale industrial software system. It is observed that the information obtained from hypothesis testing, and model selection techniques was not consistent with the predictive performances of the count models. Moreover, the comparative analysis based on one criterion did not match that of another criterion. However, with respect to a given criterion, the performance of a count model is consistent for both the fit, and test data sets. This ensures that, if a fitted model is considered good based on a given criterion, then the model will yield a good prediction based on the same criterion. The relative performances of the eight models are evaluated based on a one-way anova model, and Tukey's multiple comparison technique. The comparative study is useful in selecting the best count model for estimating the quality of a given software system},
author = {Gao, Kehan and Khoshgoftaar, Taghi M.},
doi = {10.1109/TR.2007.896761},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/IEEE Transactions on Reliability/A comprehensive empirical study of count models for software fault prediction.pdf:pdf},
issn = {00189529},
journal = {IEEE Transactions on Reliability},
keywords = {Anova,Count models,Hypothesis testing,Information criteria,Pearson's chi-square,Software metrics,Software quality,Tukey's multiple comparison},
number = {2},
pages = {223--236},
title = {{A comprehensive empirical study of count models for software fault prediction}},
volume = {56},
year = {2007}
}
@article{Geisser1975,
author = {Geisser, Seymour},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1975/Journal of the American Statistical Association/Sample Reuse Method The Predictive with Applications.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {350},
pages = {320--328},
title = {{Sample Reuse Method The Predictive with Applications}},
volume = {70},
year = {1975}
}
@article{Gray2010,
abstract = {Many studies have been carried out to predict the presence of software code defects using static code metrics. Such studies typically report how a classifier performs with real world data, but usually no analysis of the predictions is carried out. An analysis of this kind may be worthwhile as it can illuminate the motivation behind the predictions and the severity of the misclassifications. This investigation involves a manual analysis of the predictions made by Support Vector Machine classifiers using data from the NASA Metrics Data Program repository. The findings show that the predictions are generally well motivated and that the classifiers were, on average, more \&amp;\#x201C;confident\&amp;\#x201D; in the predictions they made which were correct.},
author = {Gray, David and Bowes, David and Davey, Neil and Sun, Yi and Christianson, Bruce},
doi = {10.1109/IJCNN.2010.5596650},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the International Joint Conference on Neural Networks/Software defect prediction using static code metrics underestimates defect-proneness.pdf:pdf},
isbn = {9781424469178},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks},
title = {{Software defect prediction using static code metrics underestimates defect-proneness}},
year = {2010}
}
@article{Guo2004,
abstract = { Accurate prediction of fault prone modules (a module is equivalent to a C function or a C+ + method) in software development process enables effective detection and identification of defects. Such prediction models are especially beneficial for large-scale systems, where verification experts need to focus their attention and resources to problem areas in the system under development. This paper presents a novel methodology for predicting fault prone modules, based on random forests. Random forests are an extension of decision tree learning. Instead of generating one decision tree, this methodology generates hundreds or even thousands of trees using subsets of the training data. Classification decision is obtained by voting. We applied random forests in five case studies based on NASA data sets. The prediction accuracy of the proposed methodology is generally higher than that achieved by logistic regression, discriminant analysis and the algorithms in two machine learning software packages, WEKA [I. H. Witten et al. (1999)] and See5. The difference in the performance of the proposed methodology over other methods is statistically significant. Further, the classification accuracy of random forests is more significant over other methods in larger data sets.},
author = {Guo, Lan and Ma, Yan and Cukic, Bojan and Singh, Harshinder},
doi = {10.1109/ISSRE.2004.35},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Proceedings - International Symposium on Software Reliability Engineering, ISSRE/Robust prediction of fault-proneness by random forests.pdf:pdf},
isbn = {0769522157},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
pages = {417--428},
title = {{Robust prediction of fault-proneness by random forests}},
year = {2004}
}
@article{Hall2012,
author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
doi = {10.1109/TSE.2011.103},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/IEEE Transactions on Software Engineering/A Systematic Literature Review on Fault Prediction Performance in Software Engineering.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = nov,
number = {6},
pages = {1276--1304},
title = {{A Systematic Literature Review on Fault Prediction Performance in Software Engineering}},
volume = {38},
year = {2012}
}
@article{Hanley1982,
abstract = {A representation and interpretation of the area under a receiver operating characteristic (ROC) curve obtained by the "rating" method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen non-diseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already well-studied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for the approximate magnitude of the sampling variability, i.e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining the size of the sample required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in the accuracy of diagnostic techniques.},
author = {Hanley, J a and McNeil, B J},
doi = {10.1148/radiology.143.1.7063747},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1982/Radiology/The meaning and use of the area under a receiver operating characteristic (ROC) curve.pdf:pdf},
isbn = {0033-8419 (Print) 0033-8419 (Linking)},
issn = {0033-8419},
journal = {Radiology},
number = {4},
pages = {29--36},
pmid = {7063747},
title = {{The meaning and use of the area under a receiver operating characteristic (ROC) curve.}},
volume = {143},
year = {1982}
}
@inproceedings{Hata2011,
author = {Hata, Hideaki and Mizuno, Osamu and Kikuno, Tohru},
booktitle = {Proceedings of the 12th International Workshop on Principles of Software Evolution (IWPSE’11)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the 12th International Workshop on Principles of Software Evolution (IWPSE’11)/Historage Fine-grained Version Control System for Java.pdf:pdf},
isbn = {9781450308489},
keywords = {fine-grained anal-,fine-grained version control,software evolution,software repository,ysis},
pages = {96--100},
title = {{Historage : Fine-grained Version Control System for Java}},
year = {2011}
}
@inproceedings{Hata2012,
author = {Hata, Hideaki and Mizuno, Osamu and Kikuno, Tohru},
booktitle = {Proceedings of the 34th International Conference on Software Engineering (ICSE'12)},
doi = {10.1109/ICSE.2012.6227193},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the 34th International Conference on Software Engineering (ICSE'12)/Bug prediction based on fine-grained module histories.pdf:pdf},
isbn = {978-1-4673-1067-3},
keywords = {-bug prediction,effort-based evaluation,fine-,fine-grained prediction,grained histories,historical metrics},
month = jun,
pages = {200--210},
title = {{Bug prediction based on fine-grained module histories}},
year = {2012}
}
@article{Hawkins2004,
author = {Hawkins, Douglas M},
doi = {10.1021/ci0342472},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Journal of chemical information and computer sciences/The problem of overfitting.pdf:pdf},
issn = {0095-2338},
journal = {Journal of chemical information and computer sciences},
number = {1},
pages = {1--12},
pmid = {14741005},
title = {{The problem of overfitting.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14741005},
volume = {44},
year = {2004}
}
@article{Herrera,
author = {Herrera, Francisco},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Dataset Shift in Classification Approaches and Problems Why is difficult to learn from DATA Intrinsic data characteristics Imbalanced.pdf:pdf},
title = {{Dataset Shift in Classification : Approaches and Problems Why is difficult to learn from DATA ? Intrinsic data characteristics Imbalanced data sets Dataset shift}}
}
@article{Huang2009,
abstract = {Effective software defect estimation can bring cost reduction and efficient resources allocation in software development and testing. Usually, estimation of defect-prone modules is based on the supervised learning of the modules at the same logical level. Various practical issues may limit the availability or quality of the attribute-value vectors extracting from the high-level modules by software metrics. In this paper, the problem of estimating the defect in high-level software modules is investigated with a multi-instance learning (MIL) perspective. In detail, each high-level module is regarded as a bag of its low-level components, and the learning task is to estimate the defect-proneness of the bags. Several typical supervised learning and MIL algorithms are evaluated on a mission critical project from NASA. Compared to the selected supervised schemas, the MIL methods improve the performance of the software defect estimation models.},
author = {Huang, Peng and Zhu, Jie},
doi = {10.1109/ICRCCS.2009.19},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/ICRCCS 2009 - 2009 International Conference on Research Challenges in Computer Science/Predicting defect-prone software modules at different logical levels.pdf:pdf},
isbn = {9780769539270},
journal = {ICRCCS 2009 - 2009 International Conference on Research Challenges in Computer Science},
keywords = {Kernel methods,Multi-instance learning,Software defect estimation,Support vector machine},
pages = {37--40},
title = {{Predicting defect-prone software modules at different logical levels}},
year = {2009}
}
@article{Jiang,
author = {Jiang, Wenyu and Simon, Richard},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/A comparison of bootstrap methods and an adjusted bootstrap approach for estimating prediction error in microarray classification Short.pdf:pdf},
title = {{A comparison of bootstrap methods and an adjusted bootstrap approach for estimating prediction error in microarray classification Short title: Bootstrap Prediction Error Estimation Wenyu Jiang* and Richard Simon}}
}
@article{Jiang2008b,
abstract = {Many statistical techniques have been proposed to predict fault-proneness of program modules in software engineering. Choosing the best candidate among many available models involves performance assessment and detailed comparison, but these comparisons are not simple due to the applicability of varying performance measures. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Methodologies for precise evaluation of fault prediction models should be at the core of empirical software engineering research, but have attracted sporadic attention. In this paper, we overview model evaluation techniques. In addition to many techniques that have been used in software engineering studies before, we introduce and discuss the merits of cost curves. Using the data from a public repository, our study demonstrates the strengths and weaknesses of performance evaluation techniques and points to a conclusion that the selection of the best model cannot be made without considering project cost characteristics, which are specific in each development environment.},
author = {Jiang, Yue and Cukic, Bojan and Ma, Yan},
doi = {10.1007/s10664-008-9079-3},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Empirical Software Engineering/Techniques for evaluating fault prediction models.pdf:pdf},
issn = {13823256},
journal = {Empirical Software Engineering},
keywords = {Empirical studies,Fault-prediction models,Model evaluation,Predictive models in software engineering},
number = {August},
pages = {561--595},
title = {{Techniques for evaluating fault prediction models}},
volume = {13},
year = {2008}
}
@article{Jiang2008a,
abstract = {Prediction of fault prone software components is one of the most researched problems in software engineering. Many statistical techniques have been proposed but there is no consensus on the methodology to select the "best model" for the specific project. In this paper, we introduce and discuss the merits of cost curve analysis of fault prediction models. Cost curves allow software quality engineers to introduce project-specific cost of module misclassification into model evaluation. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Through the analysis of sixteen projects from public repositories, we observe that software quality does not necessarily benefit from the prediction of fault prone components. The inclusion of misclassification cost in model evaluation may indicate that even the "best" models achieve performance no better than trivial classification. Our results support a recommendation to adopt cost curves as one of the standard methods for software quality model performance evaluation.},
author = {Jiang, Yue and Cukic, Bojan and Menzies, Tim},
doi = {10.1109/ISSRE.2008.54},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings - International Symposium on Software Reliability Engineering, ISSRE/Cost curve evaluation of fault prediction models.pdf:pdf},
isbn = {9780769534053},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
pages = {197--206},
title = {{Cost curve evaluation of fault prediction models}},
year = {2008}
}
@inproceedings{Jiang2008c,
author = {Jiang, Yue and Cukic, Bojan and Menzies, Tim},
booktitle = {Proceedings of the workshop on Defects in Large Software Systems (DEFECTS'08)},
doi = {10.1145/1390817.1390822},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the workshop on Defects in Large Software Systems (DEFECTS'08)/Can data transformation help in the detection of fault-prone modules.pdf:pdf},
isbn = {9781605580517},
pages = {16--20},
title = {{Can data transformation help in the detection of fault-prone modules?}},
url = {http://portal.acm.org/citation.cfm?doid=1390817.1390822},
year = {2008}
}
@misc{KalpanaSeshadrinathanMemberIEEERajivSoundararajanStudentMemberIEEEAlanConradBovikFellowIEEE,
author = {{Kalpana Seshadrinathan, Member, IEEE, Rajiv Soundararajan, Student Member, IEEE Alan Conrad Bovik, Fellow, IEEE}, and Lawrence K. Cormack},
doi = {10.1109/ICCCE.2010.5556751},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Login @ Ieeexplore.Ieee.Org.html:html},
keywords = {Study of Subjective and Objective Quality Assessme},
title = {{Login @ Ieeexplore.Ieee.Org}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1245301}
}
@article{Kanmani2004,
author = {Kanmani, S. and Uthariaraj, V. Rhymend and Sankaranarayanan, V. and Thambidurai, P.},
doi = {10.1145/1022494.1022515},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/ACM SIGSOFT Software Engineering Notes/Object oriented software quality prediction using general regression neural networks.pdf:pdf},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {metrics,neural network,object oriented,software quality},
number = {5},
pages = {1},
title = {{Object oriented software quality prediction using general regression neural networks}},
volume = {29},
year = {2004}
}
@article{Kanmani2007,
abstract = {This paper introduces two neural network based software fault prediction models using Object-Oriented metrics. They are empirically validated using a data set collected from the software modules developed by the graduate students of our academic institution. The results are compared with two statistical models using five quality attributes and found that neural networks do better. Among the two neural networks, Probabilistic Neural Networks outperform in predicting the fault proneness of the Object-Oriented modules developed. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Kanmani, S. and Uthariaraj, V. Rhymend and Sankaranarayanan, V. and Thambidurai, P.},
doi = {10.1016/j.infsof.2006.07.005},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Information and Software Technology/Object-oriented software fault prediction using neural networks.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Back propagation neural network,Discriminant analysis,Fault proneness,Logistic regression,Object-Oriented metrics,Probabilistic neural network},
pages = {483--492},
title = {{Object-oriented software fault prediction using neural networks}},
volume = {49},
year = {2007}
}
@article{Kaur2009,
abstract = {Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using clustering techniques. This approach has been tested with three real time defect datasets of NASA software projects, JM1, PC1 and CM1. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The results show that when all the prediction techniques are evaluated, the best prediction model is found to be the fusion of requirement and code metric model.},
author = {Kaur, a. and Sandhu, P.S. and a.S. Bra},
doi = {10.1109/ICMV.2009.54},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 Second International Conference on Machine Vision/Early Software Fault Prediction Using Real Time Defect Data.pdf:pdf},
isbn = {978-0-7695-3944-7},
journal = {2009 Second International Conference on Machine Vision},
keywords = {- clustering,defect data,k-means,roc curve and},
title = {{Early Software Fault Prediction Using Real Time Defect Data}},
year = {2009}
}
@article{Kaur2008,
abstract = {There are available metrics for predicting fault prone classes, which may help software organizations for planning and performing testing activities. This may be possible due to proper allocation of resources on fault prone parts of the design and code of the software. Hence, importance and usefulness of such metrics is understandable, but empirical validation of these metrics is always a great challenge. Random forest (RF) algorithm has been successfully applied for solving regression and classification problems in many applications. This paper evaluates the capability of RF algorithm in predicting fault prone software classes using open source software. The results indicate that the prediction performance of random forest is good. However, similar types of studies are required to be carried out in order to establish the acceptability of the RF model.},
author = {Kaur, Arvinder and Malhotra, Ruchika},
doi = {10.1109/ICACTE.2008.204},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings - 2008 International Conference on Advanced Computer Theory and Engineering, ICACTE 2008/Application of random forest in predicting fault-prone classes.pdf:pdf},
isbn = {9780769534893},
journal = {Proceedings - 2008 International Conference on Advanced Computer Theory and Engineering, ICACTE 2008},
keywords = {Fault prediction,Machine learning,Random Forest,Software metrics,Software quality},
pages = {37--43},
title = {{Application of random forest in predicting fault-prone classes}},
year = {2008}
}
@article{Khoshgoftaar2000a,
abstract = {The paper presents the Software Measurement Analysis and
Reliability Toolkit (SMART) which is a research tool for software
quality modeling using case based reasoning (CBR) and other modeling
techniques. Modern software systems must have high reliability. Software
quality models are tools for guiding reliability enhancement activities
to high risk modules for maximum effectiveness and efficiency. A
software quality model predicts a quality factor, such as the number of
faults in a module, early in the life cycle in time for effective
action. Software product and process metrics can be the basis for such
fault predictions. Moreover, classification models can identify fault
prone modules. CBR is an attractive modeling method based on automated
reasoning processes. However, to our knowledge, few CBR systems for
software quality modeling have been developed. SMART addresses this
area. There are currently three types of models supported by SMART:
classification based on CBR, CBR classification extended with cluster
analysis, and module-order models, which predict the rank-order of
modules according to a quality factor. An empirical case study of a
military command, control, and communications applied SMART at the end
of coding. The models built by SMART had a level of accuracy that could
be very useful to software developers},
author = {Khoshgoftaar, T.M. and Allen, E.B. and Busboom, J.C.},
doi = {10.1109/TAI.2000.889846},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000/Modeling software quality the Software Measurement Analysis and Reliability Toolkit.pdf:pdf},
isbn = {0-7695-0909-6},
issn = {1082-3409},
journal = {Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000},
keywords = {analogy models,authors through taghi m,case-based reasoning,data clustering,fault-prone,khosh-,models,module-order model,readers may contact the,software quality,software reliability,software tools},
pages = {54--61},
title = {{Modeling software quality: the Software Measurement Analysis and
Reliability Toolkit}},
year = {2000}
}
@article{Khoshgoftaar2000,
author = {Khoshgoftaar, Taghi M and Allen, Edward B},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Empirical Software Engineering/lVlo deling Fault -Yrone LVlodules 01 Subsystems.pdf:pdf},
isbn = {0769508073},
journal = {Empirical Software Engineering},
keywords = {classification,els,empirical study,fault-prone modules,software quality mod-,software reliability},
number = {561},
pages = {259--267},
title = {{lVlo deling Fault -Yrone LVlodules 01 Subsystems}},
year = {2000}
}
@article{Khoshgoftaar2004,
author = {Khoshgoftaar, Taghi and Seliya, Naeem},
doi = {10.1023/B:EMSE.0000027781.18360.9b},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Empirical Software Engineering/Comparative Assessment of Software Quality Classification Techniques An Empirical Case Study.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {analysis of variance,case-based reasoning,decision trees,expected cost of misclassification,logistic regression,software quality classification},
pages = {229--257},
title = {{Comparative Assessment of Software Quality Classification Techniques : An Empirical Case Study}},
volume = {9},
year = {2004}
}
@article{Kim2009,
author = {Kim, Ji-Hyun},
doi = {10.1016/j.csda.2009.04.009},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Computational Statistics \& Data Analysis/Estimating classification error rate Repeated cross-validation, repeated hold-out and bootstrap.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
number = {11},
pages = {3735--3745},
title = {{Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947309001601},
volume = {53},
year = {2009}
}
@inproceedings{Kim2011,
author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
booktitle = {Proceeding of the international conference on Software engineering (ICSE '11)},
doi = {10.1145/1985793.1985859},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceeding of the international conference on Software engineering (ICSE '11)/Dealing with noise in defect prediction.pdf:pdf},
isbn = {9781450304450},
keywords = {buggy changes,buggy files,defect prediction,noise resistance},
pages = {481--490},
title = {{Dealing with noise in defect prediction}},
year = {2011}
}
@inproceedings{Kim2006,
annote = {http://www.slideshare.net/tom.zimmermann/automatic-identification-of-bugintroducing-changes},
author = {Kim, Sunghun and Zimmermann, Thomas and Pan, Kai and Whitehead, E James},
booktitle = {Proceedings of the 21st IEEE/ACM International Conference on Automated Software Engineering},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the 21st IEEEACM International Conference on Automated Software Engineering/Automatic Identification of Bug-Introducing Changes.pdf:pdf},
pages = {81--90},
title = {{Automatic Identification of Bug-Introducing Changes}},
year = {2006}
}
@article{Klas2010,
abstract = {Defining strategies on how to perform quality assurance (QA) and how to control such activities is a challenging task for organizations developing or maintaining software and software-intensive systems. Planning and adjusting QA activities could benefit from accurate estimations of the expected defect content of relevant artifacts and the effectiveness of important quality assurance activities. Combining expert opinion with commonly available measurement data in a hybrid way promises to overcome the weaknesses of purely data-driven or purely expert-based estimation methods. This article presents a case study of the hybrid estimation method HyDEEP for estimating defect content and QA effectiveness in the telecommunication domain. The specific focus of this case study is the use of the method for gaining quantitative predictions. This aspect has not been empirically analyzed in previous work. Among other things, the results show that for defect content estimation, the method performs significantly better statistically than purely data-based methods, with a relative error of 0.3 on average (MMRE).},
author = {Klas, Michael and Elberzhager, Frank and Munch, Jurgen and Hartjes, Klaus and Graevemeyer, Olaf Von},
doi = {10.1145/1810295.1810313},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 ACMIEEE 32nd International Conference on Software Engineering/Transparent combination of expert and measurement data for defect prediction an industrial case study.pdf:pdf},
isbn = {978-1-60558-719-6},
issn = {0270-5257},
journal = {2010 ACM/IEEE 32nd International Conference on Software Engineering},
keywords = {HyDEEP,defect content,effectiveness,hybrid estimation},
pages = {119--128},
title = {{Transparent combination of expert and measurement data for defect prediction: an industrial case study}},
volume = {2},
year = {2010}
}
@article{Lee2011,
abstract = {There is a common belief that developers' behavioral interaction patterns may affect software quality. However, widely used defect prediction metrics such as source code metrics, change churns, and the number of previous defects do not capture developers' direct interactions. We propose 56 novel micro interaction metrics (MIMs) that leverage developers' interaction information stored in the Mylyn data. Mylyn is an Eclipse plug-in, which captures developers' interactions such as file editing and selection events with time spent. To evaluate the performance of MIMs in defect prediction, we build defect prediction (classification and regression) models using MIMs, traditional metrics, and their combinations. Our experimental results show that MIMs significantly improve defect classification and regression accuracy.},
author = {Lee, Taek and Nam, Jaechang and Han, DongGyun and Kim, Sunghun and In, Hoh Peter},
doi = {10.1145/2025113.2025156},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering/Micro Interaction Metrics for Defect Prediction.pdf:pdf},
isbn = {978-1-4503-0443-6},
journal = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
keywords = {Defect prediction,micro interaction metrics,mylyn},
pages = {311--321},
title = {{Micro Interaction Metrics for Defect Prediction}},
url = {http://doi.acm.org/10.1145/2025113.2025156},
year = {2011}
}
@article{Lessmann2008,
author = {Lessmann, Stefan and Member, Student and Baesens, Bart and Mues, Christophe and Pietsch, Swantje},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/IEEE Transactions on Software Engineering/Benchmarking Classification Models for Software Defect Prediction A Proposed Framework and Novel Findings.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
number = {4},
pages = {485--496},
title = {{Benchmarking Classification Models for Software Defect Prediction : A Proposed Framework and Novel Findings}},
volume = {34},
year = {2008}
}
@article{Li2006,
abstract = {Quantitatively-based risk management can reduce the risks associated with field defects for both software producers and software consumers. In this paper, we report experiences and results from initiating risk-management activities at a large systems development organization. The initiated activities aim to improve product testing (system/integration testing), to improve maintenance resource allocation, and to plan for future process improvements. The experiences we report address practical issues not commonly addressed in research studies: how to select an appropriate modeling method for product testing prioritization and process improvement planning, how to evaluate accuracy of predictions across multiple releases in time, and how to conduct analysis with incomplete information. In addition, we report initial empirical results for two systems with 13 and 15 releases. We present prioritization of configurations to guide product testing, field defect predictions within the first year of deployment to aid maintenance resource allocation, and important predictors across both systems to guide process improvement planning. Our results and experiences are steps towards quantitatively-based risk management.},
author = {Li, Paul Luo and Herbsleb, James and Shaw, Mary and Robinson, Brian},
doi = {10.1145/1134285.1134343},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceeding of the 28th international conference on Software engineering - ICSE '06/Experiences and results from initiating field defect prediction and product test prioritization efforts at ABB Inc.pdf:pdf},
isbn = {1595933751},
issn = {02705257},
journal = {Proceeding of the 28th international conference on Software engineering - ICSE '06},
keywords = {Software reliability modeling,System test prioritization,deployment and usage metrics,software and hardware configuration metrics},
pages = {413},
title = {{Experiences and results from initiating field defect prediction and product test prioritization efforts at ABB Inc.}},
url = {http://portal.acm.org/citation.cfm?doid=1134285.1134343},
year = {2006}
}
@article{Li2005,
abstract = {Open source software systems are important components of many business software applications. Field defect predictions for open source software systems may allow organizations to make informed decisions regarding open source software components. In this paper, we remotely measure and analyze predictors (metrics available before release) mined from established data sources (the code repository and the request tracking system) as well as a novel source of data (mailing list archives) for nine releases of OpenBSD. First, we attempt to predict field defects by extending a software reliability model fitted to development defects. We find this approach to be infeasible, which motivates examining metrics-based field defect prediction. Then, we evaluate 139 predictors using established statistical methods: Kendall's rank correlation, Pearson's rank correlation, and forward AIC model selection. The metrics we collect include product metrics, development metrics, deployment and usage metrics, and software and hardware configurations metrics. We find the number of messages to the technical discussion mailing list during the development period (a deployment and usage metric captured from mailing list archives) to be the best predictor of field defects. Our work identifies predictors of field defects in commonly available data sources for open source software systems and is a step towards metrics-based field defect prediction for quantitatively-based decision making regarding open source software components},
author = {Li, Paul Luo and Herbsleb, Jim and Shaw, Mary},
doi = {10.1109/METRICS.2005.26},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Proceedings - International Software Metrics Symposium/Finding predictors of field defects for open source software systems in commonly available data sources A case study of OpenBSD.pdf:pdf},
isbn = {0769523714},
issn = {15301435},
journal = {Proceedings - International Software Metrics Symposium},
keywords = {CVS repository,Deployment and usage metrics,Field defect prediction,Mailing list archives,Open source software,Reliability modeling,Request tracking system,Software and hardware configurations metrics},
number = {Metrics},
pages = {291--300},
pmid = {188},
title = {{Finding predictors of field defects for open source software systems in commonly available data sources: A case study of OpenBSD}},
volume = {2005},
year = {2005}
}
@article{Li2004,
abstract = {Defect-occurrence projection is necessary for the development of methods to mitigate the risks of software defect occurrences. In this paper, we examine user-reported software defect-occurrence patterns across twenty-two releases of four widely-deployed, business-critical, production, software systems: a commercial operating system, a commercial middleware system, an open source operating system (OpenBSD), and an open source middleware system (Tomcat). We evaluate the suitability of common defect-occurrence models by first assessing the match between characteristics of widely-deployed production software systems and model structures. We then evaluate how well the models fit real world data. We find that the Weibull model is flexible enough to capture defect-occurrence behavior across a wide range of systems. It provides the best model fit in 16 out of the 22 releases. We then evaluate the ability of the moving averages and the exponential smoothing methods to extrapolate Weibull model parameters using fitted model parameters from historical releases. Our results show that in 50\% of our forecasting experiments, these two naive parameter-extrapolation methods produce projections that are worse than the projection from using the same model parameters as the most recent release. These findings establish the need for further research on parameter-extrapolation methods that take into account variations in characteristics of widely-deployed, production, software systems across multiple releases.},
author = {Li, Paul Luo and Shaw, Mary and Herbsleb, Jim and Ray, Bonnie and Santhanam, P.},
doi = {10.1145/1041685.1029930},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/ACM SIGSOFT Software Engineering Notes/Empirical evaluation of defect projection models for widely-deployed production software systems.pdf:pdf},
isbn = {1-58113-855-5},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
pages = {263},
title = {{Empirical evaluation of defect projection models for widely-deployed production software systems}},
volume = {29},
year = {2004}
}
@inproceedings{Ma2007,
abstract = {Many statistical techniques have been proposed and introduced to predict fault-proneness of program modules in software engineering. Choosing the "best" candidate among many available models involves performance assessment and detailed comparison. But these comparisons are not simple due to varying performance measures and the related verification and validation cost implications. Therefore, a methodology for precise definition and evaluation of the predictive models is still needed. We believe the procedure we outline here, if followed, has a potential to enhance the statistical validity of future experiments.},
author = {Ma, Yan and Cukic, B.},
booktitle = {Proceedings of the International Workshop on Predictor Models in Software Engineering (PROMISE'07)},
doi = {10.1109/PROMISE.2007.1},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the International Workshop on Predictor Models in Software Engineering (PROMISE'07)/Adequate and Precise Evaluation of Quality Models in Software Engineering Studies.pdf:pdf},
isbn = {0-7695-2954-2},
pages = {1--9},
title = {{Adequate and Precise Evaluation of Quality Models in Software Engineering Studies}},
year = {2007}
}
@article{Ma2012,
author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
doi = {10.1016/j.infsof.2011.09.007},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Information and Software Technology/Transfer learning for cross-company software defect prediction.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
number = {3},
pages = {248--256},
publisher = {Elsevier B.V.},
title = {{Transfer learning for cross-company software defect prediction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584911001996},
volume = {54},
year = {2012}
}
@article{Madhavan2007,
abstract = {We present a tool that predicts whether the software under development inside an IDE has a bug. An IDE plugin performs this prediction, using the Change Classification technique to classify source code changes as buggy or clean during the editing session. Change Classification uses Support Vector Machines (SVM), a machine learning classifier algorithm, to classify changes to projects mined from their configuration management repository. This technique, besides being language independent and relatively accurate, can (a) classify a change immediately upon its completion and (b) use features extracted solely from the change delta (added, deleted) and the source code to predict buggy changes. Thus, integrating change classification within an IDE can predict potential bugs in the software as the developer edits the source code, ideally reducing the amount of time spent on fixing bugs later. To this end, we have developed a Change Classification plugin for Eclipse based on client-server architecture, described in this paper.},
author = {Madhavan, Janaki T. and Whitehead, E. James},
doi = {10.1145/1328279.1328287},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proc. ETX at OOPSLA/Predicting buggy changes inside an integrated development environment.pdf:pdf},
isbn = {9781605580159},
journal = {Proc. ETX at OOPSLA},
keywords = {bug prediction,integrated development environments},
number = {d},
pages = {36--40},
title = {{Predicting buggy changes inside an integrated development environment}},
url = {http://dl.acm.org/citation.cfm?id=1328279.1328287},
year = {2007}
}
@inproceedings{Mahaweerawat,
abstract = {This paper presents a new approach for predicting software faults by means of two-level clustering with unknown number of clusters. We employed Self-Organizing Map method and our proposed clustering approach in the first and second level, respectively, to classify historical and development data into clusters. Next we applied the Radial-Basis Function Network to predict software faults occurred in cluster components. In so doing, we were able to predict software faults reasonably accurate. I.},
author = {Mahaweerawat, Atchara and Sophatsathit, Peraphon and Lursinsap, Chidchanok},
booktitle = {JCSSE},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/JCSSE/Adaptive Self-Organizing Map Clustering for Software Fault Prediction.pdf:pdf},
title = {{Adaptive Self-Organizing Map Clustering for Software Fault Prediction}},
year = {2007}
}
@article{Mcintosha,
author = {Mcintosh, Shane and Adams, Bram and Nagappan, Meiyappan and Hassan, Ahmed E},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Mining Co-Change Information to Understand when Build Changes are Necessary.pdf:pdf},
title = {{Mining Co-Change Information to Understand when Build Changes are Necessary}}
}
@article{Meaney2014,
abstract = {BACKGROUND: In biomedical research, response variables are often encountered which have bounded support on the open unit interval--(0,1). Traditionally, researchers have attempted to estimate covariate effects on these types of response data using linear regression. Alternative modelling strategies may include: beta regression, variable-dispersion beta regression, and fractional logit regression models. This study employs a Monte Carlo simulation design to compare the statistical properties of the linear regression model to that of the more novel beta regression, variable-dispersion beta regression, and fractional logit regression models.

METHODS: In the Monte Carlo experiment we assume a simple two sample design. We assume observations are realizations of independent draws from their respective probability models. The randomly simulated draws from the various probability models are chosen to emulate average proportion/percentage/rate differences of pre-specified magnitudes. Following simulation of the experimental data we estimate average proportion/percentage/rate differences. We compare the estimators in terms of bias, variance, type-1 error and power. Estimates of Monte Carlo error associated with these quantities are provided.

RESULTS: If response data are beta distributed with constant dispersion parameters across the two samples, then all models are unbiased and have reasonable type-1 error rates and power profiles. If the response data in the two samples have different dispersion parameters, then the simple beta regression model is biased. When the sample size is small (N0 = N1 = 25) linear regression has superior type-1 error rates compared to the other models. Small sample type-1 error rates can be improved in beta regression models using bias correction/reduction methods. In the power experiments, variable-dispersion beta regression and fractional logit regression models have slightly elevated power compared to linear regression models. Similar results were observed if the response data are generated from a discrete multinomial distribution with support on (0,1).

CONCLUSIONS: The linear regression model, the variable-dispersion beta regression model and the fractional logit regression model all perform well across the simulation experiments under consideration. When employing beta regression to estimate covariate effects on (0,1) response data, researchers should ensure their dispersion sub-model is properly specified, else inferential errors could arise.},
author = {Meaney, Christopher and Moineddin, Rahim},
doi = {10.1186/1471-2288-14-14},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/BMC medical research methodology/A Monte Carlo simulation study comparing linear regression, beta regression, variable-dispersion beta regression and fractional logit re.pdf:pdf},
issn = {1471-2288},
journal = {BMC medical research methodology},
keywords = {Biomedical Research,Biomedical Research: statistics \& numerical data,Data Interpretation, Statistical,Linear Models,Monte Carlo Method},
month = jan,
number = {1},
pages = {14},
pmid = {24461057},
publisher = {BMC Medical Research Methodology},
title = {{A Monte Carlo simulation study comparing linear regression, beta regression, variable-dispersion beta regression and fractional logit regression at recovering average difference measures in a two sample design.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3999882\&tool=pmcentrez\&rendertype=abstract},
volume = {14},
year = {2014}
}
@article{Mende2010,
author = {Mende, T and Koschke, R},
doi = {10.1109/CSMR.2010.18},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the 14th European Conference on Software Maintenance and Reengineering (CSMR'10)/Effort-Aware Defect Prediction Models.pdf:pdf},
isbn = {978-1-61284-369-8},
journal = {Proceedings of the 14th European Conference on Software Maintenance and Reengineering (CSMR'10)},
keywords = {-defect},
month = mar,
pages = {107--116},
publisher = {Ieee},
title = {{Effort-Aware Defect Prediction Models}},
year = {2010}
}
@article{Mende2009,
abstract = {A plethora of defect prediction models has been proposed and empirically evaluated, often using standard classification performance measures. In this paper, we explore defect prediction models for a large, multi-release software system from the telecommunications domain. A history of roughly 3 years is analyzed to extract process and static code metrics that are used to build several defect prediction models with random forests. The performance of the resulting models is comparable to previously published work. Furthermore, we develop a new evaluation measure based on the comparison to an optimal model.},
author = {Mende, T. and Koschke, R. and Leszak, M.},
doi = {10.1109/CSMR.2009.55},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 13th European Conference on Software Maintenance and Reengineering/Evaluating Defect Prediction Models for a Large Evolving Software System.pdf:pdf},
isbn = {978-0-7695-3589-0},
issn = {1534-5351},
journal = {2009 13th European Conference on Software Maintenance and Reengineering},
keywords = {defect prediction},
number = {01},
pages = {247--250},
title = {{Evaluating Defect Prediction Models for a Large Evolving Software System}},
year = {2009}
}
@article{Mende2011,
abstract = {Testing is an important and cost-intensive part of the software development life cycle. Defect prediction models try to identify error-prone components, so that these can be tested earlier or more in-depth, and thus improve the cost-effectiveness during testing. Such models have been researched extensively, but whether and when they are applicable in practice is still debated. The applicability depends on many factors, and we argue that it cannot be analyzed without a specific scenario in mind. In this paper, we therefore present an analysis of the utility for one case study, based on data collected during the hardware/software integration test of a system from the avionic domain. An analysis of all defects found during this phase reveals that more than half of them are not identifiable by a code-based defect prediction model. We then investigate the predictive performance of different prediction models for the remaining defects. The small ratio of defective instances results in relatively poor performance. Our analysis of the cost-effectiveness then shows that the prediction model is not able to outperform simple models, which order files either randomly or by lines of code. Hence, in our setup, the application of defect prediction models does not offer any advantage in practice.},
author = {Mende, Thilo and Koschke, Rainer and Peleska, Jan},
doi = {10.1109/CSMR.2011.32},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/2011 15th European Conference on Software Maintenance and Reengineering/On the Utility of a Defect Prediction Model during HWSW Integration Testing A Retrospective Case Study.pdf:pdf},
isbn = {978-0-7695-4343-7},
issn = {1534-5351},
journal = {2011 15th European Conference on Software Maintenance and Reengineering},
keywords = {defect prediction models,empirical case study,software metrics},
pages = {259--268},
title = {{On the Utility of a Defect Prediction Model during HW/SW Integration Testing: A Retrospective Case Study}},
year = {2011}
}
@article{Mendes,
author = {Mendes, Emilia and Triggs, Chris and Street, Shortland and Counsell, Steve},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/A Comparison of Development Effort Estimation Techniques for Web Hypermedia Applications.pdf:pdf},
keywords = {case-based,effort,multiple regression models,prediction accuracy,prediction models,reasoning techniques,web hypermedia applications},
pages = {1--10},
title = {{A Comparison of Development Effort Estimation Techniques for Web Hypermedia Applications}}
}
@article{Meneely2008,
address = {New York, New York, USA},
author = {Meneely, Andrew and Williams, Laurie and Snipes, Will and Osborne, Jason},
doi = {10.1145/1453101.1453106},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering - SIGSOFT '08FSE-16/Predicting failures with developer networks and social network analysis.pdf:pdf},
isbn = {9781595939951},
journal = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering - SIGSOFT '08/FSE-16},
keywords = {developer network,failure prediction,logistic,negative binomial regression,regression,social network analysis},
pages = {13},
publisher = {ACM Press},
title = {{Predicting failures with developer networks and social network analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1453101.1453106},
year = {2008}
}
@article{Menzies2011,
author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
doi = {10.1109/ASE.2011.6100072},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/2011 26th IEEEACM International Conference on Automated Software Engineering (ASE 2011)/Local vs. global models for effort estimation and defect prediction(2).pdf:pdf},
isbn = {978-1-4577-1639-3},
journal = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
month = nov,
pages = {343--351},
publisher = {Ieee},
title = {{Local vs. global models for effort estimation and defect prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100072},
year = {2011}
}
@article{Menzies2011a,
author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
doi = {10.1109/ASE.2011.6100072},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/2011 26th IEEEACM International Conference on Automated Software Engineering (ASE 2011)/Local vs. global models for effort estimation and defect prediction.pdf:pdf},
isbn = {978-1-4577-1639-3},
journal = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
month = nov,
pages = {343--351},
publisher = {Ieee},
title = {{Local vs. global models for effort estimation and defect prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100072},
year = {2011}
}
@article{Menzies2007,
author = {Menzies, Tim and Greenwald, Jeremy and Frank, Art},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/IEEE Transactions on Software Engineering/Data Mining Static Code Attributes to Learn Defect Predictors.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
number = {1},
pages = {2--13},
title = {{Data Mining Static Code Attributes to Learn Defect Predictors}},
volume = {33},
year = {2007}
}
@article{Menzies2008,
abstract = {Context: There are many methods that input static code features and output a predictor for faulty code modules. These data mining methods have hit a "performance ceiling"; i.e., some inherent upper bound on the amount of information offered by, say, static code features when identifying modules which contain faults. Objective: We seek an explanation for this ceiling effect. Perhaps static code features have "limited information content"; i.e. their information can be quickly and completely discovered by even simple learners. Method: An initial literature review documents the ceiling effect in other work. Next, using three sub-sampling techniques (under-, over-, and micro-sampling), we look for the lower useful bound on the number of training instances. Results: Using micro-sampling, we find that as few as 50 instances yield as much information as larger training sets. Conclusions: We have found much evidence for the limited information hypothesis. Further progress in learning defect predictors may not come from better algorithms. Rather, we need to be improving the information content of the training data, perhaps with case-based reasoning methods. Copyright 2008 ACM.},
author = {Menzies, Tim and Turhan, Burak and Bener, Ayse and Gay, Gregory and Cukic, Bojan and Jiang, Yue},
doi = {10.1145/1370788.1370801},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 4th international workshop on Predictor models in software engineering - PROMISE '08/Implications of ceiling effects in defect predictors.pdf:pdf},
isbn = {9781605580364},
issn = {02705257},
journal = {Proceedings of the 4th international workshop on Predictor models in software engineering - PROMISE '08},
keywords = {Defect prediction,Naive bayes,Over-sampling,Under-sampling},
number = {i},
pages = {47},
title = {{Implications of ceiling effects in defect predictors}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-57049155106\&partnerID=tZOtx3y1},
year = {2008}
}
@article{Mittas2013,
author = {Mittas, Nikolaos and Angelis, Lefteris},
doi = {10.1109/TSE.2012.45},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/IEEE Transactions on Software Engineering/Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {4},
pages = {537--551},
title = {{Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6235961},
volume = {39},
year = {2013}
}
@article{Mizuno2007a,
author = {Mizuno, Osamu},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Techniques/Training on Errors Experiment to Detect Fault-Prone.pdf:pdf},
isbn = {9781595938114},
journal = {Techniques},
pages = {405--414},
title = {{Training on Errors Experiment to Detect Fault-Prone}},
year = {2007}
}
@article{Mizuno2007,
abstract = {Because of the increase of needs for spam e-mail detection, the spam filtering technique has been improved as a convenient and effective technique for text mining. We propose a novel approach to detect fault-prone modules in a way that the source code modules are considered as text files and are applied to the spam filter directly. In order to show the applicability of our approach, we conducted experimental applications using source code repositories of Java based open source developments. The result of experiments shows that our approach can classify more than 75\% of software modules correctly.},
author = {Mizuno, Osamu and Ikami, Shiro and Nakaichi, Shuya and Kikuno, Tohru},
doi = {10.1109/MSR.2007.29},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings - ICSE 2007 Workshops Fourth International Workshop on Mining Software Repositories, MSR 2007/Spam filter based approach for finding fault-prone software modules.pdf:pdf},
isbn = {076952950X},
journal = {Proceedings - ICSE 2007 Workshops: Fourth International Workshop on Mining Software Repositories, MSR 2007},
pages = {7--10},
title = {{Spam filter based approach for finding fault-prone software modules}},
year = {2007}
}
@article{Mockus2005,
abstract = { Predicting software quality as perceived by a customer may allow an organization to adjust deployment to meet the quality expectations of its customers, to allocate the appropriate amount of maintenance resources, and to direct quality improvement efforts to maximize the return on investment. However, customer perceived quality may be affected not simply by the software content and the development process, but also by a number of other factors including deployment issues, amount of usage, software platform, and hardware configurations. We predict customer perceived quality as measured by various service interactions, including software defect reports, requests for assistance, and field technician dispatches using the afore mentioned and other factors for a large telecommunications software system. We employ the non-intrusive data gathering technique of using existing data captured in automated project monitoring and tracking systems as well as customer support and tracking systems. We find that the effects of deployment schedule, hardware configurations, and software platform can increase the probability of observing a software failure by more than 20 times. Furthermore, we find that the factors affect all quality measures in a similar fashion. Our approach can be applied at other organizations, and we suggest methods to independently validate and replicate our results.},
author = {Mockus, a. and Zhang, Ping Zhang Ping and Li, P.L.},
doi = {10.1109/ICSE.2005.1553565},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005/Predictors of customer perceived software quality.pdf:pdf},
isbn = {1-59593-963-2},
journal = {Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.},
keywords = {metrics,modeling,quality},
pages = {225--233},
title = {{Predictors of customer perceived software quality}},
year = {2005}
}
@article{Molinaro2005,
abstract = {MOTIVATION: In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection. RESULTS: For small studies where features are selected from thousands of candidates, the resubstitution and simple split-sample estimates are seriously biased. In these small samples, leave-one-out cross-validation (LOOCV), 10-fold cross-validation (CV) and the .632+ bootstrap have the smallest bias for diagonal discriminant analysis, nearest neighbor and classification trees. LOOCV and 10-fold CV have the smallest bias for linear discriminant analysis. Additionally, LOOCV, 5- and 10-fold CV, and the .632+ bootstrap have the lowest mean square error. The .632+ bootstrap is quite biased in small sample sizes with strong signal-to-noise ratios. Differences in performance among resampling methods are reduced as the number of specimens available increase. SUPPLEMENTARY INFORMATION: A complete compilation of results and R code for simulations and analyses are available in Molinaro et al. (2005) (http://linus.nci.nih.gov/brb/TechReport.htm).},
author = {Molinaro, Annette M and Simon, Richard and Pfeiffer, Ruth M},
doi = {10.1093/bioinformatics/bti499},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Bioinformatics/Prediction error estimation a comparison of resampling methods.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Computer Simulation,Data Interpretation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic,Models,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Software,Statistical},
month = aug,
number = {15},
pages = {3301--3307},
pmid = {15905277},
title = {{Prediction error estimation: a comparison of resampling methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905277},
volume = {21},
year = {2005}
}
@article{Morasca2000,
abstract = {Knowledge discovery from software engineering measurement data is essential in deriving the right conclusions from experiments. Various data analysis techniques may provide data analysts with different and complementary insights into the studied phenomena. In this paper, two data analysis techniques – Rough Sets (RSs) and Logistic Regression (LR) are compared, from both the theoretical and the experimental point of view. In particular, the empirical study was performed as a part of the ESPRIT/ESSI project CEMP on a real-life maintenance project, the DATATRIEVE™ project carried out at Digital Engineering Italy. We have applied both techniques to the same data set. The goal of the experimental study was to predict module fault-proneness and to determine the major factors affecting software reliability in the application context. The results obtained with either analysis technique are discussed and compared. Then, a hybrid approach is built, by integrating different and complementary knowledge obtained from either approach on the fault-proneness of modules. This knowledge can be reused in the organizational framework of a company-wide experience factory.},
author = {Morasca, Sandro and Ruhe, G\"{u}nther},
doi = {10.1016/S0164-1212(00)00014-5},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Journal of Systems and Software/A hybrid approach to analyze empirical software engineering data and its application to predict module fault-proneness in maintenance.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {empirical case studies,experience factory,fault-proneness,hybrid approach,knowledge discovery,logistic regression,rough sets,software maintenance},
pages = {225--237},
title = {{A hybrid approach to analyze empirical software engineering data and its application to predict module fault-proneness in maintenance}},
volume = {53},
year = {2000}
}
@article{Myrtveit2011,
author = {Myrtveit, Ingunn and Stensrud, Erik},
doi = {10.1007/s10664-011-9183-7},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Empirical Software Engineering/Validity and reliability of evaluation procedures in comparative studies of effort prediction models.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {comparative studies,effort prediction,error,evaluation criteria,mean magnitude of relative,mmre,software cost estimation},
number = {1-2},
pages = {23--33},
title = {{Validity and reliability of evaluation procedures in comparative studies of effort prediction models}},
url = {http://link.springer.com/10.1007/s10664-011-9183-7},
volume = {17},
year = {2011}
}
@article{Nagappan2005,
abstract = { Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn. Using statistical regression models, we show that while absolute measures of code chum are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.},
author = {Nagappan, N. and Ball, T.},
doi = {10.1109/ICSE.2005.1553571},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005/Use of relative code churn measures to predict system defect density.pdf:pdf},
isbn = {1-59593-963-2},
journal = {Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.},
keywords = {defect density,fault-proneness,multiple,principal component analysis,regression,relative code churn},
title = {{Use of relative code churn measures to predict system defect density}},
year = {2005}
}
@article{Nagappan2008,
abstract = {Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.},
author = {Nagappan, Nachiappan and Murphy, Brendan and Basili, Victor R},
doi = {10.1145/1368088.1368160},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Organization/The Influence of Organizational Structure on Software Quality An Empirical Case Study.pdf:pdf},
isbn = {9781605580791},
journal = {Organization},
pages = {521--530},
title = {{The Influence of Organizational Structure on Software Quality : An Empirical Case Study}},
url = {http://portal.acm.org/citation.cfm?id=1368160},
year = {2008}
}
@article{Nagappan2010,
abstract = {In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90\%, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.},
author = {Nagappan, Nachiappan and Zeller, Andreas and Zimmermann, Thomas and Herzig, Kim and Murphy, Brendan},
doi = {10.1109/ISSRE.2010.25},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings - International Symposium on Software Reliability Engineering, ISSRE/Change bursts as defect predictors.pdf:pdf},
isbn = {9780769542553},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
keywords = {Change history,Defects,Developers,Empirical studies,Process metrics,Product metrics,Software mining,Software quality assurance,Version control},
pages = {309--318},
title = {{Change bursts as defect predictors}},
year = {2010}
}
@article{Nagwani2010,
abstract = {Software bug estimation is a very essential activity for effective and proper software project planning. All the software bug related data are kept in software bug repositories. Software bug (defect) repositories contains lot of useful information related to the development of a project. Data mining techniques can be applied on these repositories to discover useful interesting patterns. In this paper a prediction data mining technique is proposed to predict the software bug estimation from a software bug repository. A two step prediction model is proposed In the first step bug for which estimation is required, its summary and description is matched against the summary and description of bugs available in bug repositories. A weighted similarity model is suggested to match the summary and description for a pair of software bugs. In the second step the fix duration of all the similar bugs are calculated and stored and its average is calculated, which indicates the predicted estimation of a bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.},
author = {Nagwani, Naresh Kumar and Verma, Shrish},
doi = {10.1109/IADCC.2010.5422923},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 IEEE 2nd International Advance Computing Conference, IACC 2010/Predictive data mining model for software bug estimation using average weighted similarity.pdf:pdf},
isbn = {9781424447916},
journal = {2010 IEEE 2nd International Advance Computing Conference, IACC 2010},
keywords = {Bug estimation,Estimation prediction,Software bug repositories,Weighted similarity},
pages = {373--378},
title = {{Predictive data mining model for software bug estimation using average weighted similarity}},
year = {2010}
}
@article{Nguyen2010,
abstract = {Dependency network measures capture various facets of the dependencies among software modules. For example, betweenness centrality measures how much information flows through a module compared to the rest of the network. Prior studies have shown that these measures are good predictors of post-release failures. However, these studies did not explore the causes for such good performance and did not provide guidance for practitioners to avoid future bugs. In this paper, we closely examine the causes for such performance by replicating prior studies using data from the Eclipse project. Our study shows that a small subset of dependency network measures have a large impact on post-release failure, while other network measures have a very limited impact. We also analyze the benefit of bug prediction in reducing testing cost. Finally, we explore the practical implications of the important network measures.},
author = {Nguyen, Thanh H D and Adams, Bram and Hassan, Ahmed E.},
doi = {10.1109/ICSM.2010.5609560},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/IEEE International Conference on Software Maintenance, ICSM/Studying the impact of dependency network measures on software quality.pdf:pdf},
isbn = {9781424486298},
issn = {1063-6773},
journal = {IEEE International Conference on Software Maintenance, ICSM},
title = {{Studying the impact of dependency network measures on software quality}},
year = {2010}
}
@article{Ostrand2005,
author = {Ostrand, T J and Weyuker, E J and Bell, R M and Ostrand, R C},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/29th Annual International Computer Software and Applications Conference, COMPSAC 2005/A Different View of Fault Prediction.pdf:pdf},
isbn = {0-7695-2413-3},
journal = {29th Annual International Computer Software and Applications Conference, COMPSAC 2005},
keywords = {Empirical Research,Fault-based},
pages = {3--4},
title = {{A Different View of Fault Prediction}},
volume = {2},
year = {2005}
}
@article{Ostrand2005,
author = {Ostrand, T.J. and Weyuker, E.J. and Bell, R.M.},
doi = {10.1109/TSE.2005.49},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/IEEE Transactions on Software Engineering/Predicting the location and number of faults in large software systems.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = apr,
number = {4},
pages = {340--355},
title = {{Predicting the location and number of faults in large software systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1435354},
volume = {31},
year = {2005}
}
@article{Ostrand2002,
abstract = {A case study is presented using thirteen releases of$\backslash$na large industrial inventory tracking$\backslash$nsystem. Several types of questions are addressed in$\backslash$nthis study. The first involved examining how faults$\backslash$nare distributed over the different files. This$\backslash$nincluded making a distinction between the release$\backslash$nduring which they were discovered, the lifecycle$\backslash$nstage at which they were first detected, and the$\backslash$nseverity of the fault. The second category of$\backslash$nquestions we considered involved studying how the$\backslash$nsize of modules affected their fault density. This$\backslash$nincluded looking at questions like whether or not$\backslash$nfiles with high fault densities at early stages of$\backslash$nthe lifecycle also had high fault densities during$\backslash$nlater stages. A third type of question we considered$\backslash$nwas whether files that contained large numbers of$\backslash$nfaults during early stages of development, also had$\backslash$nlarge numbers of faults during later stages, and$\backslash$nwhether faultiness persisted from release to$\backslash$nrelease. Finally, we examined whether newly written$\backslash$nfiles were more fault-prone than ones that were$\backslash$nwritten for earlier releases of the product. The$\backslash$nultimate goal of this study is to help identify$\backslash$ncharacteristics of files that can be used as$\backslash$npredictors of fault-proneness, thereby helping$\backslash$norganizations determine how best to use their$\backslash$ntesting resources.},
author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
doi = {10.1145/566171.566181},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/ACM SIGSOFT Software Engineering Notes/The distribution of faults in a large industrial software system.pdf:pdf},
isbn = {1581135629},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {cal study,empiri-,empirical studies published that,fault-prone,few,importance,in spite of its,investigate issues relating,pareto,software faults,software testing,there have been relatively},
pages = {55},
title = {{The distribution of faults in a large industrial software system}},
volume = {27},
year = {2002}
}
@article{Pandey2010,
abstract = {Identification of fault-prone or not fault-prone modules is very essential to improve the reliability and quality of a software system. Once modules are categorized as fault-prone or not fault-prone, test effort are allocated accordingly. Testing effort and efficiency are primary concern and can be optimized by prediction and ranking of fault-prone modules. This paper discusses a new model for prediction and ranking of fault-prone software modules for test effort optimization. Model utilizes the classification capability of data mining techniques and knowledge stored in software metrics to classify the software module as fault-prone or not fault-prone. A decision tree is constructed using ID3 algorithm for the existing project data. Rules are derived form the decision tree and integrated with fuzzy inference system to classify the modules as either fault-prone or not fault-prone for the target data. The model is also able to rank the fault-prone module on the basis of its degree of fault-proneness. The model accuracy are validated and compared with some other models by using the NASA projects data set of PROMOSE repository.},
author = {Pandey, Ajeet Kumar and Goyal, Neeraj Kumar},
doi = {10.1109/ICRESH.2010.5779531},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 2nd International Conference on Reliability, Safety and Hazard, ICRESH-2010 Risk-Based Technology and Physics-of-Failure Methods/Test effort optimization by prediction and ranking of fault-prone software modules.pdf:pdf},
isbn = {9781424483433},
journal = {2010 2nd International Conference on Reliability, Safety and Hazard, ICRESH-2010: Risk-Based Technology and Physics-of-Failure Methods},
keywords = {ID3 algorithm,fault-prone modules,fuzzy inference system (FIS),software metrics,software testing},
pages = {136--142},
title = {{Test effort optimization by prediction and ranking of fault-prone software modules}},
year = {2010}
}
@inproceedings{Pinzger2007,
author = {Pinzger, Martin and Murphy, Brendan},
booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering (FSE'07)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering (FSE'07)/Can Developer-Module Networks Predict Failures.pdf:pdf},
isbn = {9781595939951},
keywords = {contribution network,failure prediction,network centrality measures,social network analysis},
pages = {2--12},
title = {{Can Developer-Module Networks Predict Failures ?}},
year = {2007}
}
@article{Raeder2010,
author = {Raeder, Troy and Hoens, T. Ryan and Chawla, Nitesh V.},
doi = {10.1109/ICDM.2010.110},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 IEEE International Conference on Data Mining/Consequences of Variability in Classifier Performance Estimates.pdf:pdf},
isbn = {978-1-4244-9131-5},
journal = {2010 IEEE International Conference on Data Mining},
month = dec,
pages = {421--430},
publisher = {Ieee},
title = {{Consequences of Variability in Classifier Performance Estimates}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693996},
year = {2010}
}
@article{Ratzinger2007,
abstract = {Defect density and defect prediction are essential for efficient resource allocation in software evolution. In an empirical study we applied data mining techniques for value series based on evolution attributes such as number of authors, commit messages, lines of code, bug fix count, etc. Daily data points of these evolution attributes were captured over a period of two months to predict the defects in the subsequent two months in a project. For that, we developed models utilizing genetic programming and linear regression to accurately predict software defects. In our study, we investigated the data of three independent projects, two open source and one commercial software system. The results show that by utilizing series of these attributes we obtain models with high correlation coefficients (between 0.716 and 0.946). Further, we argue that prediction models based on series of a single variable are sometimes superior to the model including all attributes: in contrast to other studies that resulted in size or complexity measures as predictors, we have identified the number of authors and the number of commit messages to versioning systems as excellent predictors of defect densities.},
author = {Ratzinger, Jacek and Gall, Harald and Pinzger, Martin},
doi = {10.1109/WCRE.2007.39},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings - Working Conference on Reverse Engineering, WCRE/Quality assessment based on attribute series of software evolution.pdf:pdf},
isbn = {0769530346},
issn = {10951350},
journal = {Proceedings - Working Conference on Reverse Engineering, WCRE},
pages = {80--89},
title = {{Quality assessment based on attribute series of software evolution}},
year = {2007}
}
@article{Reformat2003,
author = {Reformat, Marek},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/Unknown/A Fuzzy-Based Meta-model for Reasoning.pdf:pdf},
pages = {644--651},
title = {{A Fuzzy-Based Meta-model for Reasoning}},
year = {2003}
}
@article{Sadowski2011,
author = {Sadowski, Caitlin and Lewis, Chris and Lin, Zhongpeng and Zhu, Xiaoyan and Whitehead, E James},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the 8th IEEE Working Conference on Mining Software Repositories (MSR'11)/An Empirical Analysis of the FixCache Algorithm.pdf:pdf},
isbn = {9781450305747},
journal = {Proceedings of the 8th IEEE Working Conference on Mining Software Repositories (MSR'11)},
keywords = {algorithms,experimentation,fixcache,measurement,software bug prediction},
pages = {0--3},
title = {{An Empirical Analysis of the FixCache Algorithm}},
year = {2011}
}
@article{Schneidewind2001,
abstract = {Investigates the possibility that logistic regression functions
(LRFs), when used in combination with Boolean discriminant functions
(BDFs), which we had previously developed, would improve the quality
classification ability of BDFs when used alone; this was found to be the
case. When the union of a BDF and LRF was used to classify quality, the
predictive accuracy of quality and inspection cost was improved over
that of using either function alone for the Space Shuttle. Also, the
LRFs proved useful for ranking the quality of modules in a build. The
significance of these results is that very high-quality classification
accuracy (1.25\% error) can be obtained while reducing the inspection
cost incurred in achieving high quality. This is particularly important
for safety-critical systems. Because the methods are general and not
particular to the Shuttle, they could be applied to other domains. A key
part of the LRF development was a method for identifying the critical
value (i.e. threshold) that could discriminate between high and low
quality, and at the same time constrain the cost of inspection to a
reasonable value},
author = {Schneidewind, N.F.},
doi = {10.1109/METRIC.2001.915540},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2001/Proceedings Seventh International Software Metrics Symposium/Investigation of logistic regression as a discriminant of software quality.pdf:pdf},
isbn = {0-7695-1043-4},
issn = {1530-1435},
journal = {Proceedings Seventh International Software Metrics Symposium},
keywords = {boolean discriminant functions,do this research for,following reasons - each,logistic,reason associated with a,regression functions,software quality prediction,the,we were motivated to},
pages = {328--337},
title = {{Investigation of logistic regression as a discriminant of software
quality}},
year = {2001}
}
@article{Zeller2008,
abstract = {The analysis of bug databases reveals that some software components are far more failure-prone than others. Yet it is hard to find properties that are universally shared by failure-prone components. We have mined the Eclipse bug and version databases to map failures to Eclipse components. The resulting data set lists the defect density of all Eclipse components, and may thus help to find features that predict how defect-prone a component will be. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Schroter, Adrian and Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
doi = {10.1007/978-3-540-77966-7\_4},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Where do bugs come from.pdf:pdf},
isbn = {3540779647},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {6},
pages = {16},
title = {{Where do bugs come from?}},
volume = {4899 LNCS},
year = {2008}
}
@article{Seiffert2009,
abstract = {Software-quality data sets tend to fall victim to the <i>class-imbalance</i> problem that plagues so many other application domains. The majority of faults in a software system, particularly high-assurance systems, usually lie in a very small percentage of the software modules. This imbalance between the number of fault-prone (fp) and non-fp (nfp) modules can have a severely negative impact on a data-mining technique's ability to differentiate between the two. This paper addresses the class-imbalance problem as it pertains to the domain of software-quality prediction. We present a comprehensive empirical study examining two different methodologies, data sampling and boosting, for improving the performance of decision-tree models designed to identify fp software modules. This paper applies five data-sampling techniques and boosting to 15 software-quality data sets of different sizes and levels of imbalance. Nearly 50 000 models were built for the experiments contained in this paper. Our results show that while data-sampling techniques are very effective in improving the performance of such models, boosting almost always outperforms even the best data-sampling techniques. This significant result, which, to our knowledge, has not been previously reported, has important consequences for practitioners developing software-quality classification models.},
author = {Seiffert, Chris and Khoshgoftaar, Taghi M. and {Van Hulse}, Jason},
doi = {10.1109/TSMCA.2009.2027131},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/IEEE Transactions on Systems, Man, and Cybernetics Part ASystems and Humans/Improving software-quality predictions with data sampling and boosting.pdf:pdf},
issn = {10834427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans},
keywords = {Binary classification,Boosting,Class imbalance,Classification,Sampling,Software quality},
number = {6},
pages = {1283--1294},
title = {{Improving software-quality predictions with data sampling and boosting}},
volume = {39},
year = {2009}
}
@article{Seliya2010,
abstract = {Reducing the number of latent software defects is a development goal that is particularly applicable to high assurance software systems. For such systems, the software measurement and defect data is highly skewed toward the not-fault-prone program modules, i.e., the number of fault-prone modules is relatively very small. The skewed data problem, also known as class imbalance, poses a unique challenge when training a software quality estimation model. However, practitioners and researchers often build defect prediction models without regard to the skewed data problem. In high assurance systems, the class imbalance problem must be addressed when building defect predictors. This study investigates the roughly balanced bagging (RBBag) algorithm for building software quality models with data sets that suffer from class imbalance. The algorithm combines bagging and data sampling into one technique. A case study of 15 software measurement data sets from different real-world high assurance systems is used in our investigation of the RBBag algorithm. Two commonly used classification algorithms in the software engineering domain, Naive Bayes and C4.5 decision tree, are combined with RBBag for building the software quality models. The results demonstrate that defect prediction models based on the RBBag algorithm significantly outperform models built without any bagging or data sampling. The RBBag algorithm provides the analyst with a tool for effectively addressing class imbalance when training defect predictors during high assurance software development.},
author = {Seliya, Naeem and Khoshgoftaar, Taghi M. and {Van Hulse}, Jason},
doi = {10.1109/HASE.2010.29},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of IEEE International Symposium on High Assurance Systems Engineering/Predicting faults in high assurance software.pdf:pdf},
isbn = {9780769542928},
issn = {15302059},
journal = {Proceedings of IEEE International Symposium on High Assurance Systems Engineering},
keywords = {Bagging,Classification,Data sampling,Defect prediction,Imbalanced data,Software measurements},
pages = {26--34},
title = {{Predicting faults in high assurance software}},
year = {2010}
}
@article{Shepperd2013,
abstract = {Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.},
author = {Shepperd, Martin and Song, Qinbao and Sun, Zhongbin and Mair, Carolyn},
doi = {10.1109/TSE.2013.11},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/IEEE Transactions on Software Engineering/Data quality Some comments on the NASA software defect datasets.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Empirical software engineering,data quality,defect prediction,machine learning},
number = {9},
pages = {1208--1215},
title = {{Data quality: Some comments on the NASA software defect datasets}},
volume = {39},
year = {2013}
}
@article{Sherriff2007,
abstract = {Static analysis tools tend to generate more alerts than a development team can reasonably examine without some form of guidance. In this paper, we propose a technique for leveraging field failures and historical change records to determine which sets of alerts are often associated with a field failure using singular value decomposition. We performed a case study on six major components of an industrial software system at IBM over six builds spanning eighteen months of development. Our technique identified fourteen alert types that comprised sets of alerts that could identify, on average, 45\% of future fault-prone files and up to 65\% in some instances. Copyright \&copy; 2007 Mark Sherriff, Sarah Smith Heckman, Mike Lake, Laurie Williams, and IBM Corp.},
author = {Sherriff, Mark and Heckman, Sarah Smith and Lake, Mike and Williams, Laurie},
doi = {http://doi.acm.org/10.1145/1321211.1321247},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the 2007 Conference of the Center for Advanced Studies on Collaborative Research, CASCON '07/Identifying fault-prone files using static analysis alerts through singular value decomposition.pdf:pdf},
journal = {Proceedings of the 2007 Conference of the Center for Advanced Studies on Collaborative Research, CASCON '07},
keywords = {Research;Static analysis;Statistical process contr},
pages = {276--279},
title = {{Identifying fault-prone files using static analysis alerts through singular value decomposition}},
url = {http://dx.doi.org/10.1145/1321211.1321247},
year = {2007}
}
@article{Sherriff2005,
abstract = {Early estimation of defect density of a product is an important step towards the remediation of the problem associated with affordably guiding corrective actions in the software development process. This paper presents a suite of in-process metrics that leverages the software testing effort to create a defect density prediction model for use throughout the software development process. A case study conducted with Galois Connections, Inc. in a Haskell programming environment indicates that the resulting defect density prediction is indicative of the actual system defect density. Copyright 2005 ACM.},
author = {Sherriff, Mark and Nagappan, Nachiappan and Williams, Laurie and Vouk, Mladen},
doi = {10.1145/1082983.1083285},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/ACM SIGSOFT Software Engineering Notes/Early estimation of defect density using an in-process Haskell metrics model.pdf:pdf},
isbn = {1595931155},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {empirical software engineering,haskell,measurement,reliability,software quality},
pages = {1},
title = {{Early estimation of defect density using an in-process Haskell metrics model}},
volume = {30},
year = {2005}
}
@inproceedings{Shihab2010,
author = {Shihab, Emad and Jiang, Zhen Ming and Ibrahim, Walid M and Adams, Bram and Hassan, Ahmed E},
booktitle = {Proceedings of the 4th International Symposium on Empirical Software Engineering and Measurement (ESEM'10)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the 4th International Symposium on Empirical Software Engineering and Measurement (ESEM'10)/Understanding the Impact of Code and Process Metrics on Post-release Defects A Case Study on the Eclipse Project.pdf:pdf},
title = {{Understanding the Impact of Code and Process Metrics on Post-release Defects : A Case Study on the Eclipse Project}},
year = {2010}
}
@inproceedings{Shihaba,
author = {Shihab, Emad and Kamei, Yasutaka and Adams, Bram and Hassan, Ahmed E},
booktitle = {Proceedings of the European Conference on Foundations of Software Engineering (ESEC/FSE'11)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the European Conference on Foundations of Software Engineering (ESECFSE'11)/High-Impact Defects A Study of Breakage and Surprise Defects Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450304436},
keywords = {all or part of,defect prediction,high-impact,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,process metrics,provided that copies are,this work for},
pages = {300--310},
title = {{High-Impact Defects: A Study of Breakage and Surprise Defects Categories and Subject Descriptors}},
year = {2011}
}
@article{Shin2009,
author = {Shin, Yonghee and Bell, Robert and Ostrand, Thomas and Weyuker, Elaine},
doi = {10.1109/MSR.2009.5069481},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 6th IEEE International Working Conference on Mining Software Repositories/Does calling structure information improve the accuracy of fault prediction.pdf:pdf},
isbn = {978-1-4244-3493-0},
journal = {2009 6th IEEE International Working Conference on Mining Software Repositories},
keywords = {[Electronic Manuscript]},
pages = {61--70},
title = {{Does calling structure information improve the accuracy of fault prediction?}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5069481},
year = {2009}
}
@article{Singh2008,
abstract = {Importance of construction of models for predicting software quality attributes is increasing leading to usage of artificial intelligence techniques such as Artificial Neural Network (ANN). The goal of this paper is to empirically compare traditional strategies such as Logistic Regression (LR) and ANN to assess software quality. The study used data collected from public domain NASA data set. We find the effect of software metrics on fault proneness. The fault proneness models were predicted using LR regression and ANN methods. The performance of the two methods was compared by Receiver Operating Characteristic (ROC) analysis. The areas under the ROC curves are 0.78 and 0.745 for the LR and ANN model, respectively. The predicted model shows that software metrics are related to fault proneness. The models predict faulty classes with more than 70 percent accuracy. The study showed that ANN method can also be used in constructing software quality models and more similar studies should further investigate the issue. Based on these results, it is reasonable to claim that such a model could help for planning and executing testing by focusing resources on fault-prone parts of the design and code.},
author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
doi = {10.1007/978-3-540-69566-0\_18},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Unknown/Predicting Software Fault Proneness Model Using Neural Network.pdf:pdf},
isbn = {978-3-540-69564-6},
keywords = {artificial neural network,empirical validation,metrics,software quality},
pages = {204--214},
title = {{Predicting Software Fault Proneness Model Using Neural Network}},
url = {http://www.springerlink.com/content/ym5rq75lr2641424/?p=883a01a0848545f7bde3ae63d15f1e96\&pi=17},
year = {2008}
}
@article{Singh2009a,
abstract = {Empirical validation of software metrics used to predict software quality attributes is important to ensure their practical relevance in software organizations. The aim of this work is to find the relation of object-oriented (OO) metrics with fault proneness at different severity levels of faults. For this purpose, different prediction models have been developed using regression and machine learning methods. We evaluate and compare the performance of these methods to find which method performs better at different severity levels of faults and empirically validate OO metrics given by Chidamber and Kemerer. The results of the empirical study are based on public domain NASA data set. The performance of the predicted models was evaluated using Receiver Operating Characteristic (ROC) analysis. The results show that the area under the curve (measured from the ROC analysis) of models predicted using high severity faults is low as compared with the area under the curve of the model predicted with respect to medium and low severity faults. However, the number of faults in the classes correctly classified by predicted models with respect to high severity faults is not low. This study also shows that the performance of machine learning methods is better than logistic regression method with respect to all the severities of faults. Based on the results, it is reasonable to claim that models targeted at different severity levels of faults could help for planning and executing testing by focusing resources on fault-prone parts of the design and code that are likely to cause serious failures.},
author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
doi = {10.1007/s11219-009-9079-6},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Software Quality Journal/Empirical validation of object-oriented metrics for predicting fault proneness models.pdf:pdf},
issn = {09639314},
journal = {Software Quality Journal},
keywords = {Empirical validation,Fault prediction,Metrics,Object-oriented,Receiver operating characteristics analysis,Software quality},
pages = {3--35},
title = {{Empirical validation of object-oriented metrics for predicting fault proneness models}},
volume = {18},
year = {2009}
}
@article{Starkweather1996,
author = {Starkweather, Jon and Support, Statistical},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1996/Unknown/Cross Validation techniques in R A brief overview of some methods , packages , and functions for assessing prediction models .pdf:pdf},
title = {{Cross Validation techniques in R : A brief overview of some methods , packages , and functions for assessing prediction models .}},
year = {1996}
}
@article{Steyerberg2000,
abstract = {Logistic regression analysis may well be used to develop a prognostic model for a dichotomous outcome. Especially when limited data are available, it is difficult to determine an appropriate selection of covariables for inclusion in such models. Also, predictions may be improved by applying some sort of shrinkage in the estimation of regression coefficients. In this study we compare the performance of several selection and shrinkage methods in small data sets of patients with acute myocardial infarction, where we aim to predict 30-day mortality. Selection methods included backward stepwise selection with significance levels alpha of 0.01, 0.05, 0. 157 (the AIC criterion) or 0.50, and the use of qualitative external information on the sign of regression coefficients in the model. Estimation methods included standard maximum likelihood, the use of a linear shrinkage factor, penalized maximum likelihood, the Lasso, or quantitative external information on univariable regression coefficients. We found that stepwise selection with a low alpha (for example, 0.05) led to a relatively poor model performance, when evaluated on independent data. Substantially better performance was obtained with full models with a limited number of important predictors, where regression coefficients were reduced with any of the shrinkage methods. Incorporation of external information for selection and estimation improved the stability and quality of the prognostic models. We therefore recommend shrinkage methods in full models including prespecified predictors and incorporation of external information, when prognostic models are constructed in small data sets.},
author = {Steyerberg, E W and Eijkemans, M J and {Harrell Jr.}, F E and Habbema, J D},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Statistics in Medicine/Prognostic modelling with logistic regression analysis a comparison of selection and estimation methods in small data sets.pdf:pdf},
isbn = {0277-6715 (Print)$\backslash$r0277-6715 (Linking)},
issn = {0277-6715},
journal = {Statistics in Medicine},
keywords = {*Logistic Models,*Models,*Regression Analysis,Age Factors,Aged,Female,Forecasting,Humans,Male,Myocardial Infarction/*mortality,Prognosis,Risk Factors,Sample Size,Sex Factors,Statistical},
pages = {1059--1079},
pmid = {10790680},
title = {{Prognostic modelling with logistic regression analysis: a comparison of selection and estimation methods in small data sets}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10790680},
volume = {19},
year = {2000}
}
@article{Stone1974,
author = {Stone, M},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1974/Journal of the Royal Statistical Society/Cross-Validatory Choice and Assessment of Statistical Predictions.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
keywords = {crossvalidation},
number = {2},
pages = {111--147},
title = {{Cross-Validatory Choice and Assessment of Statistical Predictions}},
volume = {36},
year = {1974}
}
@article{Stringfellow2002,
abstract = {Defect analysis of software components can be used to guide testing, with the goal of focusing on parts of the software that were fault-prone in earlier releases or earlier life cycle phases, such as development. We replicate a study that adapted a reverse architecting technique using defect reports to derive fault architectures. A fault architecture determines and visualizes components that are fault-prone in their relationships with other components, as well as those that are locally fault-prone. Our case study uses defect data from three releases of a large medical record system to identify relationships among system components, based on whether they are involved in the same defect report. We investigate measures that assess the fault-proneness of components and component relationships. Component relationships are used to derive a fault architecture. The resulting fault architecture indicates what the most fault-prone relationships are in a release. We also apply the technique in a new way. Not only do we derive fault architectures for each release, we derive fault architectures for the development, system test and post release phases within each release. Comparing across releases, makes it possible to see whether some components are repeatedly in fault-prone relationships. Comparing across phases, makes it possible to see whether development fault architectures can be used to identify those parts of the software that need to be tested more. We validate our predictions using system test data from the same release. We also use the development and system test fault architectures to identify fault-prone components after release, and validate our predictions using post release data.},
author = {Stringfellow, C. and Andrews, a.},
doi = {10.1023/A:1022138004472},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Software Quality Journal/Deriving a Fault Architecture to Guide Testing.pdf:pdf},
isbn = {09639314 (ISSN)},
issn = {09639314},
journal = {Software Quality Journal},
keywords = {Fault-architecture,Fault-proneness,Software testing},
pages = {299--330},
title = {{Deriving a Fault Architecture to Guide Testing}},
volume = {10},
year = {2002}
}
@article{Tarvo2008,
abstract = {Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.},
author = {Tarvo, a.},
doi = {10.1109/ISSRE.2008.21},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/2008 19th International Symposium on Software Reliability Engineering (ISSRE)/Using Statistical Models to Predict Software Regressions.pdf:pdf},
isbn = {978-0-7695-3405-3},
issn = {1071-9458},
journal = {2008 19th International Symposium on Software Reliability Engineering (ISSRE)},
keywords = {software metrics,software regression,statistical model,testing},
title = {{Using Statistical Models to Predict Software Regressions}},
year = {2008}
}
@inproceedings{Thung2012a,
author = {Thung, Ferdian and Lo, David and Jiang, Lingxiao and Rahman, Foyzur and Devanbu, Premkumar T},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering (ASE'12)},
doi = {10.1145/2351676.2351685},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the 27th IEEEACM International Conference on Automated Software Engineering (ASE'12)/To what extent could we detect field defects an empirical study of false negatives in static bug finding tools.pdf:pdf},
isbn = {9781450312042},
keywords = {false negatives,field defects,static bug-finding tools},
pages = {50--59},
title = {{To what extent could we detect field defects? an empirical study of false negatives in static bug finding tools}},
year = {2012}
}
@article{Tian2012,
author = {Tian, Yuan and Lo, David and Sun, Chengnian},
doi = {10.1109/WCRE.2012.31},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/2012 19th Working Conference on Reverse Engineering/Information Retrieval Based Nearest Neighbor Classification for Fine-Grained Bug Severity Prediction.pdf:pdf},
isbn = {978-0-7695-4891-3},
journal = {2012 19th Working Conference on Reverse Engineering},
month = oct,
pages = {215--224},
publisher = {Ieee},
title = {{Information Retrieval Based Nearest Neighbor Classification for Fine-Grained Bug Severity Prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6385117},
year = {2012}
}
@article{Tosun2010,
abstract = {Context: Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results. Objective: In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software. Method: We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Na??ve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Na??ve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost-benefit analysis to show that our approach can be efficiently put into practice. Results: Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22\%, the use of dependencies between modules further reduced false alarms by 8\%, and the decision threshold optimization for the Na??ve Bayes classifier using code metrics and version history information further improved false alarms by 30\% in comparison to a prediction using only code metrics and a default decision threshold. Conclusion: Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88\%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Na??ve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Tosun, Aye and Bener, Aye and Turhan, Burak and Menzies, Tim},
doi = {10.1016/j.infsof.2010.06.006},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Information and Software Technology/Practical considerations in deploying statistical methods for defect prediction A case study within the Turkish telecommunications indus.pdf:pdf},
isbn = {9781605586342},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Experience report,Na??ve Bayes,Software defect prediction,Static code attributes},
pages = {1242--1257},
title = {{Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry}},
volume = {52},
year = {2010}
}
@article{Turhan2008,
abstract = {Recent research on static code attribute (SCA) based defect prediction suggests that a performance ceiling has been achieved and this barrier can be exceeded by increasing the information content in data. In this research we propose static call graph based ranking (CGBR) framework, which can be applied to any defect prediction model based on SCA. In this framework, we model both intra module properties and inter module relations. Our results show that defect predictors using CGBR framework can detect the same number of defective modules, while yielding significantly lower false alarm rates. On industrial public data, we also show that using CGBR framework can improve testing efforts by 23\%.},
author = {Turhan, B. and Kocak, G. and Bener, a.},
doi = {10.1109/SEAA.2008.52},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/2008 34th Euromicro Conference Software Engineering and Advanced Applications/Software Defect Prediction Using Call Graph Based Ranking (CGBR) Framework.pdf:pdf},
isbn = {978-0-7695-3276-9},
issn = {1089-6503},
journal = {2008 34th Euromicro Conference Software Engineering and Advanced Applications},
keywords = {call graph,cost-benefit analysis,defect prediction},
pages = {191--198},
title = {{Software Defect Prediction Using Call Graph Based Ranking (CGBR) Framework}},
year = {2008}
}
@article{Wang2010b,
abstract = {Feature selection has become the essential step in many data mining applications. Using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. We present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly-used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 13,600 classification models. Experimental results indicate that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.},
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
doi = {10.1109/ICMLA.2010.27},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 Ninth International Conference on Machine Learning and Applications/A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction.pdf:pdf},
isbn = {978-1-4244-9211-4},
journal = {2010 Ninth International Conference on Machine Learning and Applications},
keywords = {defect prediction,ensembles,feature ranking},
pages = {135--140},
title = {{A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction}},
year = {2010}
}
@inproceedings{Wang2012,
address = {New York, New York, USA},
author = {Wang, Jue and Zhang, Hongyu},
booktitle = {Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '12},
doi = {10.1145/2372251.2372287},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '12/Predicting defect numbers based on defect state transition models.pdf:pdf},
isbn = {9781450310567},
keywords = {defect numbers,defect prediction,defect state transitions,defect-,fixing performance,markov models},
pages = {191},
publisher = {ACM Press},
title = {{Predicting defect numbers based on defect state transition models}},
url = {http://dl.acm.org/citation.cfm?doid=2372251.2372287},
year = {2012}
}
@article{Wang2010a,
author = {Wang, Tao and Li, Wei-hua},
doi = {10.1109/CISE.2010.5677057},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/2010 International Conference on Computational Intelligence and Software Engineering/Naive Bayes Software Defect Prediction Model.pdf:pdf},
isbn = {978-1-4244-5391-7},
journal = {2010 International Conference on Computational Intelligence and Software Engineering},
keywords = {-software defect prediction,Na\"{\i}ve Bayes Software Defect Prediction Model,na\"{\i}ve bayes,software metrics,statistical methods},
number = {2006},
pages = {1--4},
title = {{Naive Bayes Software Defect Prediction Model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5677057},
year = {2010}
}
@article{Weyuker2008,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Empirical Software Engineering/Do too many cooks spoil the broth Using the number of developers to enhance defect prediction models.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {empirical study,negative binomial model,software faults},
month = jul,
number = {5},
pages = {539--559},
title = {{Do too many cooks spoil the broth? Using the number of developers to enhance defect prediction models}},
volume = {13},
year = {2008}
}
@article{Weyuker2009,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
doi = {10.1007/s10664-009-9111-2},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Empirical Software Engineering/Comparing the effectiveness of several modeling methods for fault prediction.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {bayesian trees,empirical study,fault prediction,fault-percentile-average,negative binomial,random forests,recursive partitioning},
month = jun,
number = {3},
pages = {277--295},
title = {{Comparing the effectiveness of several modeling methods for fault prediction}},
url = {http://link.springer.com/10.1007/s10664-009-9111-2},
volume = {15},
year = {2009}
}
@article{Wohlin2000,
abstract = {The paper presents a method proposal of how to use product
measures and defect data to enable understanding and identification of
design and programming constructs that contribute more than expected to
the defect statistics. The paper describes a method that can be used to
identify the most defect-prone design and programming constructs and the
method proposal is illustrated on data collected from a large software
project in the telecommunication domain. The example indicates that it
is feasible, based on defect data and product measures, to identify the
main sources of defects in terms of design and programming constructs.
Potential actions to be taken include less usage of particular design
and programming constructs, additional resources for verification of the
constructs and further education into how to use the constructs},
author = {Wohlin, C. and Host, M. and Ohlsson, M.C.},
doi = {10.1109/WPC.2000.852475},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Proceedings IWPC 2000. 8th International Workshop on Program Comprehension/Understanding the sources of software defects a filtering approach.pdf:pdf},
isbn = {0-7695-0656-9},
issn = {1092-8138},
journal = {Proceedings IWPC 2000. 8th International Workshop on Program Comprehension},
keywords = {defect understanding,ments,prediction system,product measure-,root cause analysis,software metrics},
title = {{Understanding the sources of software defects: a filtering approach}},
year = {2000}
}
@article{Wong2000,
abstract = {Three metrics were used to extract design information from
existing code to identify structural stress points in a software system
being analyzed: D<sub>i</sub>, an internal design metric which
incorporates factors related to a module's internal structure; D<sub>e
</sub>, an external design metric which focuses on a module's external
relationships to other modules in the software system; and D(G), a
composite design metric which is the sum of D<sub>i</sub> and D<sub>e
</sub>. Since stress point modules generally have a high probability for
being fault-prone, project managers can use the information to determine
where additional testing effort should be spent and assign these modules
to more experienced programmers if modifications are needed. To make the
analysis more accurate and efficient, a design metrics analyzer
(<sub>\&amp;chi;</sub>Metrics) was implemented. We conducted experiments
using <sub>\&amp;chi;</sub>Metrics on part of a distributed software system,
written in C, with a client-server architecture, and identified a small
percentage of its functions as good candidates for fault proneness.
Files containing these functions were then validated by the real defect
data collected from a recent major release to its next release for their
fault proneness. Normalized metrics values were also computed by
dividing the D<sub>i</sub>, D<sub>e</sub>, and D(G) values by the
corresponding function size determined by non-blank and non-comment
lines of code to study the possible impact of function size on these
metrics. Results indicate that function size has little impact on the
predictive quality of our design metrics in identifying fault-prone
functions},
author = {Wong, W. Eric and Horgan, Joseph R. and Syring, Michael and Zage, Wayne and Zage, Dolores},
doi = {10.1002/1097-024X(20001125)30:14<1587::AID-SPE352>3.0.CO;2-1},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/Software - Practice and Experience/Applying design metrics to predict fault-proneness A case study on a large-scale software system.pdf:pdf},
isbn = {0-8186-8991-9},
issn = {00380644},
journal = {Software - Practice and Experience},
keywords = {445 south street,a,and d,correspondence to,d e,d i,design metrics,eric wong,fault-laden modules,fault-prone modules,files,functions,g,mcc-1a348r,morristown,nj 07960,s,structural stress points,telcordia technologies,u,w},
number = {June},
pages = {1587--1608},
title = {{Applying design metrics to predict fault-proneness: A case study on a large-scale software system}},
volume = {30},
year = {2000}
}
@inproceedings{Wu2011,
author = {Wu, Rongxin and Zhang, Hongyu and Kim, Sunghun and Cheung, S C},
booktitle = {Proceedings of the International Symposium on the Foundations of Software Engineering (FSE'11)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the International Symposium on the Foundations of Software Engineering (FSE'11)/ReLink Recovering Links between Bugs and Changes.pdf:pdf},
isbn = {9781450304436},
pages = {15--25},
title = {{ReLink : Recovering Links between Bugs and Changes}},
year = {2011}
}
@article{Yu2002,
abstract = {Software quality is an important external software attribute that
is difficult to measure objectively. In this case study, we empirically
validate a set of object-oriented metrics in terms of their usefulness
in predicting fault-proneness, an important software quality indicator
We use a set of ten software product metrics that relate to the
following software attributes: the size of the software, coupling,
cohesion, inheritance, and reuse. Eight hypotheses on the correlations
of the metrics with fault-proneness are given. These hypotheses are
empirically tested in a case study, in which the client side of a large
network service management system is studied. The subject system is
written in Java and it consists of 123 classes. The validation is
carried out using two data analysis techniques: regression analysis and
discriminant analysis},
author = {Yu, Ping and Systa, T. and Muller, H.},
doi = {10.1109/CSMR.2002.995794},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR/Predicting fault-proneness using OO metrics. An industrial case study.pdf:pdf},
isbn = {0769514383},
issn = {15345351},
journal = {Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
pages = {99--107},
title = {{Predicting fault-proneness using OO metrics. An industrial case study}},
year = {2002}
}
@article{Zhang2014,
author = {Zhang, Hao Helen},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Unknown/Cross Validation.pdf:pdf},
title = {{Cross Validation}},
year = {2014}
}
@article{Zhang2008,
abstract = {We analyze the Eclipse defect data from June 2004 to November 2007, and find that the growth of the number of defects can be well modeled by polynomial functions. Furthermore, we can predict the number of future Eclipse defects based on the nature of defect growth.},
author = {Zhang, Hongyu},
doi = {10.1145/1370750.1370785},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proc. MSR/An initial study of the growth of eclipse defects.pdf:pdf},
isbn = {9781605580241},
issn = {02705257},
journal = {Proc. MSR},
keywords = {defect growth model,defect prediction,polynomial regression},
pages = {141--144},
title = {{An initial study of the growth of eclipse defects}},
url = {http://portal.acm.org/citation.cfm?doid=1370750.1370785},
year = {2008}
}
@inproceedings{Zimmermann2008,
abstract = {In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10\% points higher than for models built from complexity metrics. In addition, network measures could identify 60\% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.},
author = {Zimmermann, T. and Nagappan, N.},
booktitle = {Proceedings of the International Conference on Software Engineering (ICSE'08)},
doi = {10.1145/1368088.1368161},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the International Conference on Software Engineering (ICSE'08)/Predicting defects using network analysis on dependency graphs.pdf:pdf},
isbn = {978-1-60558-079-1},
issn = {0270-5257},
keywords = {defect prediction,dependency graph,network analysis,windows server 2003},
pages = {531--540},
title = {{Predicting defects using network analysis on dependency graphs}},
year = {2008}
}
@inproceedings{Zimmermann2007a,
author = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
booktitle = {Prooceedings of the International Workshop on Predictor Models in Software Engineering (PROMISE'07)},
doi = {10.1109/PROMISE.2007.10},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Prooceedings of the International Workshop on Predictor Models in Software Engineering (PROMISE'07)/Predicting Defects for Eclipse.pdf:pdf},
isbn = {0-7695-2954-2},
pages = {9--20},
title = {{Predicting Defects for Eclipse}},
year = {2007}
}
@article{,
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2001/Unknown/ipred Improved Predictors.pdf:pdf},
pages = {1--11},
title = {{ipred : Improved Predictors}},
year = {2001}
}
@article{Rothman2008,
author = {Rothman, Kj},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/\ldots of Quantitative Risk \ldots/Case–control studies.pdf:pdf},
isbn = {9780470022740},
journal = {\ldots of Quantitative Risk \ldots},
pages = {189--212},
title = {{Case–control studies}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470061596.risk0599/full},
year = {2008}
}
@article{Wacholder1992,
abstract = {A synthesis of classical and recent thinking on the issues involved in selecting controls for case-control studies is presented in this and two companion papers (S. Wacholder et al. Am J Epidemiol 1992;135:1029-50). In this paper, a theoretical framework for selecting controls in case-control studies is developed. Three principles of comparability are described: 1) study base, that all comparisons be made within the study base; 2) deconfounding, that comparisons of the effects of the levels of exposure on disease risk not be distorted by the effects of other factors; and 3) comparable accuracy, that any errors in measurement of exposure be nondifferential between cases and controls. These principles, if adhered to in a study, can reduce selection, confounding, and information bias, respectively. The principles, however, are constrained by an additional efficiency principle regarding resources and time. Most problems and controversies in control selection reflect trade-offs among these four principles.},
author = {Wacholder, S and McLaughlin, J K and Silverman, D T and Mandel, J S},
doi = {10.1002/bdra.20388},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1992/American journal of epidemiology/Selection of controls in case-control studies. I. Principles.pdf:pdf},
isbn = {0002-9262 (Print)$\backslash$r0002-9262 (Linking)},
issn = {1873-2585},
journal = {American journal of epidemiology},
number = {5},
pages = {1019--1028},
pmid = {1595688},
title = {{Selection of controls in case-control studies. I. Principles.}},
volume = {135},
year = {1992}
}
@article{Abreu2009,
abstract = {Communication between developers plays a very central role in team-based software development for a variety of tasks such as coordinating development and maintenance activities, discussing requirements for better comprehension, assessing alternative solutions to complex problems, and like. However, the frequency of communication varies from time to time - sometimes developers exchange more messages with each other than at other times. In this paper, we investigate whether developer communication has any bearing with software quality by examining the relationship between communication frequency and number of bugs injected into the software. The data used for this study is drawn from the bug database, version archive, and mailing lists of the JDT sub-project in ECLIPSE. Our results show a statistically significant positive correlation between communication frequency and number of injected bugs in the software. We also noted that communication levels of key developers in the project do no correlate with number of injected bugs. Moreover, we show that defect prediction models can accommodate social aspects of the development process and potentially deliver more reliable results.},
author = {Abreu, Roberto and Premraj, Rahul},
doi = {10.1145/1595808.1595835},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the joint international and annual ERCIM workshops on Principles of software evolution (IWPSE) and software evolution (Evol)./How developer communication frequency relates to bug introducing changes(2).pdf:pdf},
isbn = {9781605586786},
journal = {Proceedings of the joint international and annual ERCIM workshops on Principles of software evolution (IWPSE) and software evolution (Evol) workshops - IWPSE-Evol '09},
pages = {153 -- 158},
title = {{How developer communication frequency relates to bug introducing changes}},
url = {http://portal.acm.org/citation.cfm?doid=1595808.1595835},
year = {2009}
}
@article{Afzal2008,
abstract = {There have been a number of software reliability growth models (SRGMs) proposed in literature. Due to several reasons, such as violation of models' assumptions and complexity of models, the practitioners face difficulties in knowing which models to apply in practice. This paper presents a comparative evaluation of traditional models and use of genetic programming (GP) for modeling software reliability growth based on weekly fault count data of three different industrial projects. The motivation of using a GP approach is its ability to evolve a model based entirely on prior data without the need of making underlying assumptions. The results show the strengths of using GP for predicting fault count data.},
author = {Afzal, W. and Torkar, R.},
doi = {10.1109/ICSEA.2008.9},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/2008 The Third International Conference on Software Engineering Advances/A Comparative Evaluation of Using Genetic Programming for Predicting Fault Count Data.pdf:pdf},
isbn = {978-1-4244-3218-9},
journal = {2008 The Third International Conference on Software Engineering Advances},
keywords = {Genetic programming,prediction,software reliability growth modeling},
pages = {407--414},
title = {{A Comparative Evaluation of Using Genetic Programming for Predicting Fault Count Data}},
year = {2008}
}
@article{Altman1994,
author = {Altman, D G and Lausen, B and Sauerbrei, W and Schumacher, M},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1994/Journal of the National Cancer Institute/Dangers of using optimal cutpoints in the evaluation of prognostic factors.pdf:pdf},
journal = {Journal of the National Cancer Institute},
keywords = {GCT,bosl,prognostic,prognostic factors},
number = {1994},
pages = {829--835},
title = {{Dangers of using "optimal" cutpoints in the evaluation of prognostic factors}},
volume = {86},
year = {1994}
}
@article{Arcuri2011,
abstract = {Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering.},
author = {Arcuri, a and Briand, L},
doi = {10.1145/1985793.1985795},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Icse'11/A practical guide for using statistical tests to assess randomized algorithms in software engineering.pdf:pdf},
isbn = {9781450304450},
issn = {0270-5257},
journal = {Icse'11},
keywords = {Algorithm design and analysis,Context,Search problems,Software algorithms,Software engineering,Statistical analysis,Testing,bonferroni adjustment,confidence interval,effect size,non-parametric test,parametric test,practical guide,randomized algorithms,snapshot representation,software engineering,statistical analysis,statistical difference,statistical tests,survey,systematic review},
pages = {1--10},
title = {{A practical guide for using statistical tests to assess randomized algorithms in software engineering}},
year = {2011}
}
@article{Austin2014,
abstract = {We conducted an extensive set of empirical analyses to examine the effect of the number of events per variable (EPV) on the relative performance of three different methods for assessing the predictive accuracy of a logistic regression model: apparent performance in the analysis sample, split-sample validation, and optimism correction using bootstrap methods. Using a single dataset of patients hospitalized with heart failure, we compared the estimates of discriminatory performance from these methods to those for a very large independent validation sample arising from the same population. As anticipated, the apparent performance was optimistically biased, with the degree of optimism diminishing as the number of events per variable increased. Differences between the bootstrap-corrected approach and the use of an independent validation sample were minimal once the number of events per variable was at least 20. Split-sample assessment resulted in too pessimistic and highly uncertain estimates of model performance. Apparent performance estimates had lower mean squared error compared to split-sample estimates, but the lowest mean squared error was obtained by bootstrap-corrected optimism estimates. For bias, variance, and mean squared error of the performance estimates, the penalty incurred by using split-sample validation was equivalent to reducing the sample size by a proportion equivalent to the proportion of the sample that was withheld for model validation. In conclusion, split-sample validation is inefficient and apparent performance is too optimistic for internal validation of regression-based prediction models. Modern validation methods, such as bootstrap-based optimism correction, are preferable. While these findings may be unsurprising to many statisticians, the results of the current study reinforce what should be considered good statistical practice in the development and validation of clinical prediction models.},
author = {Austin, Peter C and Steyerberg, Ewout W},
doi = {10.1177/0962280214558972},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Statistical methods in medical research/Events per variable (EPV) and the relative performance of different strategies for estimating the out-of-sample validity of logistic reg.pdf:pdf},
issn = {1477-0334},
journal = {Statistical methods in medical research},
number = {0},
pages = {1--13},
pmid = {25411322},
title = {{Events per variable (EPV) and the relative performance of different strategies for estimating the out-of-sample validity of logistic regression models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25411322},
volume = {0},
year = {2014}
}
@article{Aversano2007,
abstract = {A version control system, such as CVS/SVN, can provide the history of software changes performed during the evolution of a software project. Among all the changes performed there are some which cause the introduction of bugs, often resolved later with other changes. In this paper we use a technique to identify bug-introducing changes to train a model that can be used to predict if a new change may introduces or not a bug. We represent software changes as elements of a n-dimensional vector space of terms coordinates extracted from source code snapshots. The evaluation of various learning algorithms on a set of open source projects looks very promising, in particular for KNN (K-Nearest Neighbor algorithm) where a significant tradeoff between precision and recall has been obtained.},
author = {Aversano, Lerina and Cerulo, Luigi and {Del Grosso}, Concettina},
doi = {10.1145/1294948.1294954},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Ninth international workshop on Principles of software evolution in conjunction with the 6th ESECFSE joint meeting IWPSE 07/Learning from bug-introducing changes to prevent fault prone code.pdf:pdf},
isbn = {9781595937223},
journal = {Ninth international workshop on Principles of software evolution in conjunction with the 6th ESECFSE joint meeting IWPSE 07},
keywords = {bug pre-,mining software repositories,software evolution},
pages = {19},
title = {{Learning from bug-introducing changes to prevent fault prone code}},
url = {http://portal.acm.org/citation.cfm?id=1294948.1294954},
year = {2007}
}
@article{Ceylan2006,
abstract = {Software engineering is a tedious job that includes people, tight deadlines and limited budgets. Delivering what customer wants involves minimizing the defects in the programs. Hence, it is important to establish quality measures early on in the project life cycle. The main objective of this research is to analyze problems in software code and propose a model that will help catching those problems earlier in the project life cycle. Our proposed model uses machine learning methods. Principal component analysis is used for dimensionality reduction, and decision tree, multi layer perceptron and radial basis functions are used for defect prediction. The experiments in this research are carried out with different software metric datasets that are obtained from real-life projects of three big software companies in Turkey. We can say that, the improved method that we proposed brings out satisfactory results in terms of defect prediction},
author = {Ceylan, E. and Kutlubay, F.O. and a.B. Bener},
doi = {10.1109/EUROMICRO.2006.56},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/32nd EUROMICRO Conference on Software Engineering and Advanced Applications (EUROMICRO'06)/Software Defect Identification Using Machine Learning Techniques.pdf:pdf},
isbn = {0-7695-2594-6},
issn = {1089-6503},
journal = {32nd EUROMICRO Conference on Software Engineering and Advanced Applications (EUROMICRO'06)},
title = {{Software Defect Identification Using Machine Learning Techniques}},
year = {2006}
}
@article{Challagulla2005,
abstract = {The wide-variety of real-time software systems, including telecontrol/telepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems perform as specified and not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models; such as stepwise multi-linear regression models and multivariate models, and machine learning approaches, such as artificial neural networks, instance-based reasoning, Bayesian-belief networks, decision trees, and rule inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper; we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of IR and instance-based learning along with the consistency-based subset evaluation technique provides a relatively better consistency in accuracy prediction compared to other models. The results also show that "size" and "complexity" metrics are not sufficient for accurately predicting real-time software defects.},
author = {Challagulla, V.U.B. and Bastani, F.B. and Paul, R.a.},
doi = {10.1109/WORDS.2005.32},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/10th IEEE International Workshop on Object-Oriented Real-Time Dependable Systems/Empirical Assessment of Machine Learning based Software Defect Prediction Techniques.pdf:pdf},
isbn = {0-7695-2347-1},
issn = {1530-1443},
journal = {10th IEEE International Workshop on Object-Oriented Real-Time Dependable Systems},
keywords = {Artificial neural networks,Bayesian methods,Bayesian-belief networks,Machine learning,Network synthesis,Predictive models,Real time systems,Regression tree analysis,Robots,Runtime,Software systems,artificial neural networks,belief networks,consistency-based subset evaluation,decision trees,dynamic code synthesis,dynamic dependability assessment,instance-based reasoning,learning (artificial intelligence),machine learning,mission planning systems,multivariate models,predictor model,real-time software systems,regression analysis,robotic systems,rule inductions,runtime mission-specific requirements,safety-critical software,software defect prediction,software performance evaluation,software quality,statistical models,stepwise multilinear regression models,telecontrol systems,telepresence systems},
pages = {263--270},
title = {{Empirical Assessment of Machine Learning based Software Defect Prediction Techniques}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544801},
year = {2005}
}
@article{Chernick1985,
author = {Chernick, M.R and Murthy, V.K and Nealy, C.D},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1985/Pattern Recognition Letters/Application of bootstrap and other resampling techniques Evaluation of classifier performance.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
month = may,
number = {3},
pages = {167--178},
title = {{Application of bootstrap and other resampling techniques: Evaluation of classifier performance}},
volume = {3},
year = {1985}
}
@article{Consonni2010,
author = {Consonni, Viviana and Ballabio, Davide and Todeschini, Roberto},
doi = {10.1002/cem.1290},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Journal of Chemometrics/Evaluation of model predictive ability by external validation techniques.pdf:pdf},
issn = {08869383},
journal = {Journal of Chemometrics},
keywords = {external test set,validation},
month = feb,
number = {3-4},
pages = {194--201},
title = {{Evaluation of model predictive ability by external validation techniques}},
url = {http://doi.wiley.com/10.1002/cem.1290},
volume = {24},
year = {2010}
}
@article{Cox1958,
author = {Cox, David R},
doi = {10.1093/biomet/45.3-4.562},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1958/Biometrika/Two Further Applications of a Model for Binary Regression.pdf:pdf},
isbn = {00063444},
issn = {0006-3444},
journal = {Biometrika},
number = {3},
pages = {562--565},
title = {{Two Further Applications of a Model for Binary Regression}},
volume = {45},
year = {1958}
}
@inproceedings{Ambros2010,
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
booktitle = {Prooceedings of the Working Conference on Mining Software Repositories (MSR'10)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Prooceedings of the Working Conference on Mining Software Repositories (MSR'10)/An Extensive Comparison of Bug Prediction Approaches.pdf:pdf},
pages = {31--41},
title = {{An Extensive Comparison of Bug Prediction Approaches}},
year = {2010}
}
@article{Eaddy2008,
author = {Eaddy, Marc and Member, Student and Zimmermann, Thomas and Sherwood, Kaitlin D and Garg, Vibhav and Murphy, Gail C and Society, Ieee Computer and Nagappan, Nachiappan and Aho, Alfred V},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/IEEE Transactions on Software Engineering/Do Crosscutting Concerns Cause Defects.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
number = {4},
pages = {497--515},
title = {{Do Crosscutting Concerns Cause Defects ?}},
volume = {34},
year = {2008}
}
@article{ElEmam2001,
abstract = {Contemporary evidence suggests that most field faults in software$\backslash$napplications are found in a small percentage of the software's components.$\backslash$nThis means that if these faulty software components can be detected$\backslash$nearly in the development project's life cycle, mitigating actions$\backslash$ncan be taken, such as a redesign. For object-oriented applications,$\backslash$nprediction models using design metrics can be used to identify faulty$\backslash$nclasses early on. In this paper we report on a study that used object-oriented$\backslash$ndesign metrics to construct such prediction models. The study used$\backslash$ndata collected from one version of a commercial Java application$\backslash$nfor constructing a Prediction model. The model was then validated$\backslash$non a subsequent release of the same application. Our results indicate$\backslash$nthat the prediction model has a high accuracy. Furthermore, we found$\backslash$nthat an export coupling (EC) metric had the strongest association$\backslash$nwith fault-proneness, indicating a structural feature that may be$\backslash$nsymptomatic of a class with a high probability of latent faults.$\backslash$n(C) 2001 Elsevier Science Inc. All rights reserved.},
author = {{El Emam}, Khaled and Melo, Walcelio and Machado, Javam C.},
doi = {10.1016/S0164-1212(00)00086-8},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2001/Journal of Systems and Software/The prediction of faulty classes using object-oriented design metrics.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {java metrics,java quality,object-oriented metrics,software metrics,software quality},
pages = {63--75},
title = {{The prediction of faulty classes using object-oriented design metrics}},
volume = {56},
year = {2001}
}
@article{Elish2008,
abstract = {Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Elish, Karim O. and Elish, Mahmoud O.},
doi = {10.1016/j.jss.2007.07.040},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Journal of Systems and Software/Predicting defect-prone software modules using support vector machines.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Defect-prone modules,Predictive models,Software metrics,Support vector machines},
pages = {649--660},
title = {{Predicting defect-prone software modules using support vector machines}},
volume = {81},
year = {2008}
}
@article{Eyolfson2014,
author = {Eyolfson, Jon and Tan, Lin and Lam, Patrick},
doi = {10.1007/s10664-013-9245-0},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Unknown/Correlations between bugginess and time-based commit characteristics.pdf:pdf},
pages = {1009--1039},
title = {{Correlations between bugginess and time-based commit characteristics}},
year = {2014}
}
@inproceedings{Fioravanti2001,
author = {Fioravanti, F and Nesi, P},
booktitle = {Proceedings of the European Conference on Software Maintenance and Reengineering},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2001/Proceedings of the European Conference on Software Maintenance and Reengineering/A Study on Fault-Proneness Detection of Object-Oriented Systems.pdf:pdf},
pages = {121--130},
title = {{A Study on Fault-Proneness Detection of Object-Oriented Systems}},
year = {2001}
}
@article{Gayatri2009,
abstract = {Data mining techniques are applied in building software fault prediction models for improving the software quality. Early identification of high-risk modules can assist in quality enhancement efforts to modules that are likely to have a high number of faults. Classification tree models are simple and effective as software quality prediction models, and timely predictions of defects from such models can be used to achieve high software reliability. In this paper, the performance of five data mining classifier algorithms named J48, CART, Random Forest, BFTree and Naive Bayesian classifier (NBC) are evaluated based on 10 fold cross validation test. Experimental results using KC2 NASA software metrics dataset demonstrates that decision trees are much useful for fault predictions and based on rules generated only some measurement attributes in the given set of the metrics play an important role in establishing final rules and for improving the software quality by giving correct predictions. Thus we can suggest that these attributes are sufficient for future classification process. To evaluate the performance of the above algorithms Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Receiver Operating Characteristic (ROC) and Accuracy measures are applied.},
author = {Gayatri, N. and Nickolas, S. and a.V. Reddy and Chitra, R.},
doi = {10.1109/ARTCom.2009.12},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 International Conference on Advances in Recent Technologies in Communication and Computing/Performance Analysis of Datamining Algorithms for Software Quality Prediction.pdf:pdf},
isbn = {978-1-4244-5104-3},
journal = {2009 International Conference on Advances in Recent Technologies in Communication and Computing},
keywords = {BFTree,CART,Classification,Cross\&\#150;Validation,J48,Naive Bayesian,Random forest,Software Quality},
title = {{Performance Analysis of Datamining Algorithms for Software Quality Prediction}},
year = {2009}
}
@article{Graves2000,
author = {Graves, Todd L and Karr, Alan F and Marron, J S and Siy, Harvey},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2000/IEEE Transactions on Software Engineering/Predicting fault incidence using software change history.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
number = {7},
pages = {653--661},
title = {{Predicting fault incidence using software change history}},
volume = {26},
year = {2000}
}
@article{Hanczar2013,
author = {Hanczar, Blaise and Dougherty, Edward R.},
doi = {10.1016/j.patcog.2012.09.019},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Pattern Recognition/The reliability of estimated confidence intervals for classification error rates when only a single sample is available.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Confidence interval,Error estimation,High dimension,Small sample setting,Supervised learning},
month = mar,
number = {3},
pages = {1067--1077},
publisher = {Elsevier},
title = {{The reliability of estimated confidence intervals for classification error rates when only a single sample is available}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320312004256},
volume = {46},
year = {2013}
}
@article{Hanczar2010,
abstract = {MOTIVATION: The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics?

RESULTS: Through a simulation study using data models and analysis of real microarray data, we show that (i) for small samples the root mean square differences of the estimated and true metrics are considerable; (ii) even for large samples, there is only weak correlation between the true and estimated metrics; and (iii) generally, there is weak regression of the true metric on the estimated metric. For classification rules, we consider linear discriminant analysis, linear support vector machine (SVM) and radial basis function SVM. For error estimation, we consider resubstitution, three kinds of cross-validation and bootstrap. Using resampling, we show the unreliability of some published ROC results.

AVAILABILITY: Companion web site at http://compbio.tgen.org/paper\_supp/ROC/roc.html

CONTACT: edward@mail.ece.tamu.edu.},
author = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R},
doi = {10.1093/bioinformatics/btq037},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Bioinformatics (Oxford, England)/Small-sample precision of ROC-related estimates.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,False Positive Reactions,Oligonucleotide Array Sequence Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,ROC Curve},
month = mar,
number = {6},
pages = {822--30},
pmid = {20130029},
title = {{Small-sample precision of ROC-related estimates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20130029},
volume = {26},
year = {2010}
}
@inproceedings{Hassan2009,
author = {Hassan, Ahmed E.},
booktitle = {Prooceedings of the 31st International Conference on Software Engineering (ICSE'09)},
doi = {10.1109/ICSE.2009.5070510},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Prooceedings of the 31st International Conference on Software Engineering (ICSE'09)/Predicting faults using the complexity of code changes.pdf:pdf},
isbn = {978-1-4244-3453-4},
pages = {78--88},
title = {{Predicting faults using the complexity of code changes}},
year = {2009}
}
@inproceedings{Herzig2013a,
author = {Herzig, Kim and Zeller, Andreas},
booktitle = {Prooceedings of the 10th Working Conference on Mining Software Repositories (MSR'13)},
doi = {10.1109/MSR.2013.6624018},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Prooceedings of the 10th Working Conference on Mining Software Repositories (MSR'13)/The impact of tangled code changes.pdf:pdf},
isbn = {978-1-4673-2936-1},
month = may,
pages = {121--130},
title = {{The impact of tangled code changes}},
year = {2013}
}
@article{Hong2008,
abstract = {In software project management, there are three major factors to predict and control; size, effort, and quality. Much software engineering work has focused on these. When it comes to software quality, there are various possible quality characteristics of software, but in practice, quality management frequently revolves around defects, and delivered defect density has become the current de facto industry standard. Thus, research related to software quality has been focused on modeling residual defects in software in order to estimate software reliability. Currently, software engineering literature still does not have a complete defect prediction for a software product although much work has been performed to predict software quality. On the other side, the number of defects alone cannot be sufficient information to provide the basis for planning quality assurance activities and assessing them during execution. That is, for project management to be improved, we need to predict other possible information about software quality such as in-process defects, their types, and so on. In this paper, we propose a new approach for predicting the distribution of defects and their types based on project characteristics in the early phase. For this approach, the model for prediction was established using the curve-fitting method and regression analysis. The maximum likelihood estimation (MLE) was used in fitting the Weibull probability density function to the actual defect data, and regression analysis was used in identifying the relationship between the project characteristics and the Weibull parameters. The research model was validated by cross-validation.},
author = {Hong, Youngki and Baik, Jongmoon and Ko, In Young and Choi, Ho Jin},
doi = {10.1109/ICIS.2008.36},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings - 7th IEEEACIS International Conference on Computer and Information Science, IEEEACI. Workshop on e-Activity, IEEEACIS IWEA 2008/A value-added predictive defect type distribution model based on project characteristics.pdf:pdf},
isbn = {9780769531311},
journal = {Proceedings - 7th IEEE/ACIS International Conference on Computer and Information Science, IEEE/ACIS ICIS 2008, In conjunction with 2nd IEEE/ACIS Int. Workshop on e-Activity, IEEE/ACIS IWEA 2008},
keywords = {Defect type distribution,In-process defect prediction,Maximum likelihood estimation,Software reliability,Weibull function},
pages = {469--474},
title = {{A value-added predictive defect type distribution model based on project characteristics}},
year = {2008}
}
@inproceedings{Hovemeyer2004,
annote = {From Duplicate 1 ( },
author = {Hovemeyer, David and Pugh, William},
booktitle = {Proceedings of the 19th annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications (OOPSLA'04)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Proceedings of the 19th annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications (OOPSLA'04)/Finding Bugs is Easy.pdf:pdf},
isbn = {1581138334},
keywords = {bug checkers,bug patterns,bugs,static analysis},
pages = {132--135},
title = {{Finding Bugs is Easy}},
year = {2004}
}
@article{Jiang2008,
abstract = {The prediction of fault-prone modules continues to attract interest due to the significant impact it has on software quality assurance. One of the most important goals of such techniques is to accurately predict the modules where faults are likely to hide as early as possible in the development lifecycle. Design, code, and most recently, requirements metrics have been successfully used for predicting fault-prone modules. The goal of this paper is to compare the performance of predictive models which use design-level metrics with those that use code-level metrics and those that use both. We analyze thirteen datasets from NASA Metrics Data Program which offer design as well as code metrics. Using a range of modeling techniques and statistical significance tests, we confirmed that models built from code metrics typically outperform design metrics based models. However, both types of models prove to be useful as they can be constructed in different project phases. Code-based models can be used to increase the performance of design-level models and, thus, increase the efficiency of assigning verification and validation activities late in the development lifecycle. We also conclude that models that utilize a combination of design and code level metrics outperform models which use either one or the other metric set. Copyright 2008 ACM.},
author = {Jiang, Yue and Cukic, Bojan and Menzies, Tim and Bartlow, Nick},
doi = {10.1145/1370788.1370793},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 4th international workshop on Predictor models in software engineering PROMISE 08/Comparing design and code metrics for software quality prediction.pdf:pdf},
isbn = {9781605580364},
journal = {Proceedings of the 4th international workshop on Predictor models in software engineering PROMISE 08},
keywords = {code metrics,design metrics,fault-proneness prediction,machine},
pages = {11--18},
title = {{Comparing design and code metrics for software quality prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1370788.1370793},
volume = {12},
year = {2008}
}
@inproceedings{Jones2005,
author = {Jones, James A and Harrold, Mary Jean},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering (ASE'05)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Proceedings of the 20th IEEEACM International Conference on Automated Software Engineering (ASE'05)/Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique Categories and Subject Descriptors.pdf:pdf},
isbn = {1581139934},
keywords = {automated debugging,fault localization,program analysis},
pages = {273--282},
title = {{Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique Categories and Subject Descriptors}},
year = {2005}
}
@article{Joshi2007,
abstract = {Finding and fixing software bugs is a challenging maintenance task, and a significant amount of effort is invested by software development companies on this issue. In this paper, we use the Eclipse project's recorded software bug history to predict occurrence of future bugs. The history contains information on when bugs have been reported and subsequently fixed.},
author = {Joshi, Hemant and Zhang, Chuanlei and Ramaswamy, S. and Bayrak, Coskun},
doi = {10.1109/MSR.2007.17},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings - ICSE 2007 Workshops Fourth International Workshop on Mining Software Repositories, MSR 2007/Local and global recency weighting approach to bug prediction.pdf:pdf},
isbn = {076952950X},
journal = {Proceedings - ICSE 2007 Workshops: Fourth International Workshop on Mining Software Repositories, MSR 2007},
number = {3},
pages = {3--4},
title = {{Local and global recency weighting approach to bug prediction}},
year = {2007}
}
@inproceedings{Kamei2010,
author = {Kamei, Yasutaka and Matsumoto, Shinsuke and Monden, Akito and Matsumoto, Ken-ichi and Adams, Bram and Hassan, Ahmed E.},
booktitle = {Proceedings of the 2010 IEEE International Conference on Software Maintenance (ICSM'10)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the 2010 IEEE International Conference on Software Maintenance (ICSM'10)/Revisiting common bug prediction findings using effort-aware models.pdf:pdf},
isbn = {978-1-4244-8630-4},
month = sep,
pages = {1--10},
title = {{Revisiting common bug prediction findings using effort-aware models}},
year = {2010}
}
@article{Kamei2007,
author = {Kamei, Yasutaka and Monden, Akito and Matsumoto, Shinsuke and Kakimoto, Takeshi and Matsumoto, Ken-ichi},
doi = {10.1109/ESEM.2007.28},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)/The Effects of Over and Under Sampling on Fault-prone Module Detection.pdf:pdf},
isbn = {978-0-7695-2886-1},
journal = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
month = sep,
pages = {196--204},
publisher = {Ieee},
title = {{The Effects of Over and Under Sampling on Fault-prone Module Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4343747},
year = {2007}
}
@article{Kamei2008,
author = {Kamei, Yasutaka and Monden, Akito and Morisaki, Shuji and Matsumoto, Ken-ichi},
doi = {10.1145/1414004.1414051},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '08/A hybrid faulty module prediction using association rule mining and logistic regression analysis.pdf:pdf},
isbn = {9781595939715},
journal = {Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '08},
keywords = {association rule,empirical study,fault-prone module prediction,logistic regression analysis,mining},
pages = {279},
title = {{A hybrid faulty module prediction using association rule mining and logistic regression analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1414004.1414051},
year = {2008}
}
@article{Kaminsky2004,
abstract = { There has been extensive research in the area of data mining over the last decade, but relatively little research in algorithmic mining. Some researchers shun the idea of incorporating explicit knowledge with a Genetic Program environment. At best, very domain specific knowledge is hard wired into the GP modeling process. This work proposes a new approach called the Genetically Engineerable Evolvable Program (GEEP). In this approach, explicit knowledge is made available to the GP. It is considered breadth-based, in that all pieces of knowledge are independent of each other. Several experiments are performed on a NASA-based data set using established equations from other researchers in order to predict software defects. All results are statistically validated.},
author = {Kaminsky, K. and Boetticher, G.},
doi = {10.1109/NAFIPS.2004.1336240},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/IEEE Annual Meeting of the Fuzzy Information, 2004. Processing NAFIPS '04/Building a genetically engineerable evolvable program (GEEP) using breadth-based explicit knowledge for predicting software defects.pdf:pdf},
isbn = {0-7803-8376-1},
journal = {IEEE Annual Meeting of the Fuzzy Information, 2004. Processing NAFIPS '04.},
pages = {10--15},
title = {{Building a genetically engineerable evolvable program (GEEP) using breadth-based explicit knowledge for predicting software defects}},
volume = {1},
year = {2004}
}
@article{Kastro2008,
abstract = {New methodologies and tools have gradually made the life cycle for software development more human-independent. Much of the research in this field focuses on defect reduction, defect identification and defect prediction. Defect prediction is a relatively new research area that involves using various methods from artificial intelligence to data mining. Identifying and locating defects in software projects is a difficult task. Measuring software in a continuous and disciplined manner provides many advantages such as the accurate estimation of project costs and schedules as well as improving product and process qualities. This study aims to propose a model to predict the number of defects in the new version of a software product with respect to the previous stable version. The new version may contain changes related to a new feature or a modification in the algorithm or bug fixes. Our proposed model aims to predict the new defects introduced into the new version by analyzing the types of changes in an objective and formal manner as well as considering the lines of code (\{LOC)\} change. Defect predictors are helpful tools for both project managers and developers. Accurate predictors may help reducing test times and guide developers towards implementing higher quality codes. Our proposed model can aid software engineers in determining the stability of software before it goes on production. Furthermore, such a model may provide useful insight for understanding the effects of a feature, bug fix or change in the process of defect detection.},
author = {Kastro, Yomi and Bener, Ayşe Basar},
doi = {10.1007/s11219-008-9053-8},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Software Quality Journal/A defect prediction method for software versioning.pdf:pdf},
issn = {09639314},
journal = {Software Quality Journal},
keywords = {Defect prediction,Neural networks,Software defects},
pages = {543--562},
title = {{A defect prediction method for software versioning}},
volume = {16},
year = {2008}
}
@article{Khan2010,
author = {Khan, Jafar a. and {Van Aelst}, Stefan and Zamar, Ruben H.},
doi = {10.1016/j.csda.2010.01.031},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Computational Statistics \& Data Analysis/Fast robust estimation of prediction error based on resampling.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
month = dec,
number = {12},
pages = {3121--3130},
publisher = {Elsevier B.V.},
title = {{Fast robust estimation of prediction error based on resampling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947310000460},
volume = {54},
year = {2010}
}
@article{Khomh,
author = {Khomh, Foutse and Dhaliwal, Tejinder and Zou, Ying and Adams, Bram and Engineering, Comp},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Do Faster Releases Improve Software Quality — An Empirical Case Study of Mozilla Firefox —.pdf:pdf},
keywords = {-software release,release cycle,software quality},
title = {{Do Faster Releases Improve Software Quality ? — An Empirical Case Study of Mozilla Firefox —}}
}
@inproceedings{Khoshgoftaar2002b,
author = {Khoshgoftaar, Taghi M},
booktitle = {Proceedings of the International Symposium on Software Reliability Engineering (ISSRE'02)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Proceedings of the International Symposium on Software Reliability Engineering (ISSRE'02)/Improving Usefulness of Software Quality Classification Models Based on Boolean Discriminant Functions.pdf:pdf},
title = {{Improving Usefulness of Software Quality Classification Models Based on Boolean Discriminant Functions}},
year = {2002}
}
@article{Khoshgoftaar2002d,
author = {Khoshgoftaar, Taghi M.},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Unknown/Software Quality Classification Modeling Using The SPRINT Decision Tree Algorithm Taghi.pdf:pdf},
isbn = {0769518494},
keywords = {classification,ibm intelligent data miner,minimum description length,principle,software metrics,software quality estimation,trees},
pages = {365--374},
title = {{Software Quality Classification Modeling Using The SPRINT Decision Tree Algorithm Taghi}},
year = {2002}
}
@article{Khoshgoftaar2005,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem and Gao, Kehan},
doi = {10.1007/s10664-004-6191-x},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Empirical Software Engineering/Assessment of a new three-group software quality classification technique An empirical case study.pdf:pdf},
isbn = {1382-3256},
issn = {13823256},
journal = {Empirical Software Engineering},
keywords = {Case-based reasoning,Decision trees,Discriminant analysis,Expected cost of misclassification,Software quality prediction,Three-group classification},
pages = {183--218},
title = {{Assessment of a new three-group software quality classification technique: An empirical case study}},
volume = {10},
year = {2005}
}
@article{Khoshgoftaar2002c,
author = {Khoshgoftaar, Taghi M. and Yuan, Xiaojing and Allen, Edward B. and Jones, Wendell D. and Hudepohl, John P.},
doi = {10.1023/A:1020511004267},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2002/Empirical Software Engineering/Uncertain classification of fault-prone software modules.pdf:pdf},
issn = {13823256},
journal = {Empirical Software Engineering},
keywords = {CHAID,Classification trees,Fault-prone modules,Software metrics,Software quality,TREEDISC,Telecommunications},
pages = {297--318},
title = {{Uncertain classification of fault-prone software modules}},
volume = {7},
year = {2002}
}
@article{Kim,
author = {Kim, Ji-hyun},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Computational Statistics \& Data Analysis/Estimating Classification Error Rate Repeated Cross-validation , Repeated Hold-out and Bootstrap.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
keywords = {boost-,bootstrap,prediction error,pruned tree,repeated cross-validation},
number = {11},
pages = {3735 -- 3745},
title = {{Estimating Classification Error Rate : Repeated Cross-validation , Repeated Hold-out and Bootstrap}},
volume = {53},
year = {2009}
}
@article{Kim2008,
author = {Kim, Sunghun and Jr, E James Whitehead and Zhang, Yi},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Unknown/Classifying Software Changes Clean or Buggy.pdf:pdf},
number = {2},
pages = {181--196},
title = {{Classifying Software Changes : Clean or Buggy ?}},
volume = {34},
year = {2008}
}
@article{Kim2006a,
abstract = {The change history of a software project contains a rich collection of code changes that record previous development experience. Changes that fix bugs are especially interesting, since they record both the old buggy code and the new fixed code. This paper presents a bug finding algorithm using bug fix memories: a project-specific bug and fix knowledge base developed by analyzing the history of bug fixes. A bug finding tool, BugMem, implements the algorithm. The approach is different from bug finding tools based on theorem proving or static model checking such as Bandera, ESC/Java, FindBugs, JLint, and PMD. Since these tools use pre-defined common bug patterns to find bugs, they do not aim to identify project-specific bugs. Bug fix memories use a learning process, so the bug patterns are project-specific, and project-specific bugs can be detected. The algorithm and tool are assessed by evaluating if real bugs and fixes in project histories can be found in the bug fix memories. Analysis of five open source projects shows that, for these projects, 19.3\%-40.3\% of bugs appear repeatedly in the memories, and 7.9\%-15.5\% of bug and fix pairs are found in memories. The results demonstrate that project-specific bug fix patterns occur frequently enough to be useful as a bug detection technique. Furthermore, for the bug and fix pairs, it is possible to both detect the bug and provide a strong suggestion for the fix. However, there is also a high false positive rate, with 20.8\%-32.5\% of non-bug containing changes also having patterns found in the memories. A comparison of BugMem with a bug finding tool, PMD, shows that the bug sets identified by both tools are mostly exclusive, indicating that BugMem complements other bug finding tools.},
author = {Kim, Sunghun and Pan, Kai and {Whitehead Jr.}, E E James},
doi = {10.1145/1181775.1181781},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the 14th ACM SIGSOFT international symposium on Foundations of software engineering/Memories of bug fixes.pdf:pdf},
isbn = {1-59593-468-5},
journal = {Proceedings of the 14th ACM SIGSOFT international symposium on Foundations of software engineering},
keywords = {bug,bug finding tool,fault,fix,patterns,prediction},
pages = {35--45},
title = {{Memories of bug fixes}},
url = {http://doi.acm.org/10.1145/1181775.1181781},
year = {2006}
}
@article{Knoke,
author = {Knoke, James D and A, Chapel Hill N C U S},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/The Robust Estimation of Classificatino Error Rates.pdf:pdf},
number = {2},
pages = {253--260},
title = {{The Robust Estimation of Classificatino Error Rates}},
volume = {12}
}
@article{Kocaguneli2013a,
author = {Kocaguneli, Ekrem and Cukic, Bojan and Lu, Huihua},
doi = {10.1109/RAISE.2013.6615203},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/2013 2nd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE)/Predicting more from less Synergies of learning.pdf:pdf},
isbn = {978-1-4673-6437-9},
journal = {2013 2nd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE)},
month = may,
pages = {42--48},
publisher = {Ieee},
title = {{Predicting more from less: Synergies of learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6615203},
year = {2013}
}
@article{Kocaguneli2012,
author = {Kocaguneli, Ekrem and Member, Student and Menzies, Tim},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Unknown/Exploiting the Essential Assumptions of Analogy-Based Effort Estimation.pdf:pdf},
number = {2},
pages = {425--438},
title = {{Exploiting the Essential Assumptions of Analogy-Based Effort Estimation}},
volume = {38},
year = {2012}
}
@article{Kocaguneli2013,
author = {Kocaguneli, Ekrem and Menzies, Tim},
doi = {10.1016/j.jss.2013.02.053},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Journal of Systems and Software/Software effort models should be assessed via leave-one-out validation.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {software cost estimation},
number = {7},
pages = {1879--1890},
publisher = {Elsevier Inc.},
title = {{Software effort models should be assessed via leave-one-out validation}},
volume = {86},
year = {2013}
}
@article{Kocaguneli2014,
author = {Kocaguneli, Ekrem and Menzies, Tim and Mendes, Emilia},
doi = {10.1007/s10664-014-9300-5},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Empirical Software Engineering/Transfer learning in effort estimation.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {communicated by,data mining,e,effort estimation,k -nn,kocaguneli,lane department of computer,martin shepperd,menzies,science and electrical engineering,t,transfer learning,west virginia university},
month = mar,
title = {{Transfer learning in effort estimation}},
url = {http://link.springer.com/10.1007/s10664-014-9300-5},
year = {2014}
}
@inproceedings{Kohavi1995,
author = {Kohavi, Ron},
booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI'95)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1995/Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI'95)/A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.pdf:pdf},
pages = {1137--1143},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
year = {1995}
}
@article{Kuhn2008,
author = {Kuhn, Max},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Journal of Statistical Software/Building Predictive Models in R Using caret Package.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {model building,networkspaces,parallel processing,r,tuning parameters},
number = {5},
title = {{Building Predictive Models in R Using caret Package}},
volume = {28},
year = {2008}
}
@article{Kutlubay2007,
abstract = {Identifying and locating defects in software projects is a difficult task. Further, estimating the density of defects is more difficult. Measuring software in a continuous and disciplined manner brings many advantages such as accurate estimation of project costs and schedules, and improving product and process qualities. Detailed analysis of software metric data gives significant clues about the locations and magnitude of possible defects in a program. The aim of this research is to establish an improved method for predicting software quality via identifying the defect density of fault prone modules using machine-learning techniques. We constructed a two-step model that predicts defect density by taking module metric data into consideration. Our proposed model utilizes classification and regression type learning methods consecutively. The results of the experiments on public data sets show that the two-step model enhances the overall performance measures as compared to applying only regression methods.},
author = {Kutlubay, Onur and Turhan, Burak and Bener, Ayşe B.},
doi = {10.1109/EUROMICRO.2007.13},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/EUROMICRO 2007 - Proceedings of the 33rd EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2007/A two-step model for defect density estimation.pdf:pdf},
isbn = {0769529771},
issn = {1089-6503},
journal = {EUROMICRO 2007 - Proceedings of the 33rd EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2007},
number = {Seaa},
pages = {322--329},
title = {{A two-step model for defect density estimation}},
year = {2007}
}
@article{Liu2009,
author = {Liu, Jun and Xu, Zheng and Qiao, Jianzhong and Lin, Shukuan},
doi = {10.1109/CCDC.2009.5191800},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 Chinese Control and Decision Conference/A defect prediction model for software based on service oriented architecture using EXPERT COCOMO.pdf:pdf},
isbn = {978-1-4244-2722-2},
journal = {2009 Chinese Control and Decision Conference},
keywords = {defect prediction,measurement,process improvement,service oriented architecture},
pages = {2591--2594},
title = {{A defect prediction model for software based on service oriented architecture using EXPERT COCOMO}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5191800},
year = {2009}
}
@article{Liu2010,
abstract = {A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.},
author = {Liu, Yi and Khoshgoftaar, Taghi M. and Seliya, Naeem},
doi = {10.1109/TSE.2010.51},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/IEEE Transactions on Software Engineering/Evolutionary optimization of software quality modeling with multiple repositories.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Genetic programming,defects,machine learning,optimization,software measurement,software quality},
number = {6},
pages = {852--864},
title = {{Evolutionary optimization of software quality modeling with multiple repositories}},
volume = {36},
year = {2010}
}
@article{Mahaweerawat2004,
author = {Mahaweerawat, a},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Advanced Virtual and \ldots/Fault prediction in object-oriented software using neural network techniques.pdf:pdf},
issn = {09505849},
journal = {Advanced Virtual and \ldots},
keywords = {back-propagation learning,neural network,predictive model,radial-basis function,software fault},
pages = {483--492},
title = {{Fault prediction in object-oriented software using neural network techniques}},
volume = {49},
year = {2004}
}
@inproceedings{Mcintosh2014,
author = {Mcintosh, Shane and Kamei, Yasutaka and Adams, Bram and Hassan, Ahmed E},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories (MSR'14)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Proceedings of the 11th Working Conference on Mining Software Repositories (MSR'14)/The Impact of Code Review Coverage and Code Review Participation on Software Quality.pdf:pdf},
isbn = {9781450328630},
keywords = {all or part of,code reviews,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,software quality,this work for},
pages = {192----201},
title = {{The Impact of Code Review Coverage and Code Review Participation on Software Quality}},
year = {2014}
}
@article{Mcintosh,
author = {Mcintosh, Shane and Kamei, Yasutaka and Hassan, Adams Ahmed E},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/An Empirical Study of the Impact of Modern Code Review Practices on Software Quality.pdf:pdf},
keywords = {ahmed e,canada,code review,hassan,lab,queen,s university,sail,shane mcintosh,software analysis and intelligence,software quality},
title = {{An Empirical Study of the Impact of Modern Code Review Practices on Software Quality}}
}
@article{Mende2a010,
author = {Mende, T and Koschke, R},
doi = {10.1109/CSMR.2010.18},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the 14th European Conference on Software Maintenance and Reengineering (CSMR'10)/Effort-Aware Defect Prediction Models.pdf:pdf},
isbn = {978-1-61284-369-8},
journal = {Proceedings of the 14th European Conference on Software Maintenance and Reengineering (CSMR'10)},
keywords = {-defect},
month = mar,
pages = {107--116},
publisher = {Ieee},
title = {{Effort-Aware Defect Prediction Models}},
year = {2010}
}
@inproceedings{Mende2010a,
abstract = {Background: The main goal of the PROMISE repository is to enable reproducible, and thus verifiable or refutable research. Over time, plenty of data sets became available, especially for defect prediction problems. Aims: In this study, we investigate possible problems and pitfalls that occur during replication. This information can be used for future replication studies, and serve as a guideline for researchers reporting novel results. Method: We replicate two recent defect prediction studies comparing different data sets and learning algorithms, and report missing information and problems. Results: Even with access to the original data sets, replicating previous studies may not lead to the exact same results. The choice of evaluation procedures, performance measures and presentation has a large influence on the reproducibility. Additionally, we show that trivial and random models can be used to identify overly optimistic evaluation measures. Conclusions: The best way to conduct easily reproducible studies is to share all associated artifacts, e.g. scripts and programs used. When this is not an option, our results can be used to simplify the replication task for other researchers.},
author = {Mende, Thilo},
booktitle = {Proceedings of the International Conference on Predictive Models in Software Engineering (PROMISE'10)},
doi = {10.1145/1868328.1868336},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Proceedings of the International Conference on Predictive Models in Software Engineering (PROMISE'10)/Replication of Defect Prediction Studies Problems, Pitfalls and Recommendations.pdf:pdf},
isbn = {9781450304047},
issn = {15393755},
keywords = {defect prediction model,replication},
pages = {1--10},
title = {{Replication of Defect Prediction Studies: Problems, Pitfalls and Recommendations}},
url = {http://dl.acm.org/citation.cfm?id=1868336},
year = {2010}
}
@article{Mende2009a,
abstract = {Defect Prediction Models aim at identifying error-prone parts of a software system as early as possible. Many such models have been proposed, their evaluation, however, is still an open question, as recent publications show. An important aspect often ignored during evaluation is the effort reduction gained by using such models. Models are usually evaluated per module by performance measures used in information retrieval, such as recall, precision, or the area under the ROC curve (AUC). These measures assume that the costs associated with additional quality assurance activities are the same for each module, which is not reasonable in practice. For example, costs for unit testing and code reviews are roughly proportional to the size of a module. In this paper, we investigate this discrepancy using optimal and trivial models. We describe a trivial model that takes only the module size measured in lines of code into account, and compare it to five classification methods. The trivial model performs surprisingly well when evaluated using AUC. However, when an effort-sensitive performance measure is used, it becomes apparent that the trivial model is in fact the worst. ACM 2009.},
author = {Mende, Thilo and Koschke, Rainer},
doi = {10.1145/1540438.1540448},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings of the 5th International Conference on Predictor Models in Software Engineering - PROMISE '09/Revisiting the evaluation of defect prediction models(2).pdf:pdf},
isbn = {9781605586342},
journal = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering - PROMISE '09},
keywords = {cost-sensitive performance measures,defect prediction},
pages = {1},
title = {{Revisiting the evaluation of defect prediction models}},
url = {http://portal.acm.org/citation.cfm?doid=1540438.1540448},
year = {2009}
}
@article{Meneely2011,
abstract = {With each new developer to a software development team comes a greater challenge to manage the communication, coordination, and knowledge transfer amongst teammates. Fred Brooks discusses this challenge in The Mythical Man-Month by arguing that rapid team expansion can lead to a complex team organization structure. While Brooks focuses on productivity loss as the negative outcome, poor product quality is also a substantial concern. But if team expansion is unavoidable, can any quality impacts be mitigated? Our objective is to guide software engineering managers by empirically analyzing the effects of team size, expansion, and structure on product quality. We performed an empirical, longitudinal case study of a large Cisco networking product over a five year history. Over that time, the team underwent periods of no expansion, steady expansion, and accelerated expansion. Using team-level metrics, we quantified characteristics of team expansion, including team size, expansion rate, expansion acceleration, and modularity with respect to department designations. We examined statistical correlations between our monthly team-level metrics and monthly product-level metrics. Our results indicate that increased team size and linear growth are correlated with later periods of better product quality. However, periods of accelerated team expansion are correlated with later periods of reduced software quality. Furthermore, our linear regression prediction model based on team metrics was able to predict the product's post-release failure rate within a 95\% prediction interval for 38 out of 40 months. Our analysis provides insight for project managers into how the expansion of development teams can impact product quality.},
author = {Meneely, Andrew and Rotella, Pete and Williams, Laurie},
doi = {10.1145/2025113.2025128},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering/Does Adding Manpower Also Affect Quality An Empirical, Longitudinal Analysis.pdf:pdf},
isbn = {978-1-4503-0443-6},
journal = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
keywords = {brooks' law,developer,linear regression,longitudinal analysis,modularity,team expansion metric},
pages = {81--90},
title = {{Does Adding Manpower Also Affect Quality?: An Empirical, Longitudinal Analysis}},
url = {http://doi.acm.org/10.1145/2025113.2025128},
year = {2011}
}
@article{Meneely2010,
abstract = {Open source software is often considered to be secure because large developer communities can be leveraged to find and fix security vulnerabilities. Eric Raymond states Linus' Law as "many eyes make all bugs shallow", reasoning that a diverse set of perspectives improves the quality of a software product. However, at what point does the multitude of developers become "too many cooks in the kitchen", causing the system's security to suffer as a result? In a previous study, we quantified Linus' Law and "too many cooks in the kitchen" with developer activity metrics and found a statistical association between these metrics and security vulnerabilities in the Linux kernel. In the replication study reported in this paper, we performed our analysis on two additional projects: the PHP programming language and the Wireshark network protocol analyzer. We also updated our Linux kernel case study with 18 additional months of newly-discovered vulnerabilities. In all three case studies, files changed by six developers or more were at least four times more likely to have a vulnerability than files changed by fewer than six developers. Furthermore, we found that our predictive models improved on average when combining data from multiple projects, indicating that models can be transferred from one project to another. 2010 ACM.},
author = {Meneely, Andrew and Williams, Laurie},
doi = {10.1145/1852786.1852798},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/4th International Symposium on Empirical Software Engineering and Measurement, ESEM 2010, September 16, 2010 - September 17, 2010/Strengthening the empirical analysis of the relationship between Linus' Law and software security.pdf:pdf},
isbn = {9781450300391},
journal = {4th International Symposium on Empirical Software Engineering and Measurement, ESEM 2010, September 16, 2010 - September 17, 2010},
keywords = {Computer operating systems,Network protocols,Network security,Open systems,Program debugging,Software engineering},
pages = {ACM Special Interest Group on Software Engineering},
title = {{Strengthening the empirical analysis of the relationship between Linus' Law and software security}},
url = {http://dx.doi.org/10.1145/1852786.1852798$\backslash$nhttp://delivery.acm.org/10.1145/1860000/1852798/a9-meneely.pdf?ip=130.15.4.210\&id=1852798\&acc=ACTIVE SERVICE\&key=C2716FEBFA981EF105571188EDCBBEC8F965373064B8B792\&CFID=364482215\&CFTOKEN=88754947\&\_\_acm\_\_=1379993395},
year = {2010}
}
@article{Menzies2004,
abstract = { Assessing software costs money and better assessment costs exponentially more money. Given finite budgets, assessment resources are typically skewed towards areas that are believed to be mission critical. This leaves blind spots: portions of the system that may contain defects which may be missed. Therefore, in addition to rigorously assessing mission critical areas, a parallel activity should sample the blind spots. This paper assesses defect detectors based on static code measures as a blind spot sampling method. In contrast to previous results, we find that such defect detectors yield results that are stable across many applications. Further, these detectors are inexpensive to use and can be tuned to the specifics of the current business situations.},
author = {Menzies, T. and Stefano, J.S. Di},
doi = {10.1109/HASE.2004.1281737},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings/How good is your blind spot sampling policy.pdf:pdf},
isbn = {0-7695-2094-4},
issn = {1530-2059},
journal = {Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.},
pages = {1--10},
title = {{How good is your blind spot sampling policy}},
year = {2004}
}
@article{Menzies2013,
author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
doi = {10.1109/TSE.2012.83},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/IEEE Transactions on Software Engineering/Local versus Global Lessons for Defect Prediction and Effort Estimation.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = jun,
number = {6},
pages = {822--834},
title = {{Local versus Global Lessons for Defect Prediction and Effort Estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6363444},
volume = {39},
year = {2013}
}
@article{Menzies2010,
abstract = {Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective. Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection AUC(pd, pf) ; i.e. the area under the curve of a probability of false alarm versus probability of detection. Accordingly, we explore changing the standard goal. Learners that maximize AUC(effort, pd) find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods. Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.},
author = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ayşe},
doi = {10.1007/s10515-010-0069-5},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Automated Software Engineering/Defect prediction from static code features Current results, limitations, new approaches.pdf:pdf},
issn = {09288910},
journal = {Automated Software Engineering},
keywords = {Defect prediction,Static code features,Which},
pages = {375--407},
title = {{Defect prediction from static code features: Current results, limitations, new approaches}},
volume = {17},
year = {2010}
}
@article{Miller1991,
abstract = {This paper presents a comprehensive approach to the validation of logistic prediction models. It reviews measures of overall goodness-of-fit, and indices of calibration and refinement. Using a model-based approach developed by Cox, we adapt logistic regression diagnostic techniques for use in model validation. This allows identification of problematic predictor variables in the prediction model as well as influential observations in the validation data that adversely affect the fit of the model. In appropriate situations, recommendations are made for correction of models that provide poor fit.},
author = {Miller, M E and Hui, S L and Tierney, W M},
doi = {10.1002/sim.4780100805},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1991/Statistics in medicine/Validation techniques for logistic regression models.pdf:pdf},
isbn = {1097-0258},
issn = {0277-6715},
journal = {Statistics in medicine},
number = {8},
pages = {1213--1226},
pmid = {1925153},
title = {{Validation techniques for logistic regression models.}},
volume = {10},
year = {1991}
}
@article{Moore,
author = {Moore, Andrew W},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Cross-validation for detecting and preventing overfitting.pdf:pdf},
title = {{Cross-validation for detecting and preventing overfitting}}
}
@inproceedings{Moser2008,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
booktitle = {Proceedings of the 13th international conference on Software engineering (ICSE '08)},
doi = {10.1145/1368088.1368114},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Proceedings of the 13th international conference on Software engineering (ICSE '08)/A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction.pdf:pdf},
isbn = {9781605580791},
keywords = {cost-sensitive classification,defect prediction,software metrics},
pages = {181},
title = {{A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction}},
year = {2008}
}
@article{Nagappan2005a,
abstract = { During software development it is helpful to obtain early estimates of the defect density of software components. Such estimates identify fault-prone areas of code requiring further testing. We present an empirical approach for the early prediction of pre-release defect density based on the defects found using static analysis tools. The defects identified by two different static analysis tools are used to fit and predict the actual pre-release defect density for Windows Server 2003. We show that there exists a strong positive correlation between the static analysis defect density and the pre-release defect density determined by testing. Further, the predicted pre-release defect density and the actual pre-release defect density are strongly correlated at a high degree of statistical significance. Discriminant analysis shows that the results of static analysis tools can be used to separate high and low quality components with an overall classification rate of 82.91\%.},
author = {Nagappan, N. and Ball, T.},
doi = {10.1109/ICSE.2005.1553604},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2005/Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005/Static analysis tools as early indicators of pre-release defect density.pdf:pdf},
isbn = {1-59593-963-2},
journal = {Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.},
keywords = {defect density,fault-,static analysis tools,statistical methods},
title = {{Static analysis tools as early indicators of pre-release defect density}},
year = {2005}
}
@inproceedings{Nagappan2010,
author = {Nagappan, Nachiappan and Zeller, Andreas and Zimmermann, Thomas and Herzig, Kim and Murphy, Brendan},
booktitle = {Prooceedings of the 21st International Symposium on Software Reliability Engineering (ISSRE'},
doi = {10.1109/ISSRE.2010.25},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Prooceedings of the 21st International Symposium on Software Reliability Engineering (ISSRE'/Change Bursts as Defect Predictors.pdf:pdf},
isbn = {978-1-4244-9056-1},
keywords = {-process metrics,assurance,change history,defects,developers,empirical studies,product metrics,software mining,software quality,version control},
month = nov,
pages = {309--318},
title = {{Change Bursts as Defect Predictors}},
year = {2010}
}
@article{Nguyen2011,
author = {Nguyen, Tung Thanh Tien N. Tung Thanh Tien N. and Phuong, Tu Minh},
doi = {10.1145/1985793.1985950},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceeding of the 33rd international conference on Software engineering - ICSE '11/Topic-based defect prediction.pdf:pdf},
isbn = {9781450304450},
journal = {Proceeding of the 33rd international conference on Software engineering - ICSE '11},
keywords = {defect prediction,topic modeling},
pages = {932},
title = {{Topic-based defect prediction}},
url = {http://dl.acm.org/citation.cfm?id=1985950},
year = {2011}
}
@article{Nikora2004,
abstract = {Over the past few years, we have been developing software fault predictors based on a system's measured structural evolution. We have previously shown there is a significant linear relationship between code chum, a set of synthesized metrics, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code chum. A limiting factor in this and other investigations of a similar nature has been the absence of a quantitative, consistent, and repeatable definition of what constitutes a fault. The rules for fault definition were not sufficiently rigorous to provide unambiguous, repeatable fault counts. Within the framework of a space mission software development effort at the Jet Propulsion Laboratory (JPL) we have developed a standard for the precise enumeration of faults. This new standard permits software faults to be measured directly from configuration control documents. Our results indicate that reasonable predictors of the number of faults inserted into a software system can be developed from measures of the system's structural evolution. We compared the new method of counting faults with two existing techniques to determine whether the fault counting technique has an effect on the quality of the fault models constructed from those counts. The new fault definition provides higher quality fault models than those obtained using the other definitions of fault},
author = {a.P. Nikora and Munson, J.C.},
doi = {10.1109/CMPSAC.2004.1342827},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004/The effects of fault counting methods on fault model quality.pdf:pdf},
isbn = {0-7695-2209-2},
issn = {0730-3157},
journal = {Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.},
keywords = {defect content estimation tech-,fault prediction,niques,soft-,software measurement},
title = {{The effects of fault counting methods on fault model quality}},
year = {2004}
}
@inproceedings{Nikora2003,
author = {Nikora, Allen P and Fax, I and Munson, John C},
booktitle = {Proceedings of the International Software Metrics Symposium},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/Proceedings of the International Software Metrics Symposium/Developing Fault Predictors for Evolving Software Systems.pdf:pdf},
keywords = {defect content estimation techniques,fault prediction,soft-,software metrics},
pages = {338----350},
title = {{Developing Fault Predictors for Evolving Software Systems}},
year = {2003}
}
@article{Oser,
author = {Oser, Scott},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Physics 509 Bootstrap and Robust Parameter Estimation.pdf:pdf},
title = {{Physics 509 : Bootstrap and Robust Parameter Estimation}}
}
@article{Pan2006,
abstract = {In this paper, we introduce 13 program slicing metrics for C language programs. These metrics use program slice information to measure the size, complexity, coupling, and cohesion properties of programs. Compared with traditional code metrics based on code statements or code structure, program slicing metrics involve measures for program behaviors. To evaluate the program slicing metrics, we compare them with the Understand for C++ suite of metrics, a set of widely-used traditional code metrics, in a series of bug classification experiments. We used the program slicing and the Understand for C++ metrics computed for 887 revisions of the Apache HTTP project and 76 revisions of the Latex2rtf project to classify source code files or functions as either buggy or bug-free. We then compared their classification prediction accuracy. Program slicing metrics have slightly better performance than the Understand for C++ metrics in classifying buggy/bug-free source code. Program slicing metrics have an overall 82.6\% (Apache) (Latex2rtf) accuracy at the file and 92\% level, better than the Understand for C++ metrics with an overall 80.4\% (Apache) and 88\% (Latex2rtf) accuracy. The experiments illustrate that the program slicing metrics have at least the same bug classification performance as the Understand for C++ metrics.},
author = {Pan, Kai and Kim, Sunghun and {Whitehead, Jr.}, E.},
doi = {10.1109/SCAM.2006.6},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/2006 Sixth IEEE International Workshop on Source Code Analysis and Manipulation/Bug Classification Using Program Slicing Metrics.pdf:pdf},
isbn = {0-7695-2353-6},
journal = {2006 Sixth IEEE International Workshop on Source Code Analysis and Manipulation},
pages = {31--42},
title = {{Bug Classification Using Program Slicing Metrics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4026853},
year = {2006}
}
@article{Peduz1996,
author = {Peduz, Peter and Concato, John and Kemper, Elizabeth and Holford, Theodore R and Feinstein, Alvan R},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/1996/Journal of Clinical Epidemiology/A Simulation Study of the Number of Events per Variable in Logistic Regression Analysis.pdf:pdf},
journal = {Journal of Clinical Epidemiology},
number = {12},
pages = {1373--1379},
title = {{A Simulation Study of the Number of Events per Variable in Logistic Regression Analysis}},
volume = {49},
year = {1996}
}
@article{Pendharkar2010,
abstract = {In this paper, we propose a software defect prediction model learning problem (SDPMLP) where a classification model selects appropriate relevant inputs, from a set of all available inputs, and learns the classification function. We show that the SDPMLP is a combinatorial optimization problem with factorial complexity, and propose two hybrid exhaustive search and probabilistic neural network (PNN), and simulated annealing (SA) and PNN procedures to solve it. For small size SDPMLP, exhaustive search PNN works well and provides an (all) optimal solution(s). However, for large size SDPMLP, the use of exhaustive search PNN approach is not pragmatic and only the SA-PNN allows us to solve the SDPMLP in a practical time limit. We compare the performance of our hybrid approaches with traditional classification algorithms and find that our hybrid approaches perform better than traditional classification algorithms. ?? 2009 Elsevier Ltd. All rights reserved.},
author = {Pendharkar, Parag C.},
doi = {10.1016/j.engappai.2009.10.001},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Engineering Applications of Artificial Intelligence/Exhaustive and heuristic search approaches for learning a software defect prediction model.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Exhaustive search,Heuristics,Probabilistic neural networks,Simulated annealing,Software engineering},
number = {1},
pages = {34--40},
publisher = {Elsevier},
title = {{Exhaustive and heuristic search approaches for learning a software defect prediction model}},
url = {http://dx.doi.org/10.1016/j.engappai.2009.10.001},
volume = {23},
year = {2010}
}
@inproceedings{Posnett2011,
author = {Posnett, Daryl},
booktitle = {Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering (FSE'11)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering (FSE'11)/BugCache for Inspections Hit or Miss.pdf:pdf},
isbn = {9781450304436},
keywords = {empirical software engineering,fault prediction,inspection},
pages = {322--331},
title = {{BugCache for Inspections : Hit or Miss ?}},
year = {2011}
}
@article{Quah2003,
author = {Quah, Tong-seng and Mie, Mie and Thwin, Thet},
doi = {10.1109/ICSM.2003.1235412},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/International Conference on Software Maintenance, 2003. ICSM 2003. Proceedings/Application of neural networks for software quality prediction using object-oriented metrics.pdf:pdf},
isbn = {0-7695-1905-9},
journal = {International Conference on Software Maintenance, 2003. ICSM 2003. Proceedings.},
pages = {116--125},
title = {{Application of neural networks for software quality prediction using object-oriented metrics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1235412},
year = {2003}
}
@inproceedings{Posnett,
author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar T.},
booktitle = {Proceedings of the International Symposium on the Foundations of Software Engineering (FSE'12)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the International Symposium on the Foundations of Software Engineering (FSE'12)/Recalling the Imprecision of Cross-Project Defect Prediction Categories and Subject Descriptors.pdf:pdf},
isbn = {9781450316149},
keywords = {all or part of,empirical software engineering,fault prediction,inspection,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
pages = {61},
title = {{Recalling the "Imprecision" of Cross-Project Defect Prediction Categories and Subject Descriptors}},
year = {2012}
}
@article{Ramler2009,
abstract = {The information about which modules in a software system's future version are potentially defective is a valuable aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. Constructing effective defect prediction models in an industrial setting involves the decision from what data source the defect predictors should be derived. In this paper we compare defect prediction results based on three different data sources of a large industrial software system to answer the question what repositories to mine. In addition, we investigate whether a combination of different data sources improves the prediction results. The findings indicate that predictors derived from static code and design analysis provide slightly yet still significant better results than predictors derived from version control, while a combination of all data sources showed no further improvement.},
author = {Ramler, Rudolf and Larndorfer, Stefan and Natschl\"{a}ger, Thomas},
doi = {10.1109/SEAA.2009.65},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Conference Proceedings of the EUROMICRO/What software repositories should be mined for defect predictors.pdf:pdf},
isbn = {9780769537849},
issn = {10896503},
journal = {Conference Proceedings of the EUROMICRO},
keywords = {Data mining,Defect prediction,Software repositories},
pages = {181--187},
title = {{What software repositories should be mined for defect predictors?}},
year = {2009}
}
@article{Rate,
author = {Rate, Decompose Error and Decomposition, Variance and Bias, Regression and Bias, Classification and Analysis, Variance and Effect, Learning Algorithms and Effect, Variance},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Bias - Variance Theory Bias - Variance Analysis in Regression.pdf:pdf},
title = {{Bias - Variance Theory Bias - Variance Analysis in Regression}}
}
@article{Sahiner2008,
author = {Sahiner, Berkman and Chan, Heang-Ping and Hadjiiski, Lubomir},
doi = {10.1118/1.2868757},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Medical Physics/Classifier performance prediction for computer-aided diagnosis using a limited dataset.pdf:pdf},
issn = {00942405},
journal = {Medical Physics},
keywords = {bootstrap,classifier performance,finite sample size,resampling},
number = {4},
pages = {1559},
title = {{Classifier performance prediction for computer-aided diagnosis using a limited dataset}},
url = {http://scitation.aip.org/content/aapm/journal/medphys/35/4/10.1118/1.2868757},
volume = {35},
year = {2008}
}
@article{Sandhu2010a,
author = {Sandhu, P S and Kaur, M and Kaur, a},
doi = {10.1109/ICEIE.2010.5559753},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/ICEIE 2010 - 2010 International Conference on Electronics and Information Engineering, Proceedings/A density based clustering approach for early detection of fault prone modules.pdf:pdf},
isbn = {978-1-4244-7679-4},
journal = {ICEIE 2010 - 2010 International Conference on Electronics and Information Engineering, Proceedings},
keywords = {- clustering,amandeep kaur,and software quality,defect data,density based clustering,deptt,fault,it,lecturer,manpreet kaur,of cse,software},
number = {Iceie},
pages = {V2525--V2530},
title = {{A density based clustering approach for early detection of fault prone modules}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-78049323525\&partnerID=40\&md5=e862126edae29d324783416ba137517c},
volume = {2},
year = {2010}
}
@article{Scanniello2013,
author = {Scanniello, Giuseppe and Gravino, Carmine and Marcus, Andrian and Menzies, Tim},
doi = {10.1109/ASE.2013.6693126},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/2013 28th IEEEACM International Conference on Automated Software Engineering (ASE)/Class level fault prediction using software clustering.pdf:pdf},
isbn = {978-1-4799-0215-6},
journal = {2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
keywords = {- empirical study,fault prediction,software},
month = nov,
pages = {640--645},
publisher = {Ieee},
title = {{Class level fault prediction using software clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6693126},
year = {2013}
}
@article{Schroter2006,
abstract = {We have mined the Eclipse bug and version databases to map failures to Eclipse components. The resulting data set lists the defect density of all Eclipse components. As we demonstrate in three simple experiments, the bug data set can be easily used to relate code, process, and developers to defects. The data set is publicly avail-able for download.},
author = {Schr\"{o}ter, a and Zimmermann, Thomas},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the 5th \ldots/If your bug database could talk.pdf:pdf},
journal = {Proceedings of the 5th \ldots},
pages = {3--5},
title = {{If your bug database could talk}},
url = {http://home.segal.uvic.ca/~pubs/pdf/14/schroeter-isese-2006b.pdf},
year = {2006}
}
@article{Schroter2006a,
abstract = {How do design decisions impact the quality of the resulting soft- ware? In an empirical study of 52 ECLIPSE plug-ins, we found that the software design as well as past failure history, can be used to build models which accurately predict failure-prone components in newprograms. Our prediction only requires usage relationships be- tween components, which are typically defined in the design phase; thus, designers can easily explore and assess design alternatives in terms of predicted quality. In the ECLIPSE study, 90\% of the 5\% most failure-prone components, as predicted by our model from design data, turned out to actually produce failures later; a random guess would have predicted only 33\%.},
author = {Schr\"{o}ter, Adrian and Zimmermann, Thomas and Zeller, Andreas},
doi = {10.1145/1159733.1159739},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2006/Proceedings of the 2006 ACMIEEE international symposium on International symposium on empirical software engineering - ISESE '06/Predicting component failures at design time.pdf:pdf},
isbn = {1595932186},
journal = {Proceedings of the 2006 ACM/IEEE international symposium on International symposium on empirical software engineering - ISESE '06},
pages = {18},
title = {{Predicting component failures at design time}},
url = {http://portal.acm.org/citation.cfm?doid=1159733.1159739},
year = {2006}
}
@article{Selvarani2009,
abstract = {Software engineering is continuously facing the challenges of growing complexity of software packages and increased level of data on defects and drawbacks from software production process. This makes a clarion call for inventions and methods which can enable a more reusable, reliable, easily maintainable and high quality software systems with deeper control on software generation process. Quality and productivity are indeed the two most important parameters for controlling any industrial process. Implementation of a successful control system requires some means of measurement. Software metrics play an important role in the management aspects of the software development process such as better planning, assessment of improvements, resource allocation and reduction of unpredictability. The process involving early detection of potential problems, productivity evaluation and evaluating external quality factors such as reusability, maintainability, defect proneness and complexity are of utmost importance. Here we discuss the application of CK metrics and estimation model to predict the external quality parameters for optimizing the design process and production process for desired levels of quality. Estimation of defect-proneness in object-oriented system at design level is developed using a novel methodology where models of relationship between CK metrics and defect-proneness index is achieved. A multifunctional estimation approach captures the correlation between CK metrics and defect proneness level of software modules.},
archivePrefix = {arXiv},
arxivId = {1001.3555},
author = {Selvarani, R. and Nair, T. R Gopalakrishnan and Prasad, V. Kamakshi},
doi = {10.1109/ICSPS.2009.163},
eprint = {1001.3555},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 International Conference on Signal Processing Systems, ICSPS 2009/Estimation of defect proneness using design complexity measurements in object-oriented software.pdf:pdf},
isbn = {9780769536545},
journal = {2009 International Conference on Signal Processing Systems, ICSPS 2009},
keywords = {DIT,Defect-proneness,Design,Estimation,Internal parameters,Metrics,Quality,RFC,WMC},
pages = {766--770},
title = {{Estimation of defect proneness using design complexity measurements in object-oriented software}},
year = {2009}
}
@article{Shatnawi2008,
abstract = {Many empirical studies have found that software metrics can predict class error proneness and the prediction can be used to accurately group error-prone classes. Recent empirical studies have used open source systems. These studies, however, focused on the relationship between software metrics and class error proneness during the development phase of software projects. Whether software metrics can still predict class error proneness in a system's post-release evolution is still a question to be answered. This study examined three releases of the Eclipse project and found that although some metrics can still predict class error proneness in three error-severity categories, the accuracy of the prediction decreased from release to release. Furthermore, we found that the prediction cannot be used to build a metrics model to identify error-prone classes with acceptable accuracy. These findings suggest that as a system evolves, the use of some commonly used metrics to identify which classes are more prone to errors becomes increasingly difficult and we should seek alternative methods (to the metric-prediction models) to locate error-prone classes if we want high accuracy. ?? 2008 Elsevier Inc. All rights reserved.},
author = {Shatnawi, Raed and Li, Wei},
doi = {10.1016/j.jss.2007.12.794},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Journal of Systems and Software/The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Class error proneness,Design evolution,Empirical study,Error-severity categories,Object-oriented metrics,Open source software},
pages = {1868--1882},
title = {{The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process}},
volume = {81},
year = {2008}
}
@phdthesis{Shihab2012,
author = {Shihab, Emad},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Unknown/An Exploration of Challenges Limiting Pragmatic Software Defect Prediction.pdf:pdf},
number = {August},
school = {Queen's University},
title = {{An Exploration of Challenges Limiting Pragmatic Software Defect Prediction}},
year = {2012}
}
@inproceedings{Shihab2012a,
author = {Shihab, Emad and Hassan, Ahmed E. and Adams, Bram and Jiang, Zhen Ming},
booktitle = {Proceedings of the International Symposium on the Foundations of Software Engineering (FSE'12)},
doi = {10.1145/2393596.2393670},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2012/Proceedings of the International Symposium on the Foundations of Software Engineering (FSE'12)/An industrial study on the risk of software changes.pdf:pdf},
isbn = {9781450316149},
pages = {62},
title = {{An industrial study on the risk of software changes}},
url = {http://dl.acm.org/citation.cfm?doid=2393596.2393670},
year = {2012}
}
@article{Shivaji2013,
abstract = {Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.},
author = {Shivaji, Shivkumar and {James Whitehead}, E. and Akella, Ram and Kim, Sunghun},
doi = {10.1109/TSE.2012.43},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/IEEE Transactions on Software Engineering/Reducing features to improve code change-based bug prediction.pdf:pdf},
isbn = {9780769538914},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Reliability,bug prediction,feature selection,machine learning},
number = {Section II},
pages = {552--569},
title = {{Reducing features to improve code change-based bug prediction}},
volume = {39},
year = {2013}
}
@article{Simon,
author = {Simon, G and Lendasse, A and Verleysen, M},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/Unknown/Unknown/Bootstrap for model selection.pdf:pdf},
pages = {182--189},
title = {{Bootstrap for model selection :}}
}
@article{Singh2009,
abstract = {Software repositories with defect logs are main resource for defect prediction. In recent years, researchers have used the vast amount of data that is contained by software repositories to predict the location of defect in the code that caused problems. In this paper we evaluate the effectiveness of software fault prediction with Naive-Bayes classifiers and J48 classifier by integrating with supervised discretization algorithm developed by Fayyad and Irani. Public datasets from the promise repository have been explored for this purpose. The repository contains software metric data and error data at the function/method level. Our experiment shows that integration of discretization method improves the software fault prediction accuracy when integrated with Naive-Bayes and J48 classifiers.},
author = {Singh, P. and Verma, S.},
doi = {10.1109/ACT.2009.212},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 International Conference on Advances in Computing, Control, and Telecommunication Technologies/An Investigation of the Effect of Discretization on Defect Prediction Using Static Measures.pdf:pdf},
isbn = {978-1-4244-5321-4},
journal = {2009 International Conference on Advances in Computing, Control, and Telecommunication Technologies},
keywords = {Defect prediction,Machine learning,Software metrics},
pages = {837--839},
title = {{An Investigation of the Effect of Discretization on Defect Prediction Using Static Measures}},
year = {2009}
}
@article{Song2011,
author = {Song, Qinbao and Jia, Zihan and Shepperd, Martin and Ying, Shi and Liu, Jin},
doi = {10.1109/TSE.2010.90},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/IEEE Transactions on Software Engineering/A General Software Defect-Proneness Prediction Framework.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = may,
number = {3},
pages = {356--370},
title = {{A General Software Defect-Proneness Prediction Framework}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5611551},
volume = {37},
year = {2011}
}
@article{Steyerberg2001,
author = {Steyerberg, Ewout W and Harrell, Frank E and Borsboom, Gerard J J M and Eijkemans, M J C Ren\'{e} and Vergouwe, Yvonne and Habbema, J Dik F},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2001/Unknown/Internal validation of predictive models Efficiency of some procedures for logistic regression analysis.pdf:pdf},
keywords = {bootstrapping,internal validation,logistic regression analysis,predictive models},
pages = {774--781},
title = {{Internal validation of predictive models : Efficiency of some procedures for logistic regression analysis}},
volume = {54},
year = {2001}
}
@article{Storkey2013,
author = {Storkey, Amos J},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2013/Unknown/When Training and Test Sets are Different Characterising Learning Transfer.pdf:pdf},
title = {{When Training and Test Sets are Different : Characterising Learning Transfer}},
year = {2013}
}
@article{Succi2003,
abstract = {The goal of this paper is to investigate and assess the ability of explanatory models based on design metrics to describe and predict defect counts in an object-oriented software system. Specifically, we empirically evaluate the influence of design decisions to defect behavior of the classes in two products from the commercial software domain. Information provided by these models can help in resource allocation and serve as a base for assessment and future improvements. We use innovative statistical methods to deal with the peculiarities of the software engineering data, such as non-normally distributed count data. To deal with overdispersed data and excess of zeroes in the dependent variable, we use negative binomial (NB) and zero-inflated NB regression in addition to Poisson regression. Furthermore, we form a framework for comparison of models' descriptive and predictive ability. Predictive capability of the models to identify most critical classes in the system early in the software development process can help in allocation of resources and foster software quality improvement. In addition to the correlation coefficients, we use additional statistics to assess a models' ability to explain high variability in the data and Pareto analysis to assess a models' ability to identify the most critical classes in the system. Results indicate that design aspects related to communication between classes and inheritance can be used as indicators of the most defect-prone classes, which require the majority of resources in development and testing phases. The zero-inflated negative binomial regression model, designed to explicitly model the occurrence of zero counts in the dataset, provides the best results for this purpose. © 2002 Elsevier Science Inc. All rights reserved.},
author = {Succi, Giancarlo and Pedrycz, Witold and Stefanovic, Milorad and Miller, James},
doi = {10.1016/S0164-1212(02)00024-9},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2003/Journal of Systems and Software/Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
pages = {1--12},
title = {{Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics}},
volume = {65},
year = {2003}
}
@inproceedings{Tan2015,
author = {Tan, Ming and Tan, Lin and Dara, Sashank and Mayeux, Caleb},
booktitle = {Prooceedings of the International Conference on Software Engineering (ICSE'15)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2015/Prooceedings of the International Conference on Software Engineering (ICSE'15)/Online Defect Prediction for Imbalanced Data.pdf:pdf},
pages = {To appear},
title = {{Online Defect Prediction for Imbalanced Data}},
year = {2015}
}
@article{Tosun2009,
abstract = {Software defect data has an imbalanced and highly skewed class distribution. The misclassification costs of two classes are not equal nor are known. It is critical to find the optimum bound, i.e. threshold, which would best separate defective and defect-free classes in software data. We have applied decision threshold optimization on Naiumlve Bayes classifier in order to find the optimum threshold for software defect data. ROC analyses show that decision threshold optimization significantly decreases false alarms (on the average by 11\%) without changing probability of detection rates.},
author = {Tosun, Ayşe and Bener, Ayşe},
doi = {10.1109/ESEM.2009.5316006},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009/Reducing false alarms in software defect prediction by decision threshold optimization.pdf:pdf},
isbn = {9781424448418},
issn = {1938-6451},
journal = {2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009},
pages = {477--480},
title = {{Reducing false alarms in software defect prediction by decision threshold optimization}},
year = {2009}
}
@article{Turhan2011,
author = {Turhan, Burak},
doi = {10.1007/s10664-011-9182-8},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2011/Empirical Software Engineering/On the dataset shift problem in software engineering prediction models(2).pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {dataset shift,defect prediction,effort estimation,prediction models},
number = {1-2},
pages = {62--74},
title = {{On the dataset shift problem in software engineering prediction models}},
url = {http://link.springer.com/10.1007/s10664-011-9182-8},
volume = {17},
year = {2011}
}
@article{Turhan2009,
abstract = {In a large software system knowing which files are most likely to be fault-prone is valuable information for project managers. They can use such information in prioritizing software testing and allocating resources accordingly. However, our experience shows that it is difficult to collect and analyze fine-grained test defects in a large and complex software system. On the other hand, previous research has shown that companies can safely use cross-company data with nearest neighbor sampling to predict their defects in case they are unable to collect local data. In this study we analyzed 25 projects of a large telecommunication system. To predict defect proneness of modules we trained models on publicly available Nasa MDP data. In our experiments we used static call graph based ranking (CGBR) as well as nearest neighbor sampling for constructing method level defect predictors. Our results suggest that, for the analyzed projects, at least 70\% of the defects can be detected by inspecting only (i) 6\% of the code using a Na??ve Bayes model, (ii) 3\% of the code using CGBR framework. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Turhan, Burak and Kocak, Gozde and Bener, Ayse},
doi = {10.1016/j.eswa.2008.12.028},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Expert Systems with Applications/Data mining source code for locating software bugs A case study in telecommunication industry.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Case study,Defect prediction,Software bugs,Software testing},
number = {6},
pages = {9986--9990},
publisher = {Elsevier Ltd},
title = {{Data mining source code for locating software bugs: A case study in telecommunication industry}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.12.028},
volume = {36},
year = {2009}
}
@article{Turhan2009a,
author = {Turhan, Burak and Menzies, Tim and Bener, Ayşe B. and {Di Stefano}, Justin},
doi = {10.1007/s10664-008-9103-7},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Empirical Software Engineering/On the relative value of cross-company and within-company data for defect prediction.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = jan,
number = {5},
pages = {540--578},
title = {{On the relative value of cross-company and within-company data for defect prediction}},
url = {http://link.springer.com/10.1007/s10664-008-9103-7},
volume = {14},
year = {2009}
}
@inproceedings{Dallmeier2007,
author = {{Valentin Dallmeier and Thomas Zimmermann}},
booktitle = {Proceedings of the 22nd IEEE/ACM international conference on Automated Software Engineering (ASE'07)},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Proceedings of the 22nd IEEEACM international conference on Automated Software Engineering (ASE'07)/Extraction of Bug Localization Bechmarks from History.pdf:pdf},
isbn = {9781595938824},
pages = {433--436},
title = {{Extraction of Bug Localization Bechmarks from History}},
year = {2007}
}
@article{Vandecruys2008,
abstract = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models. © 2007 Elsevier Inc. All rights reserved.},
author = {Vandecruys, Olivier and Martens, David and Baesens, Bart and Mues, Christophe and {De Backer}, Manu and Haesen, Raf},
doi = {10.1016/j.jss.2007.07.034},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/Journal of Systems and Software/Mining software repositories for comprehensible software fault prediction models.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Ant Colony Optimization,Classification,Comprehensibility,Fault prediction,Software mining},
pages = {823--839},
title = {{Mining software repositories for comprehensible software fault prediction models}},
volume = {81},
year = {2008}
}
@article{Vivanco2010,
abstract = {Predictive models can be used in the detection of fault prone modules using source code metrics as inputs for the classifier. However, there exist numerous structural measures that capture different aspects of size, coupling and complexity. Identifying a metric subset that enhances the performance for the predictive objective would not only improve the model but also provide insights into the structural properties that lead to problematic modules. Another difficulty in building predictive models comes from unbalanced datasets, which are common in empirical software engineering as a majority of the modules are not likely to be faulty. Oversampling attempts to overcome this deficiency by generating new training instances from the faulty modules. We present the results of applying search-based metric selection and oversampling to three NASA datasets. For these datasets, oversampling results in the largest improvement. Metric subset selection was able to reduce up to 52\% of the metrics without decreasing the predictive performance gained with oversampling.},
author = {Vivanco, R. and Kamei, Y. and Monden, a. and Matsumoto, K. and Jin, D.},
doi = {10.1109/CCECE.2010.5575249},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Canadian Conference on Electrical and Computer Engineering/Using search-based metric selection and oversampling to predict fault prone modules.pdf:pdf},
isbn = {9781424453764},
issn = {08407789},
journal = {Canadian Conference on Electrical and Computer Engineering},
keywords = {Dataset oversampling,Genetic algorithm,Software quality models,Source code metrics},
title = {{Using search-based metric selection and oversampling to predict fault prone modules}},
year = {2010}
}
@article{Weyuker2008,
abstract = {Two different software fault prediction models have been used to predict the N\% of the files of a large software system that are likely to contain the largest numbers of faults. We used the same predictor variables in a negative binomial regression model and a recursive partitioning model, and compared their effectiveness on three large industrial software systems. The negative binomial model identified files that contain 76 to 93 percent of the faults, and recursive partitioning identified files that contain 68 to 85 percent. Copyright 2008 ACM.},
author = {Weyuker, Elaine J and Ostrand, Thomas J and Bell, Robert M},
doi = {10.1145/1370788.1370792},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2008/\ldots workshop on Predictor models in \ldots/Comparing negative binomial and recursive partitioning models for fault prediction.pdf:pdf},
isbn = {9781605580364},
issn = {02705257},
journal = {\ldots workshop on Predictor models in \ldots},
keywords = {binary splits,building a tree based,each on a single,empirical study,fault predic-,negative binomial,on successive,predictor variable,recur-,recursive partition,software testing,tion,variation in outcomes by},
pages = {3--9},
title = {{Comparing negative binomial and recursive partitioning models for fault prediction}},
url = {http://dl.acm.org/citation.cfm?id=1370792},
year = {2008}
}
@article{Wiegand2010,
abstract = {Some research studies in the medical literature use multiple stepwise variable selection (SVS) algorithms to build multivariable models. The purpose of this study is to determine whether the use of multiple SVS algorithms in tandem (stepwise agreement) is a valid variable selection procedure. Computer simulations were developed to address stepwise agreement. Three popular SVS algorithms were tested (backward elimination, forward selection, and stepwise) on three statistical methods (linear, logistic, and Cox proportional hazards regression). Other simulation parameters explored were the sample size, number of predictors considered, degree of correlation between pairs of predictors, p-value-based entrance and exit criteria, predictor type (normally distributed or binary), and differences between stepwise agreement between any two or all three algorithms. Among stepwise methods, the rate of agreement, agreement on a model including only those predictors truly associated with the outcome, and agreement on a model containing the predictors truly associated with the outcome were measured. These rates were dependent on all simulation parameters. Mostly, the SVS algorithms agreed on a final model, but rarely on a model with only the true predictors. Sample size and candidate predictor pool size are the most influential simulation conditions. To conclude, stepwise agreement is often a poor strategy that gives misleading results and researchers should avoid using multiple SVS algorithms to build multivariable models. More research on the relationship between sample size and variable selection is needed.},
author = {Wiegand, Ryan E.},
doi = {10.1002/sim.3943},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2010/Statistics in Medicine/Performance of using multiple stepwise algorithms for variable selection.pdf:pdf},
isbn = {1097-0258 (Electronic)$\backslash$r0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Regression,Stepwise,Variable selection},
number = {5},
pages = {1647--1659},
pmid = {20552568},
title = {{Performance of using multiple stepwise algorithms for variable selection}},
volume = {29},
year = {2010}
}
@article{Wood2007,
abstract = {MOTIVATION: Gene expression data offer a large number of potentially useful predictors for the classification of tissue samples into classes, such as diseased and non-diseased. The predictive error rate of classifiers can be estimated using methods such as cross-validation. We have investigated issues of interpretation and potential bias in the reporting of error rate estimates. The issues considered here are optimization and selection biases, sampling effects, measures of misclassification rate, baseline error rates, two-level external cross-validation and a novel proposal for detection of bias using the permutation mean.

RESULTS: Reporting an optimal estimated error rate incurs an optimization bias. Downward bias of 3-5\% was found in an existing study of classification based on gene expression data and may be endemic in similar studies. Using a simulated non-informative dataset and two example datasets from existing studies, we show how bias can be detected through the use of label permutations and avoided using two-level external cross-validation. Some studies avoid optimization bias by using single-level cross-validation and a test set, but error rates can be more accurately estimated via two-level cross-validation. In addition to estimating the simple overall error rate, we recommend reporting class error rates plus where possible the conditional risk incorporating prior class probabilities and a misclassification cost matrix. We also describe baseline error rates derived from three trivial classifiers which ignore the predictors.

AVAILABILITY: R code which implements two-level external cross-validation with the PAMR package, experiment code, dataset details and additional figures are freely available for non-commercial use from http://www.maths.qut.edu.au/profiles/wood/permr.jsp},
author = {Wood, Ian a and Visscher, Peter M and Mengersen, Kerrie L},
doi = {10.1093/bioinformatics/btm117},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2007/Bioinformatics (Oxford, England)/Classification based upon gene expression data bias and precision of error rates.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Artifacts,Artificial Intelligence,Cluster Analysis,Data Interpretation, Statistical,Databases, Genetic,Gene Expression Profiling,Gene Expression Profiling: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = jun,
number = {11},
pages = {1363--70},
pmid = {17392326},
title = {{Classification based upon gene expression data: bias and precision of error rates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17392326},
volume = {23},
year = {2007}
}
@article{Zhang2009,
abstract = {It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.},
author = {Zhang, Hongyu},
doi = {10.1109/ICSM.2009.5306304},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/IEEE International Conference on Software Maintenance, ICSM/An investigation of the relationships between lines of code and defects.pdf:pdf},
isbn = {9781424448289},
issn = {1063-6773},
journal = {IEEE International Conference on Software Maintenance, ICSM},
pages = {274--283},
title = {{An investigation of the relationships between lines of code and defects}},
year = {2009}
}
@article{Ieee2004,
author = {Zhong, Shi and Khoshgoftaar, Taghi M. and Seliya, Naeem},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2004/Proceedings of the International Symposium on High Assurance Systems Engineering (HASE'04)/Unsupervised Learning for Expert-Based Software Quality Estimation.pdf:pdf},
journal = {Proceedings of the International Symposium on High Assurance Systems Engineering (HASE'04)},
title = {{Unsupervised Learning for Expert-Based Software Quality Estimation}},
year = {2004}
}
@article{Zhu2009,
abstract = {Software defects are the parts of software products that software development company have to face. How to deal with them suitably is very important for software company's survival. This article will use the collecting data of software defects. Then, according to GM (1, 1), which is the core theory of the gray-prediction, establish the prediction model. Finally, we gain the prediction values. The results show that software company can improve software quality, control development process and allocate resources effectively.},
author = {Zhu, Dianqin and Wu, Zhongyuan},
doi = {10.1109/CISE.2009.5366957},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/Proceedings - 2009 International Conference on Computational Intelligence and Software Engineering, CiSE 2009/The application of Gray-prediction theory in the software defects management.pdf:pdf},
isbn = {9781424445073},
journal = {Proceedings - 2009 International Conference on Computational Intelligence and Software Engineering, CiSE 2009},
keywords = {GM (1, 1) model,Gray-prediction,Software defects},
number = {1},
title = {{The application of Gray-prediction theory in the software defects management}},
volume = {1},
year = {2009}
}
@article{Zimmermann2009a,
abstract = {Software development is a complex and error-prone task. An important factor during the development of complex systems is the understanding of the dependencies that exist between different pieces of the code. In this paper, we show that for Windows Server 2003 dependency data can predict the defect-proneness of software elements. Since most dependencies of a component are already known in the design phase, our prediction models can support design decisions.},
author = {Zimmermann, Thomas and Nagappan, Nachiappan},
doi = {10.1109/ESEM.2009.5316024},
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2009/2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009/Predicting defects with program dependencies.pdf:pdf},
isbn = {9781424448418},
issn = {1938-6451},
journal = {2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009},
pages = {435--438},
title = {{Predicting defects with program dependencies}},
year = {2009}
}
@article{,
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Unknown/the Estimation Logistic the in Bootstrap Forward Error Regression.pdf:pdf},
keywords = {error rate estimation,prediction,variables se-},
number = {393},
pages = {108--113},
title = {{the Estimation Logistic the in Bootstrap : Forward Error Regression}},
volume = {81},
year = {2014}
}
@article{,
file = {:Users/klainfo/Documents/Dropbox/Mendeley Desktop/2014/Unknown/Better Bootstrap Confidence Intervals.pdf:pdf},
number = {397},
pages = {171--185},
title = {{Better Bootstrap Confidence Intervals}},
volume = {82},
year = {2014}
}
